{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c44c9f-4ddc-4929-811f-d6e90124d6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tarfile\n",
    "import zipfile\n",
    "import hashlib\n",
    "import re\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import PyPDF2\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b4dd4-ef70-4f08-92e9-6b6d16abf5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FileContent:\n",
    "    \"\"\"Represents a file with its content and metadata.\"\"\"\n",
    "\n",
    "    path: str\n",
    "    content: str\n",
    "    file_type: str\n",
    "    hash: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExtractedFileInfo:\n",
    "    \"\"\"Information about an extracted file.\"\"\"\n",
    "\n",
    "    original_path: str\n",
    "    extracted_path: str\n",
    "    model_source: str  # \"A\" or \"B\"\n",
    "    file_type: str\n",
    "    extraction_status: str\n",
    "    file_size: int\n",
    "    hash: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExtractionReport:\n",
    "    \"\"\"Report of the extraction process.\"\"\"\n",
    "\n",
    "    timestamp: str\n",
    "    total_files_extracted: int\n",
    "    model_a_files: int\n",
    "    model_b_files: int\n",
    "    failed_extractions: List[str]\n",
    "    extracted_files: List[ExtractedFileInfo]\n",
    "    pdf_analysis: Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec6db75-7941-4b52-bc1e-0fcdfe00989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileExtractionAgent:\n",
    "    \"\"\"\n",
    "    Enhanced agent for extracting files from AI model responses with selective filtering.\n",
    "    Only extracts files specifically mentioned in each model's response.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_output_dir: str = \"extracted_files\"):\n",
    "        \"\"\"Initialize the enhanced file extraction agent.\"\"\"\n",
    "        self.base_output_dir = Path(base_output_dir)\n",
    "        self.model_a_dir = self.base_output_dir / \"model_A_extracted\"\n",
    "        self.model_b_dir = self.base_output_dir / \"model_B_extracted\"\n",
    "\n",
    "        # Supported file extensions (same as evaluator_builder)\n",
    "        self.supported_extensions = {\n",
    "            \".py\": \"python\",\n",
    "            \".js\": \"javascript\",\n",
    "            \".ts\": \"typescript\",\n",
    "            \".java\": \"java\",\n",
    "            \".cpp\": \"cpp\",\n",
    "            \".c\": \"c\",\n",
    "            \".cs\": \"csharp\",\n",
    "            \".go\": \"go\",\n",
    "            \".rs\": \"rust\",\n",
    "            \".php\": \"php\",\n",
    "            \".rb\": \"ruby\",\n",
    "            \".swift\": \"swift\",\n",
    "            \".kt\": \"kotlin\",\n",
    "            \".scala\": \"scala\",\n",
    "            \".html\": \"html\",\n",
    "            \".css\": \"css\",\n",
    "            \".sql\": \"sql\",\n",
    "            \".sh\": \"shell\",\n",
    "            \".yaml\": \"yaml\",\n",
    "            \".yml\": \"yaml\",\n",
    "            \".json\": \"json\",\n",
    "            \".xml\": \"xml\",\n",
    "            \".md\": \"markdown\",\n",
    "            \".txt\": \"text\",\n",
    "            \".conf\": \"config\",\n",
    "            \".ini\": \"config\",\n",
    "            \".log\": \"text\",\n",
    "        }\n",
    "\n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        # Create output directories\n",
    "        self._setup_directories()\n",
    "\n",
    "    def _setup_directories(self):\n",
    "        \"\"\"Create necessary output directories.\"\"\"\n",
    "        self.model_a_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.model_b_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.logger.info(\n",
    "            f\"Created output directories: {self.model_a_dir}, {self.model_b_dir}\"\n",
    "        )\n",
    "\n",
    "    def _calculate_file_hash(self, content: str) -> str:\n",
    "        \"\"\"Calculate MD5 hash of file content.\"\"\"\n",
    "        return hashlib.md5(content.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    def _get_file_type(self, file_path: str) -> str:\n",
    "        \"\"\"Determine file type from extension.\"\"\"\n",
    "        ext = Path(file_path).suffix.lower()\n",
    "        return self.supported_extensions.get(ext, \"text\")\n",
    "\n",
    "    def _get_timestamp(self) -> str:\n",
    "        \"\"\"Get current timestamp.\"\"\"\n",
    "        return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
    "\n",
    "    def parse_pdf_responses_enhanced(self, pdf_path: str) -> Dict:\n",
    "        \"\"\"Enhanced PDF parsing with better model differentiation.\"\"\"\n",
    "        pdf_path = Path(pdf_path)\n",
    "        if not pdf_path.exists():\n",
    "            raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "\n",
    "        self.logger.info(f\"Parsing PDF: {pdf_path}\")\n",
    "\n",
    "        # Extract text from PDF\n",
    "        pdf_text = \"\"\n",
    "        try:\n",
    "            with open(pdf_path, \"rb\") as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                for page_num, page in enumerate(pdf_reader.pages):\n",
    "                    page_text = page.extract_text()\n",
    "                    pdf_text += f\"\\n--- PAGE {page_num + 1} ---\\n\" + page_text\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error reading PDF: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Split into logical sections for better analysis\n",
    "        sections = self._split_into_logical_sections(pdf_text)\n",
    "\n",
    "        # Analyze each section for model indicators\n",
    "        model_analysis = self._analyze_sections_for_models(sections)\n",
    "\n",
    "        return {\n",
    "            \"raw_text\": pdf_text,\n",
    "            \"sections\": sections,\n",
    "            \"model_analysis\": model_analysis,\n",
    "            \"file_references\": self._extract_all_file_references(pdf_text),\n",
    "            \"model_sections\": self._map_files_to_models(model_analysis),\n",
    "        }\n",
    "\n",
    "    def _split_into_logical_sections(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Split text into logical sections based on headers, breaks, etc.\"\"\"\n",
    "        sections = []\n",
    "        lines = text.split(\"\\n\")\n",
    "        current_section = {\"lines\": [], \"start_line\": 0}\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            line_stripped = line.strip()\n",
    "\n",
    "            # Detect section breaks\n",
    "            is_section_break = (\n",
    "                re.match(r\"^#{1,6}\\s+\", line_stripped)  # Markdown headers\n",
    "                or re.match(r\"^[A-Z\\s]{5,}$\", line_stripped)  # ALL CAPS headers\n",
    "                or re.match(r\"^[\\*\\-=]{5,}$\", line_stripped)  # Separator lines\n",
    "                or re.match(\n",
    "                    r\"^(RESPONSE|MODEL|ASSISTANT|ANSWER)\\s*[AB12]?[:\\.]\",\n",
    "                    line_stripped,\n",
    "                    re.IGNORECASE,\n",
    "                )\n",
    "                or re.match(\n",
    "                    r\"^(MODELO|RESPUESTA|ASISTENTE)\\s*[AB12]?[:\\.]\",\n",
    "                    line_stripped,\n",
    "                    re.IGNORECASE,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if is_section_break and len(current_section[\"lines\"]) > 3:\n",
    "                # End current section\n",
    "                current_section[\"end_line\"] = i\n",
    "                current_section[\"text\"] = \"\\n\".join(current_section[\"lines\"])\n",
    "                sections.append(current_section)\n",
    "\n",
    "                # Start new section\n",
    "                current_section = {\"lines\": [line], \"start_line\": i}\n",
    "            else:\n",
    "                current_section[\"lines\"].append(line)\n",
    "\n",
    "        # Add final section\n",
    "        if current_section[\"lines\"]:\n",
    "            current_section[\"end_line\"] = len(lines)\n",
    "            current_section[\"text\"] = \"\\n\".join(current_section[\"lines\"])\n",
    "            sections.append(current_section)\n",
    "\n",
    "        self.logger.info(f\"Split PDF into {len(sections)} logical sections\")\n",
    "        return sections\n",
    "\n",
    "    def _analyze_sections_for_models(self, sections: List[Dict]) -> Dict:\n",
    "        \"\"\"Analyze each section to determine which model it belongs to.\"\"\"\n",
    "        model_sections = {\"A\": [], \"B\": [], \"unknown\": []}\n",
    "\n",
    "        # Enhanced patterns for model detection\n",
    "        model_a_indicators = [\n",
    "            r\"(?:^|\\W)(?:model|assistant|response|option|implementation|solution)\\s*a(?:\\W|$)\",\n",
    "            r\"(?:^|\\W)(?:modelo|asistente|respuesta|opci√≥n|implementaci√≥n|soluci√≥n)\\s*a(?:\\W|$)\",\n",
    "            r\"(?:^|\\W)first\\s+(?:model|assistant|response|option|implementation)\",\n",
    "            r\"(?:^|\\W)primer(?:o|a)?\\s+(?:modelo|asistente|respuesta|opci√≥n)\",\n",
    "            r\"(?:^|\\W)a[:\\.\\s]\\s*(?:model|assistant|implementation)\",\n",
    "            r\"\\ba\\)\\s*(?:model|assistant|implementation|solution)\",\n",
    "            r\"approach\\s*a\\b\",\n",
    "            r\"version\\s*a\\b\",\n",
    "        ]\n",
    "\n",
    "        model_b_indicators = [\n",
    "            r\"(?:^|\\W)(?:model|assistant|response|option|implementation|solution)\\s*b(?:\\W|$)\",\n",
    "            r\"(?:^|\\W)(?:modelo|asistente|respuesta|opci√≥n|implementaci√≥n|soluci√≥n)\\s*b(?:\\W|$)\",\n",
    "            r\"(?:^|\\W)second\\s+(?:model|assistant|response|option|implementation)\",\n",
    "            r\"(?:^|\\W)segundo?\\s+(?:modelo|asistente|respuesta|opci√≥n)\",\n",
    "            r\"(?:^|\\W)b[:\\.\\s]\\s*(?:model|assistant|implementation)\",\n",
    "            r\"\\bb\\)\\s*(?:model|assistant|implementation|solution)\",\n",
    "            r\"approach\\s*b\\b\",\n",
    "            r\"version\\s*b\\b\",\n",
    "        ]\n",
    "\n",
    "        for section in sections:\n",
    "            text_lower = section[\"text\"].lower()\n",
    "            section_id = (\n",
    "                len(model_sections[\"A\"])\n",
    "                + len(model_sections[\"B\"])\n",
    "                + len(model_sections[\"unknown\"])\n",
    "            )\n",
    "\n",
    "            # Score each section for model A and B\n",
    "            score_a = sum(\n",
    "                len(re.findall(pattern, text_lower, re.IGNORECASE))\n",
    "                for pattern in model_a_indicators\n",
    "            )\n",
    "            score_b = sum(\n",
    "                len(re.findall(pattern, text_lower, re.IGNORECASE))\n",
    "                for pattern in model_b_indicators\n",
    "            )\n",
    "\n",
    "            # Assign to model based on scores\n",
    "            if score_a > score_b and score_a > 0:\n",
    "                model_sections[\"A\"].append(section)\n",
    "                self.logger.info(\n",
    "                    f\"Section {section_id} assigned to Model A (score: A={score_a}, B={score_b})\"\n",
    "                )\n",
    "            elif score_b > score_a and score_b > 0:\n",
    "                model_sections[\"B\"].append(section)\n",
    "                self.logger.info(\n",
    "                    f\"Section {section_id} assigned to Model B (score: A={score_a}, B={score_b})\"\n",
    "                )\n",
    "            else:\n",
    "                model_sections[\"unknown\"].append(section)\n",
    "                self.logger.debug(\n",
    "                    f\"Section {section_id} unassigned (score: A={score_a}, B={score_b})\"\n",
    "                )\n",
    "\n",
    "        return model_sections\n",
    "\n",
    "    def _map_files_to_models(self, model_analysis: Dict) -> Dict:\n",
    "        \"\"\"Map specific files mentioned to each model.\"\"\"\n",
    "        model_a_content = \"\\n\".join([s[\"text\"] for s in model_analysis[\"A\"]])\n",
    "        model_b_content = \"\\n\".join([s[\"text\"] for s in model_analysis[\"B\"]])\n",
    "\n",
    "        model_a_files = self._extract_file_references(model_a_content)\n",
    "        model_b_files = self._extract_file_references(model_b_content)\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"Model A files identified: {len(model_a_files)} - {model_a_files[:5]}\"\n",
    "        )\n",
    "        self.logger.info(\n",
    "            f\"Model B files identified: {len(model_b_files)} - {model_b_files[:5]}\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"model_a_content\": model_a_content,\n",
    "            \"model_b_content\": model_b_content,\n",
    "            \"model_a_files\": model_a_files,\n",
    "            \"model_b_files\": model_b_files,\n",
    "        }\n",
    "\n",
    "    def _extract_file_references(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract file references with improved patterns and code block detection.\"\"\"\n",
    "        found_files = set()\n",
    "\n",
    "        # Enhanced file patterns with word boundaries\n",
    "        file_patterns = [\n",
    "            r\"\\b[a-zA-Z_][a-zA-Z0-9_]*\\.py\\b\",\n",
    "            r\"\\b[a-zA-Z_][a-zA-Z0-9_]*\\.js\\b\",\n",
    "            r\"\\b[a-zA-Z_][a-zA-Z0-9_]*\\.html\\b\",\n",
    "            r\"\\b[a-zA-Z_][a-zA-Z0-9_]*\\.css\\b\",\n",
    "            r\"\\b[a-zA-Z_][a-zA-Z0-9_]*\\.json\\b\",\n",
    "            r\"\\b[a-zA-Z_][a-zA-Z0-9_]*\\.md\\b\",\n",
    "            r\"\\b[a-zA-Z_][a-zA-Z0-9_]*\\.yaml\\b\",\n",
    "            r\"\\b[a-zA-Z_][a-zA-Z0-9_]*\\.yml\\b\",\n",
    "            r\"\\b[a-zA-Z_][a-zA-Z0-9_]*\\.txt\\b\",\n",
    "            r\"\\b[a-zA-Z_][a-zA-Z0-9_]*\\.conf\\b\",\n",
    "            r\"\\b[a-zA-Z_][a-zA-Z0-9_]*\\.ini\\b\",\n",
    "            r\"\\b[a-zA-Z_][a-zA-Z0-9_]*\\.log\\b\",\n",
    "        ]\n",
    "\n",
    "        # First, extract from regular text\n",
    "        for pattern in file_patterns:\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            found_files.update(matches)\n",
    "\n",
    "        # Extract from code blocks specifically\n",
    "        code_blocks = re.findall(r\"```[\\w]*\\n(.*?)\\n```\", text, re.DOTALL)\n",
    "        for block in code_blocks:\n",
    "            for pattern in file_patterns:\n",
    "                matches = re.findall(pattern, block, re.IGNORECASE)\n",
    "                found_files.update(matches)\n",
    "\n",
    "        # Look for file paths in quotes or backticks\n",
    "        quoted_patterns = [\n",
    "            r'[\"`\\']([a-zA-Z_][a-zA-Z0-9_/]*\\.[a-zA-Z]{1,5})[\"`\\']',\n",
    "            r\"File:\\s*([a-zA-Z_][a-zA-Z0-9_/]*\\.[a-zA-Z]{1,5})\",\n",
    "            r\"filename:\\s*([a-zA-Z_][a-zA-Z0-9_/]*\\.[a-zA-Z]{1,5})\",\n",
    "        ]\n",
    "\n",
    "        for pattern in quoted_patterns:\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            found_files.update(matches)\n",
    "\n",
    "        # Filter out common false positives\n",
    "        filtered_files = []\n",
    "        for file_ref in found_files:\n",
    "            # Skip overly generic names or obvious false positives\n",
    "            if not any(\n",
    "                skip in file_ref.lower()\n",
    "                for skip in [\"example.\", \"test.\", \"sample.\", \"dummy.\"]\n",
    "            ):\n",
    "                filtered_files.append(file_ref)\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"Found {len(filtered_files)} file references: {filtered_files[:10]}\"\n",
    "        )\n",
    "        return sorted(list(set(filtered_files)))\n",
    "\n",
    "    def _extract_all_file_references(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract all file references from the entire text.\"\"\"\n",
    "        return self._extract_file_references(text)\n",
    "\n",
    "    def extract_files_selectively(\n",
    "        self, source_dir: str, target_dir: str, model_id: str, required_files: List[str]\n",
    "    ) -> List[ExtractedFileInfo]:\n",
    "        \"\"\"Extract ONLY the files that are specifically mentioned in the model's response.\"\"\"\n",
    "        source_path = Path(source_dir)\n",
    "        target_path = Path(target_dir)\n",
    "\n",
    "        if not source_path.exists():\n",
    "            self.logger.error(f\"Source directory not found: {source_path}\")\n",
    "            return []\n",
    "\n",
    "        if not required_files:\n",
    "            self.logger.warning(f\"No required files specified for Model {model_id}\")\n",
    "            return []\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"Looking for {len(required_files)} specific files for Model {model_id}\"\n",
    "        )\n",
    "        self.logger.info(f\"Required files: {required_files}\")\n",
    "\n",
    "        target_path.mkdir(parents=True, exist_ok=True)\n",
    "        extracted_files = []\n",
    "        files_found = set()\n",
    "\n",
    "        # Find all archive files\n",
    "        archive_files = self._find_archive_files(source_path)\n",
    "\n",
    "        if not archive_files:\n",
    "            self.logger.error(f\"No archive files found in {source_path}\")\n",
    "            return []\n",
    "\n",
    "        # Search for required files in each archive\n",
    "        for archive_file in archive_files:\n",
    "            try:\n",
    "                found_in_archive = self._extract_specific_files_from_archive(\n",
    "                    archive_file, target_path, model_id, required_files\n",
    "                )\n",
    "                extracted_files.extend(found_in_archive)\n",
    "                files_found.update([f.original_path for f in found_in_archive])\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to process {archive_file}: {e}\")\n",
    "\n",
    "        # Report on missing files\n",
    "        missing_files = set(required_files) - files_found\n",
    "        if missing_files:\n",
    "            self.logger.warning(\n",
    "                f\"Model {model_id}: Could not find {len(missing_files)} required files: {list(missing_files)[:5]}\"\n",
    "            )\n",
    "        else:\n",
    "            self.logger.info(\n",
    "                f\"Model {model_id}: Successfully found all {len(required_files)} required files\"\n",
    "            )\n",
    "\n",
    "        return extracted_files\n",
    "\n",
    "    def _find_archive_files(self, source_path: Path) -> List[Path]:\n",
    "        \"\"\"Find all archive files in the source directory.\"\"\"\n",
    "        archive_extensions = [\".tar\", \".tar.gz\", \".tar.bz2\", \".tar.xz\", \".zip\", \".7z\"]\n",
    "        archive_files = []\n",
    "\n",
    "        for ext in archive_extensions:\n",
    "            found = list(source_path.glob(f\"*{ext}\"))\n",
    "            archive_files.extend(found)\n",
    "            if found:\n",
    "                self.logger.info(f\"Found {len(found)} files with extension {ext}\")\n",
    "\n",
    "        # Also check for files without clear extensions that might be archives\n",
    "        for file_path in source_path.iterdir():\n",
    "            if file_path.is_file() and not any(\n",
    "                str(file_path).endswith(ext) for ext in archive_extensions\n",
    "            ):\n",
    "                # Try to identify by content\n",
    "                try:\n",
    "                    with open(file_path, \"rb\") as f:\n",
    "                        header = f.read(10)\n",
    "                        # Check magic numbers for common archive formats\n",
    "                        if header.startswith(b\"PK\"):  # ZIP\n",
    "                            archive_files.append(file_path)\n",
    "                            self.logger.info(\n",
    "                                f\"Detected ZIP archive by magic number: {file_path}\"\n",
    "                            )\n",
    "                        elif header.startswith(b\"\\x1f\\x8b\"):  # GZIP\n",
    "                            archive_files.append(file_path)\n",
    "                            self.logger.info(\n",
    "                                f\"Detected GZIP archive by magic number: {file_path}\"\n",
    "                            )\n",
    "                except Exception as e:\n",
    "                    self.logger.debug(f\"Could not read {file_path}: {e}\")\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"Total archive files found in {source_path}: {len(archive_files)}\"\n",
    "        )\n",
    "        for archive in archive_files:\n",
    "            self.logger.info(f\"  - {archive.name} ({archive.stat().st_size} bytes)\")\n",
    "\n",
    "        return archive_files\n",
    "\n",
    "    def _extract_specific_files_from_archive(\n",
    "        self,\n",
    "        archive_path: Path,\n",
    "        target_dir: Path,\n",
    "        model_id: str,\n",
    "        required_files: List[str],\n",
    "    ) -> List[ExtractedFileInfo]:\n",
    "        \"\"\"Extract only specific files from an archive.\"\"\"\n",
    "        extracted_files = []\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"Searching {archive_path.name} for {len(required_files)} specific files\"\n",
    "        )\n",
    "\n",
    "        if archive_path.suffix == \".zip\":\n",
    "            with zipfile.ZipFile(archive_path, \"r\") as zip_ref:\n",
    "                available_files = zip_ref.namelist()\n",
    "\n",
    "                for required_file in required_files:\n",
    "                    # Try exact match first\n",
    "                    matching_files = self._find_matching_files(\n",
    "                        required_file, available_files\n",
    "                    )\n",
    "\n",
    "                    for match in matching_files:\n",
    "                        try:\n",
    "                            # Extract only this specific file\n",
    "                            extracted_path = target_dir / match\n",
    "                            extracted_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                            zip_ref.extract(match, target_dir)\n",
    "\n",
    "                            # Create FileInfo\n",
    "                            file_info = self._create_file_info(\n",
    "                                extracted_path, match, model_id\n",
    "                            )\n",
    "                            if file_info:\n",
    "                                extracted_files.append(file_info)\n",
    "                                self.logger.info(f\"‚úÖ Extracted: {match}\")\n",
    "\n",
    "                        except Exception as e:\n",
    "                            self.logger.error(f\"Failed to extract {match}: {e}\")\n",
    "\n",
    "        elif archive_path.suffix in [\".tar\", \".tar.gz\", \".tar.bz2\"]:\n",
    "            mode = self._get_tar_mode(archive_path)\n",
    "\n",
    "            with tarfile.open(archive_path, mode) as tar_ref:\n",
    "                available_files = [m.name for m in tar_ref.getmembers() if m.isfile()]\n",
    "\n",
    "                for required_file in required_files:\n",
    "                    matching_files = self._find_matching_files(\n",
    "                        required_file, available_files\n",
    "                    )\n",
    "\n",
    "                    for match in matching_files:\n",
    "                        try:\n",
    "                            # Extract only this specific file\n",
    "                            extracted_path = target_dir / match\n",
    "                            extracted_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                            member = tar_ref.getmember(match)\n",
    "                            tar_ref.extract(member, target_dir)\n",
    "\n",
    "                            # Create FileInfo\n",
    "                            file_info = self._create_file_info(\n",
    "                                extracted_path, match, model_id\n",
    "                            )\n",
    "                            if file_info:\n",
    "                                extracted_files.append(file_info)\n",
    "                                self.logger.info(f\"‚úÖ Extracted: {match}\")\n",
    "\n",
    "                        except Exception as e:\n",
    "                            self.logger.error(f\"Failed to extract {match}: {e}\")\n",
    "\n",
    "        return extracted_files\n",
    "\n",
    "    def _get_tar_mode(self, archive_path: Path) -> str:\n",
    "        \"\"\"Get the appropriate mode for opening tar archives.\"\"\"\n",
    "        if archive_path.suffix == \".tar.gz\":\n",
    "            return \"r:gz\"\n",
    "        elif archive_path.suffix == \".tar.bz2\":\n",
    "            return \"r:bz2\"\n",
    "        elif archive_path.suffix == \".tar.xz\":\n",
    "            return \"r:xz\"\n",
    "        else:\n",
    "            return \"r\"\n",
    "\n",
    "    def _find_matching_files(\n",
    "        self, required_file: str, available_files: List[str]\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Find files in archive that match the required file name.\"\"\"\n",
    "        matches = []\n",
    "\n",
    "        # Clean the required file name\n",
    "        required_clean = required_file.strip().replace(\"\\\\\", \"/\")\n",
    "\n",
    "        for available in available_files:\n",
    "            available_clean = available.replace(\"\\\\\", \"/\")\n",
    "\n",
    "            # Exact match\n",
    "            if available_clean == required_clean:\n",
    "                matches.append(available)\n",
    "                continue\n",
    "\n",
    "            # Basename match (file.py matches path/to/file.py)\n",
    "            if Path(available_clean).name == Path(required_clean).name:\n",
    "                matches.append(available)\n",
    "                continue\n",
    "\n",
    "            # Path ends with required file\n",
    "            if available_clean.endswith(required_clean):\n",
    "                matches.append(available)\n",
    "                continue\n",
    "\n",
    "        if matches:\n",
    "            self.logger.debug(\n",
    "                f\"Found {len(matches)} matches for '{required_file}': {matches}\"\n",
    "            )\n",
    "        else:\n",
    "            self.logger.debug(f\"No matches found for '{required_file}' in archive\")\n",
    "\n",
    "        return matches\n",
    "\n",
    "    def _create_file_info(\n",
    "        self, extracted_path: Path, original_path: str, model_id: str\n",
    "    ) -> Optional[ExtractedFileInfo]:\n",
    "        \"\"\"Create FileInfo object for successfully extracted file.\"\"\"\n",
    "        try:\n",
    "            if extracted_path.exists():\n",
    "                with open(extracted_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "\n",
    "                return ExtractedFileInfo(\n",
    "                    original_path=original_path,\n",
    "                    extracted_path=str(extracted_path),\n",
    "                    model_source=model_id,\n",
    "                    file_type=self._get_file_type(original_path),\n",
    "                    extraction_status=\"SUCCESS\",\n",
    "                    file_size=len(content),\n",
    "                    hash=self._calculate_file_hash(content),\n",
    "                )\n",
    "            else:\n",
    "                self.logger.error(f\"Extracted file not found: {extracted_path}\")\n",
    "                return None\n",
    "\n",
    "        except UnicodeDecodeError:\n",
    "            # Handle binary files\n",
    "            file_size = extracted_path.stat().st_size if extracted_path.exists() else 0\n",
    "            return ExtractedFileInfo(\n",
    "                original_path=original_path,\n",
    "                extracted_path=str(extracted_path),\n",
    "                model_source=model_id,\n",
    "                file_type=self._get_file_type(original_path),\n",
    "                extraction_status=\"SUCCESS (BINARY)\",\n",
    "                file_size=file_size,\n",
    "                hash=\"\",\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating file info for {extracted_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def organize_extracted_files(self, files: List[ExtractedFileInfo]) -> Dict:\n",
    "        \"\"\"Organize extracted files and generate summary.\"\"\"\n",
    "        model_a_files = [f for f in files if f.model_source == \"A\"]\n",
    "        model_b_files = [f for f in files if f.model_source == \"B\"]\n",
    "\n",
    "        successful_a = [\n",
    "            f for f in model_a_files if f.extraction_status.startswith(\"SUCCESS\")\n",
    "        ]\n",
    "        successful_b = [\n",
    "            f for f in model_b_files if f.extraction_status.startswith(\"SUCCESS\")\n",
    "        ]\n",
    "\n",
    "        failed_a = [\n",
    "            f for f in model_a_files if not f.extraction_status.startswith(\"SUCCESS\")\n",
    "        ]\n",
    "        failed_b = [\n",
    "            f for f in model_b_files if not f.extraction_status.startswith(\"SUCCESS\")\n",
    "        ]\n",
    "\n",
    "        # Group by file type\n",
    "        file_types_a = {}\n",
    "        file_types_b = {}\n",
    "\n",
    "        for f in successful_a:\n",
    "            if f.file_type not in file_types_a:\n",
    "                file_types_a[f.file_type] = []\n",
    "            file_types_a[f.file_type].append(f)\n",
    "\n",
    "        for f in successful_b:\n",
    "            if f.file_type not in file_types_b:\n",
    "                file_types_b[f.file_type] = []\n",
    "            file_types_b[f.file_type].append(f)\n",
    "\n",
    "        return {\n",
    "            \"summary\": {\n",
    "                \"total_files\": len(files),\n",
    "                \"model_a_files\": len(model_a_files),\n",
    "                \"model_b_files\": len(model_b_files),\n",
    "                \"successful_extractions\": len(successful_a) + len(successful_b),\n",
    "                \"failed_extractions\": len(failed_a) + len(failed_b),\n",
    "            },\n",
    "            \"model_a\": {\n",
    "                \"successful\": successful_a,\n",
    "                \"failed\": failed_a,\n",
    "                \"by_type\": file_types_a,\n",
    "            },\n",
    "            \"model_b\": {\n",
    "                \"successful\": successful_b,\n",
    "                \"failed\": failed_b,\n",
    "                \"by_type\": file_types_b,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def convert_to_file_content(\n",
    "        self, extracted_files: List[ExtractedFileInfo]\n",
    "    ) -> List[FileContent]:\n",
    "        \"\"\"Convert ExtractedFileInfo to FileContent for compatibility with evaluator_builder.\"\"\"\n",
    "        file_contents = []\n",
    "\n",
    "        for file_info in extracted_files:\n",
    "            if file_info.extraction_status.startswith(\"SUCCESS\") and file_info.hash:\n",
    "                try:\n",
    "                    with open(file_info.extracted_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        content = f.read()\n",
    "\n",
    "                    file_content = FileContent(\n",
    "                        path=file_info.original_path,\n",
    "                        content=content,\n",
    "                        file_type=file_info.file_type,\n",
    "                        hash=file_info.hash,\n",
    "                    )\n",
    "                    file_contents.append(file_content)\n",
    "\n",
    "                except Exception as e:\n",
    "                    self.logger.error(\n",
    "                        f\"Failed to convert {file_info.extracted_path}: {e}\"\n",
    "                    )\n",
    "\n",
    "        return file_contents\n",
    "\n",
    "    def generate_extraction_report(\n",
    "        self, pdf_analysis: Dict, organization: Dict, output_path: Optional[str] = None\n",
    "    ) -> ExtractionReport:\n",
    "        \"\"\"Generate comprehensive extraction report.\"\"\"\n",
    "\n",
    "        all_files = []\n",
    "        if organization.get(\"model_a\", {}).get(\"successful\"):\n",
    "            all_files.extend(organization[\"model_a\"][\"successful\"])\n",
    "        if organization.get(\"model_a\", {}).get(\"failed\"):\n",
    "            all_files.extend(organization[\"model_a\"][\"failed\"])\n",
    "        if organization.get(\"model_b\", {}).get(\"successful\"):\n",
    "            all_files.extend(organization[\"model_b\"][\"successful\"])\n",
    "        if organization.get(\"model_b\", {}).get(\"failed\"):\n",
    "            all_files.extend(organization[\"model_b\"][\"failed\"])\n",
    "\n",
    "        failed_extractions = [\n",
    "            f.original_path\n",
    "            for f in all_files\n",
    "            if not f.extraction_status.startswith(\"SUCCESS\")\n",
    "        ]\n",
    "\n",
    "        report = ExtractionReport(\n",
    "            timestamp=self._get_timestamp(),\n",
    "            total_files_extracted=organization[\"summary\"][\"successful_extractions\"],\n",
    "            model_a_files=len(organization[\"model_a\"][\"successful\"]),\n",
    "            model_b_files=len(organization[\"model_b\"][\"successful\"]),\n",
    "            failed_extractions=failed_extractions,\n",
    "            extracted_files=all_files,\n",
    "            pdf_analysis=pdf_analysis,\n",
    "        )\n",
    "\n",
    "        # Save report if path provided\n",
    "        if output_path:\n",
    "            self._save_report(report, output_path)\n",
    "\n",
    "        return report\n",
    "\n",
    "    def _save_report(self, report: ExtractionReport, output_path: str):\n",
    "        \"\"\"Save extraction report to file.\"\"\"\n",
    "        report_data = {\n",
    "            \"timestamp\": report.timestamp,\n",
    "            \"summary\": {\n",
    "                \"total_files_extracted\": report.total_files_extracted,\n",
    "                \"model_a_files\": report.model_a_files,\n",
    "                \"model_b_files\": report.model_b_files,\n",
    "                \"failed_extractions_count\": len(report.failed_extractions),\n",
    "            },\n",
    "            \"failed_extractions\": report.failed_extractions,\n",
    "            \"pdf_analysis\": report.pdf_analysis,\n",
    "            \"extracted_files\": [\n",
    "                {\n",
    "                    \"original_path\": f.original_path,\n",
    "                    \"extracted_path\": f.extracted_path,\n",
    "                    \"model_source\": f.model_source,\n",
    "                    \"file_type\": f.file_type,\n",
    "                    \"extraction_status\": f.extraction_status,\n",
    "                    \"file_size\": f.file_size,\n",
    "                    \"hash\": f.hash,\n",
    "                }\n",
    "                for f in report.extracted_files\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(report_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        self.logger.info(f\"Extraction report saved to: {output_path}\")\n",
    "\n",
    "    def run_selective_extraction(\n",
    "        self, pdf_path: str, model_a_archives_dir: str, model_b_archives_dir: str\n",
    "    ) -> Tuple[List[FileContent], List[FileContent], ExtractionReport]:\n",
    "        \"\"\"Run extraction process with selective file filtering based on PDF analysis.\"\"\"\n",
    "        self.logger.info(\"Starting selective extraction process...\")\n",
    "\n",
    "        # Enhanced PDF parsing\n",
    "        pdf_analysis = self.parse_pdf_responses_enhanced(pdf_path)\n",
    "\n",
    "        # Get the files specifically mentioned for each model\n",
    "        model_sections = pdf_analysis[\"model_sections\"]\n",
    "        model_a_required_files = model_sections[\"model_a_files\"]\n",
    "        model_b_required_files = model_sections[\"model_b_files\"]\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"Model A requires {len(model_a_required_files)} specific files\"\n",
    "        )\n",
    "        self.logger.info(\n",
    "            f\"Model B requires {len(model_b_required_files)} specific files\"\n",
    "        )\n",
    "\n",
    "        if not model_a_required_files and not model_b_required_files:\n",
    "            self.logger.warning(\n",
    "                \"No specific files identified for either model - falling back to all file extraction\"\n",
    "            )\n",
    "            # Fallback to extracting all files mentioned in PDF\n",
    "            all_files = pdf_analysis[\"file_references\"]\n",
    "            model_a_required_files = all_files[\n",
    "                : len(all_files) // 2\n",
    "            ]  # Split arbitrarily\n",
    "            model_b_required_files = all_files[len(all_files) // 2 :]\n",
    "\n",
    "        # Selective extraction for each model\n",
    "        model_a_extracted = self.extract_files_selectively(\n",
    "            model_a_archives_dir, str(self.model_a_dir), \"A\", model_a_required_files\n",
    "        )\n",
    "\n",
    "        model_b_extracted = self.extract_files_selectively(\n",
    "            model_b_archives_dir, str(self.model_b_dir), \"B\", model_b_required_files\n",
    "        )\n",
    "\n",
    "        # Generate comprehensive report\n",
    "        all_extracted = model_a_extracted + model_b_extracted\n",
    "        organization = self.organize_extracted_files(all_extracted)\n",
    "\n",
    "        # Enhanced report with selective extraction info\n",
    "        enhanced_pdf_analysis = pdf_analysis.copy()\n",
    "        enhanced_pdf_analysis[\"selective_extraction\"] = {\n",
    "            \"model_a_required\": model_a_required_files,\n",
    "            \"model_b_required\": model_b_required_files,\n",
    "            \"model_a_found\": len(model_a_extracted),\n",
    "            \"model_b_found\": len(model_b_extracted),\n",
    "        }\n",
    "\n",
    "        report = self.generate_extraction_report(\n",
    "            enhanced_pdf_analysis,\n",
    "            organization,\n",
    "            str(self.base_output_dir / \"selective_extraction_report.json\"),\n",
    "        )\n",
    "\n",
    "        # Convert to FileContent\n",
    "        model_a_files = self.convert_to_file_content(model_a_extracted)\n",
    "        model_b_files = self.convert_to_file_content(model_b_extracted)\n",
    "\n",
    "        self.logger.info(f\"Selective extraction complete:\")\n",
    "        self.logger.info(\n",
    "            f\"  Model A: {len(model_a_files)}/{len(model_a_required_files)} files\"\n",
    "        )\n",
    "        self.logger.info(\n",
    "            f\"  Model B: {len(model_b_files)}/{len(model_b_required_files)} files\"\n",
    "        )\n",
    "\n",
    "        return model_a_files, model_b_files, report\n",
    "\n",
    "    def debug_extraction_setup(self) -> Dict:\n",
    "        \"\"\"Debug function to check extraction setup and environment.\"\"\"\n",
    "        debug_info = {\n",
    "            \"working_director\": str(Path.cwd()),\n",
    "            \"base_output_dir\": str(self.base_output_dir),\n",
    "            \"model_a_dir\": str(self.model_a_dir),\n",
    "            \"model_b_dir\": str(self.model_b_dir),\n",
    "            \"directories_exist\": {\n",
    "                \"base_output\": self.base_output_dir.exists(),\n",
    "                \"model_a\": self.model_a_dir.exists(),\n",
    "                \"model_b\": self.model_b_dir.exists(),\n",
    "            },\n",
    "            \"available_files_and_dirs\": [],\n",
    "        }\n",
    "\n",
    "        # List current directory contents\n",
    "        try:\n",
    "            for item in Path.cwd().iterdir():\n",
    "                item_info = {\n",
    "                    \"name\": item.name,\n",
    "                    \"type\": \"directory\" if item.is_dir() else \"file\",\n",
    "                    \"path\": str(item),\n",
    "                }\n",
    "                if item.is_dir():\n",
    "                    try:\n",
    "                        contents = [f.name for f in item.iterdir()]\n",
    "                        item_info[\"contents\"] = contents[:10]  # First 10 items\n",
    "                    except:\n",
    "                        item_info[\"contents\"] = [\"<access denied>\"]\n",
    "                debug_info[\"available_files_and_dirs\"].append(item_info)\n",
    "        except Exception as e:\n",
    "            debug_info[\"directory_listing_error\"] = str(e)\n",
    "\n",
    "        return debug_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcabb9a-f739-4200-84a6-648cd85cd9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_selective_extraction():\n",
    "    \"\"\"Main function to run selective extraction.\"\"\"\n",
    "\n",
    "    # Initialize the agent\n",
    "    agent = FileExtractionAgent(base_output_dir=\"extracted_files\")\n",
    "\n",
    "    # Run selective extraction\n",
    "    try:\n",
    "        model_a_files, model_b_files, report = agent.run_selective_extraction(\n",
    "            pdf_path=\"conversation.pdf\",\n",
    "            model_a_archives_dir=\"files_model_A\",\n",
    "            model_b_archives_dir=\"files_model_B\",\n",
    "        )\n",
    "\n",
    "        print(f\"\\nüéØ SELECTIVE EXTRACTION COMPLETED!\")\n",
    "        print(f\"üìÅ Model A Files: {len(model_a_files)}\")\n",
    "        print(f\"üìÅ Model B Files: {len(model_b_files)}\")\n",
    "        print(f\"üìä Total Extractions: {report.total_files_extracted}\")\n",
    "        print(f\"‚ùå Failed Extractions: {len(report.failed_extractions)}\")\n",
    "        print(f\"üìã Report saved to: extracted_files/selective_extraction_report.json\")\n",
    "\n",
    "        # The files are now ready for use with evaluator_builder\n",
    "        print(f\"\\n‚úÖ Files ready for evaluation with CodeReviewerAgent!\")\n",
    "\n",
    "        return model_a_files, model_b_files, report\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Extraction failed: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144a8c74-0dea-4cc6-a1a2-21639f3b75f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main_selective_extraction()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
