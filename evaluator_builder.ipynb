{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415da323-0a70-4ef2-ace6-cb41e7f34a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-community langchain-core openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f5292",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from dataclasses import dataclass\n",
    "import difflib\n",
    "from enum import Enum\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6ba301-33d6-4263-baa9-5fcb94287e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelChoice(Enum):\n",
    "    \"\"\"Enum for model selection results.\"\"\"\n",
    "    MODEL_A = \"model_a\"\n",
    "    MODEL_B = \"model_b\"\n",
    "    NEITHER = \"neither\"\n",
    "    BOTH_GOOD = \"both_good\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcff9637-7068-4cf9-b2e8-1738361274e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FileContent:\n",
    "    \"\"\"Represents a file with its content and metadata.\"\"\"\n",
    "    path: str\n",
    "    content: str\n",
    "    file_type: str\n",
    "    hash: str\n",
    "\n",
    "@dataclass\n",
    "class EvaluatorPrompt:\n",
    "    \"\"\"Structure for the original evaluator prompt and requirements.\"\"\"\n",
    "    original_prompt: str\n",
    "    requirements: List[str] = None\n",
    "    success_criteria: List[str] = None\n",
    "    priority_aspects: List[str] = None\n",
    "\n",
    "@dataclass\n",
    "class ModelComparisonResult:\n",
    "    \"\"\"Result of comparing two model implementations.\"\"\"\n",
    "    chosen_model: ModelChoice\n",
    "    confidence_score: float\n",
    "    reasoning: str\n",
    "    pros_model_a: List[str]\n",
    "    cons_model_a: List[str]\n",
    "    pros_model_b: List[str]\n",
    "    cons_model_b: List[str]\n",
    "    detailed_analysis: str\n",
    "\n",
    "@dataclass\n",
    "class RefinedMultiModelAnalysisRequest:\n",
    "    \"\"\"Enhanced request structure focusing on evaluator requirements.\"\"\"\n",
    "    evaluator_prompt: EvaluatorPrompt\n",
    "    current_files: List[FileContent]\n",
    "    model_a_files: List[FileContent]\n",
    "    model_b_files: List[FileContent]\n",
    "    analysis_type: str = \"requirement_focused\"\n",
    "    custom_evaluation_criteria: List[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809d33b7-30e9-4e8d-b9cf-93bfb2e4909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeReviewerAgent:\n",
    "    \"\"\"\n",
    "    Enhanced agent for performing structured requirement-focused code analysis and comparison.\n",
    "    Evaluates two AI model implementations against original user requirements with structured output.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model: str = \"gpt-4o-mini\", temperature: float = 0):\n",
    "        \"\"\"Initialize the code reviewer agent.\"\"\"\n",
    "        self.llm = ChatOpenAI(model=model, temperature=temperature, api_key=api_key)\n",
    "        self.supported_extensions = {\n",
    "            '.py': 'python',\n",
    "            '.js': 'javascript', \n",
    "            '.ts': 'typescript',\n",
    "            '.java': 'java',\n",
    "            '.cpp': 'cpp',\n",
    "            '.c': 'c',\n",
    "            '.cs': 'csharp',\n",
    "            '.go': 'go',\n",
    "            '.rs': 'rust',\n",
    "            '.php': 'php',\n",
    "            '.rb': 'ruby',\n",
    "            '.swift': 'swift',\n",
    "            '.kt': 'kotlin',\n",
    "            '.scala': 'scala',\n",
    "            '.html': 'html',\n",
    "            '.css': 'css',\n",
    "            '.sql': 'sql',\n",
    "            '.sh': 'shell',\n",
    "            '.yaml': 'yaml',\n",
    "            '.yml': 'yaml',\n",
    "            '.json': 'json',\n",
    "            '.xml': 'xml',\n",
    "            '.md': 'markdown'\n",
    "        }\n",
    "\n",
    "    def _calculate_file_hash(self, content: str) -> str:\n",
    "        \"\"\"Calculate MD5 hash of file content.\"\"\"\n",
    "        return hashlib.md5(content.encode('utf-8')).hexdigest()\n",
    "\n",
    "    def _get_file_type(self, file_path: str) -> str:\n",
    "        \"\"\"Determine file type from extension.\"\"\"\n",
    "        ext = Path(file_path).suffix.lower()\n",
    "        return self.supported_extensions.get(ext, 'text')\n",
    "\n",
    "    def _get_timestamp(self) -> str:\n",
    "        \"\"\"Get current timestamp.\"\"\"\n",
    "        return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
    "\n",
    "    def load_files_from_directory(self, directory_path: str) -> List[FileContent]:\n",
    "        \"\"\"\n",
    "        Load all supported files from a directory.\n",
    "        \n",
    "        Args:\n",
    "            directory_path: Path to the directory containing files\n",
    "            \n",
    "        Returns:\n",
    "            List of FileContent objects\n",
    "        \"\"\"\n",
    "        files = []\n",
    "        directory = Path(directory_path)\n",
    "        \n",
    "        if not directory.exists():\n",
    "            raise FileNotFoundError(f\"Directory not found: {directory_path}\")\n",
    "        \n",
    "        # Recursively find all files\n",
    "        for file_path in directory.rglob('*'):\n",
    "            if file_path.is_file() and file_path.suffix.lower() in self.supported_extensions:\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                    \n",
    "                    # Create relative path from directory\n",
    "                    relative_path = str(file_path.relative_to(directory))\n",
    "                    \n",
    "                    file_content = FileContent(\n",
    "                        path=relative_path,\n",
    "                        content=content,\n",
    "                        file_type=self._get_file_type(str(file_path)),\n",
    "                        hash=self._calculate_file_hash(content)\n",
    "                    )\n",
    "                    files.append(file_content)\n",
    "                    \n",
    "                except (UnicodeDecodeError, PermissionError) as e:\n",
    "                    print(f\"Warning: Could not read file {file_path}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        return files\n",
    "\n",
    "    def _generate_file_diff(self, file_a: FileContent, file_b: FileContent, \n",
    "                          label_a: str = \"Version A\", label_b: str = \"Version B\") -> str:\n",
    "        \"\"\"Generate unified diff between two files.\"\"\"\n",
    "        lines_a = file_a.content.splitlines(keepends=True)\n",
    "        lines_b = file_b.content.splitlines(keepends=True)\n",
    "        \n",
    "        diff = difflib.unified_diff(\n",
    "            lines_a,\n",
    "            lines_b,\n",
    "            fromfile=f\"{label_a}/{file_a.path}\",\n",
    "            tofile=f\"{label_b}/{file_b.path}\",\n",
    "            lineterm=''\n",
    "        )\n",
    "        \n",
    "        return ''.join(diff)\n",
    "\n",
    "    def _extract_requirements_from_prompt(self, prompt: str) -> EvaluatorPrompt:\n",
    "        \"\"\"\n",
    "        Extract requirements and success criteria from the evaluator prompt.\n",
    "        \n",
    "        Args:\n",
    "            prompt: Original evaluator prompt\n",
    "            \n",
    "        Returns:\n",
    "            EvaluatorPrompt with extracted requirements\n",
    "        \"\"\"\n",
    "        extraction_prompt = f\"\"\"\n",
    "        Analyze the following prompt and extract the key requirements and success criteria:\n",
    "\n",
    "        PROMPT: \"{prompt}\"\n",
    "\n",
    "        Please identify:\n",
    "        1. Main functional requirements (what the code should do)\n",
    "        2. Technical requirements (language, frameworks, specific approaches)\n",
    "        3. Quality requirements (performance, security, maintainability)\n",
    "        4. Success criteria (how to measure if the implementation is successful)\n",
    "\n",
    "        Format your response as:\n",
    "        **FUNCTIONAL_REQUIREMENTS:**\n",
    "        - [requirement 1]\n",
    "        - [requirement 2]\n",
    "\n",
    "        **TECHNICAL_REQUIREMENTS:**\n",
    "        - [requirement 1]\n",
    "        - [requirement 2]\n",
    "\n",
    "        **QUALITY_REQUIREMENTS:**\n",
    "        - [requirement 1]\n",
    "        - [requirement 2]\n",
    "\n",
    "        **SUCCESS_CRITERIA:**\n",
    "        - [criteria 1]\n",
    "        - [criteria 2]\n",
    "\n",
    "        **PRIORITY_ASPECTS:**\n",
    "        - [most important aspect 1]\n",
    "        - [most important aspect 2]\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            messages = [\n",
    "                SystemMessage(content=\"You are an expert requirements analyst. Extract clear, actionable requirements from prompts.\"),\n",
    "                HumanMessage(content=extraction_prompt)\n",
    "            ]\n",
    "            \n",
    "            response = self.llm.invoke(messages)\n",
    "            \n",
    "            # Parse the response\n",
    "            requirements = []\n",
    "            success_criteria = []\n",
    "            priority_aspects = []\n",
    "            \n",
    "            lines = response.content.split('\\n')\n",
    "            current_section = None\n",
    "            \n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if \"**FUNCTIONAL_REQUIREMENTS:**\" in line:\n",
    "                    current_section = \"functional\"\n",
    "                elif \"**TECHNICAL_REQUIREMENTS:**\" in line:\n",
    "                    current_section = \"technical\"\n",
    "                elif \"**QUALITY_REQUIREMENTS:**\" in line:\n",
    "                    current_section = \"quality\"\n",
    "                elif \"**SUCCESS_CRITERIA:**\" in line:\n",
    "                    current_section = \"success\"\n",
    "                elif \"**PRIORITY_ASPECTS:**\" in line:\n",
    "                    current_section = \"priority\"\n",
    "                elif line.startswith(\"- \") and current_section:\n",
    "                    item = line[2:].strip()\n",
    "                    if current_section in [\"functional\", \"technical\", \"quality\"]:\n",
    "                        requirements.append(f\"[{current_section.upper()}] {item}\")\n",
    "                    elif current_section == \"success\":\n",
    "                        success_criteria.append(item)\n",
    "                    elif current_section == \"priority\":\n",
    "                        priority_aspects.append(item)\n",
    "            \n",
    "            return EvaluatorPrompt(\n",
    "                original_prompt=prompt,\n",
    "                requirements=requirements if requirements else [f\"Fulfill the request: {prompt}\"],\n",
    "                success_criteria=success_criteria if success_criteria else [\"Code works as requested\", \"Follows best practices\"],\n",
    "                priority_aspects=priority_aspects if priority_aspects else [\"Correctness\", \"Code quality\"]\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not extract requirements automatically: {e}\")\n",
    "            return EvaluatorPrompt(\n",
    "                original_prompt=prompt,\n",
    "                requirements=[f\"Fulfill the request: {prompt}\"],\n",
    "                success_criteria=[\"Code works as requested\", \"Follows best practices\"],\n",
    "                priority_aspects=[\"Correctness\", \"Code quality\"]\n",
    "            )\n",
    "\n",
    "    def _get_requirement_focused_system_message(self, analysis_type: str) -> str:\n",
    "        \"\"\"Get refined system message for requirement-focused evaluation with specific output format.\"\"\"\n",
    "        \n",
    "        return \"\"\"\n",
    "        You are an expert code evaluator specializing in requirement compliance assessment and technical analysis.\n",
    "        \n",
    "        Your PRIMARY goal is to determine which AI model implementation better fulfills the original evaluator request.\n",
    "        \n",
    "        EVALUATION PRIORITY ORDER:\n",
    "        1. **Requirement Fulfillment** - Does it do what was asked?\n",
    "        2. **Correctness** - Does the code work as intended?\n",
    "        3. **Completeness** - Does it address all aspects of the request?\n",
    "        4. **Code Quality** - Is it well-written and maintainable?\n",
    "        5. **Best Practices** - Does it follow good coding standards?\n",
    "        \n",
    "        REQUIRED OUTPUT FORMAT:\n",
    "        You must provide your response in this EXACT structure with NO additional sections:\n",
    "        \n",
    "        ## ðŸŽ¯ Evaluation Result: ðŸ† [MODEL_A/MODEL_B] (Winner)\n",
    "        \n",
    "        ## ðŸ“‹ Original Evaluator Request\n",
    "        > \"[original request text]\"\n",
    "        \n",
    "        ## ðŸ“Š File Analysis Summary\n",
    "        - **Current Files:** [number]\n",
    "        - **Model A Files:** [number] \n",
    "        - **Model B Files:** [number]\n",
    "        - **Total Unique Files:** [number]\n",
    "        \n",
    "        ## âœ… Why [CHOSEN_MODEL] is the Superior Implementation\n",
    "        [Write a technical argumentation paragraph of 200-300 words explaining why the chosen model offers the best option for the implementation requested in the user's prompt. Use technical concepts to justify this choice. Focus on how well it fulfills the original requirements, code quality, best practices, and technical soundness.]\n",
    "        \n",
    "        ## âŒ Why [REJECTED_MODEL] is Inferior\n",
    "        [Write a technical argumentation paragraph of 100-200 words explaining why the rejected model is inferior using technical concepts. Focus on specific technical shortcomings, requirement gaps, or code quality issues.]\n",
    "        \n",
    "        **Issue Type:** [technical_inconsistency | tool | code_correctness | setup | other]\n",
    "        \n",
    "        ## ðŸ“ˆ Technical Assessment Scores\n",
    "        \n",
    "        **Scoring Rules:**\n",
    "        - Numbers 0-3: Positive score for Model A (0 = best, 2-3 = not good enough or equal quality)\n",
    "        - Numbers 4-7: Positive score for Model B (7 = best, 4-5 = not good enough or equal quality)\n",
    "        \n",
    "        **Model A Scores:**\n",
    "        - interaction_rating: [1-7]\n",
    "        - code_logic: [1-7]\n",
    "        - naming_clarity: [1-7]\n",
    "        - organization_modularity: [1-7]\n",
    "        - interface_design: [1-7]\n",
    "        - error_handling: [1-7]\n",
    "        - documentation: [1-7]\n",
    "        - review_readiness: [1-7]\n",
    "        \n",
    "        **Model B Scores:**\n",
    "        - interaction_rating: [1-7]\n",
    "        - code_logic: [1-7]\n",
    "        - naming_clarity: [1-7]\n",
    "        - organization_modularity: [1-7]\n",
    "        - interface_design: [1-7]\n",
    "        - error_handling: [1-7]\n",
    "        - documentation: [1-7]\n",
    "        - review_readiness: [1-7]\n",
    "        \n",
    "        ## ðŸ”„ Next Improvement Prompt\n",
    "        [Generate a well-scoped follow-up prompt focused on improving the implementation without increasing task difficulty or adding new requirements. The prompt should be oriented toward code review improvements and maintaining the original scope. Think of this as a detailed Jira ticket for a mid-level engineer that focuses on refinement rather than expansion.]\n",
    "        \n",
    "        EVALUATION CRITERIA DEFINITIONS:\n",
    "        - **interaction_rating**: How well does the model engage with the problem? Does it explore edge cases, explain decisions, and allow for user iteration?\n",
    "        - **code_logic**: Is the code logically correct, efficient, and follows best practices? Are there bugs or performance issues?\n",
    "        - **naming_clarity**: Are variable, function, and class names descriptive, intuitive, and consistent?\n",
    "        - **organization_modularity**: Is the code well-structured, modular, readable, and maintainable?\n",
    "        - **interface_design**: Are user interfaces clear, usable, and appropriate for the task?\n",
    "        - **error_handling**: Does the code handle invalid inputs and edge cases gracefully with appropriate validation?\n",
    "        - **documentation**: Are comments and documentation useful, concise, and focused on non-obvious aspects?\n",
    "        - **review_readiness**: Is the code ready for production-level pull request review with consistent style?\n",
    "        \n",
    "        ISSUE TYPE DEFINITIONS:\n",
    "        - **technical_inconsistency**: Code has inconsistent patterns, conflicting approaches, or technical contradictions\n",
    "        - **tool**: Incorrect or inappropriate use of tools, libraries, or frameworks\n",
    "        - **code_correctness**: Logical errors, bugs, or incorrect implementation that prevents proper functionality\n",
    "        - **setup**: Problems with configuration, environment setup, or deployment-related issues\n",
    "        - **other**: Issues that don't fit the above categories but represent clear technical problems\n",
    "        \n",
    "        Be objective, thorough, and provide clear technical reasoning. Focus on how well each implementation serves the original purpose while maintaining high code quality standards.\n",
    "        \"\"\"\n",
    "\n",
    "    def _create_requirement_focused_analysis_prompt(self, request: RefinedMultiModelAnalysisRequest) -> str:\n",
    "        \"\"\"\n",
    "        Create a refined prompt focused on structured evaluation output.\n",
    "        \"\"\"\n",
    "        evaluator_prompt = request.evaluator_prompt\n",
    "        \n",
    "        # Create file mappings\n",
    "        current_files_map = {f.path: f for f in request.current_files}\n",
    "        model_a_files_map = {f.path: f for f in request.model_a_files}\n",
    "        model_b_files_map = {f.path: f for f in request.model_b_files}\n",
    "        \n",
    "        # Get all unique file paths\n",
    "        all_file_paths = set(current_files_map.keys()) | set(model_a_files_map.keys()) | set(model_b_files_map.keys())\n",
    "        \n",
    "        prompt_parts = [\n",
    "            \"# Structured Code Implementation Evaluation\",\n",
    "            \n",
    "            f\"\\n## Original Evaluator Request:\",\n",
    "            f'\"{evaluator_prompt.original_prompt}\"',\n",
    "            \n",
    "            \"\\n## Your Mission:\",\n",
    "            \"Evaluate two AI model implementations and determine which better fulfills the original request.\",\n",
    "            \"Provide a structured response following the EXACT format specified in your system message.\",\n",
    "            \n",
    "            \"\\n## File Analysis Data:\",\n",
    "            f\"- Current Files: {len(request.current_files)}\",\n",
    "            f\"- Model A Files: {len(request.model_a_files)}\",\n",
    "            f\"- Model B Files: {len(request.model_b_files)}\",\n",
    "            f\"- Total Unique Files: {len(all_file_paths)}\",\n",
    "            \n",
    "            \"\\n## Code Implementations to Evaluate:\",\n",
    "        ]\n",
    "        \n",
    "        # Show code files\n",
    "        for file_path in sorted(all_file_paths):\n",
    "            current_file = current_files_map.get(file_path)\n",
    "            model_a_file = model_a_files_map.get(file_path)\n",
    "            model_b_file = model_b_files_map.get(file_path)\n",
    "            \n",
    "            prompt_parts.append(f\"\\n### File: {file_path}\")\n",
    "            \n",
    "            # Current version\n",
    "            if current_file:\n",
    "                prompt_parts.extend([\n",
    "                    f\"\\n#### CURRENT VERSION ({current_file.file_type}):\",\n",
    "                    f\"```{current_file.file_type}\",\n",
    "                    current_file.content,\n",
    "                    \"```\"\n",
    "                ])\n",
    "            else:\n",
    "                prompt_parts.append(\"\\n#### CURRENT VERSION: *File does not exist*\")\n",
    "            \n",
    "            # Model A version\n",
    "            if model_a_file:\n",
    "                prompt_parts.extend([\n",
    "                    f\"\\n#### MODEL A IMPLEMENTATION ({model_a_file.file_type}):\",\n",
    "                    f\"```{model_a_file.file_type}\",\n",
    "                    model_a_file.content,\n",
    "                    \"```\"\n",
    "                ])\n",
    "            else:\n",
    "                prompt_parts.append(\"\\n#### MODEL A IMPLEMENTATION: *File does not exist*\")\n",
    "            \n",
    "            # Model B version\n",
    "            if model_b_file:\n",
    "                prompt_parts.extend([\n",
    "                    f\"\\n#### MODEL B IMPLEMENTATION ({model_b_file.file_type}):\",\n",
    "                    f\"```{model_b_file.file_type}\",\n",
    "                    model_b_file.content,\n",
    "                    \"```\"\n",
    "                ])\n",
    "            else:\n",
    "                prompt_parts.append(\"\\n#### MODEL B IMPLEMENTATION: *File does not exist*\")\n",
    "        \n",
    "        prompt_parts.extend([\n",
    "            \"\\n## IMPORTANT:\",\n",
    "            \"Follow the EXACT output format specified in your system message.\",\n",
    "            \"Include all required sections in the specified order.\",\n",
    "            \"Provide technical justifications for your evaluations.\",\n",
    "            \"Generate scores according to the specified scoring rules.\",\n",
    "            \"Create a meaningful next improvement prompt.\"\n",
    "        ])\n",
    "        \n",
    "        return \"\\n\".join(prompt_parts)\n",
    "\n",
    "    def _parse_structured_comparison_result(self, response_content: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Parse the structured LLM response into organized data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize result structure\n",
    "        result = {\n",
    "            \"chosen_model\": \"neither\",\n",
    "            \"confidence_score\": 5.0,\n",
    "            \"original_request\": \"\",\n",
    "            \"file_summary\": {},\n",
    "            \"winner_justification\": \"\",\n",
    "            \"loser_critique\": \"\",\n",
    "            \"issue_type\": \"other\",\n",
    "            \"model_a_scores\": {},\n",
    "            \"model_b_scores\": {},\n",
    "            \"next_prompt\": \"\",\n",
    "            \"raw_response\": response_content\n",
    "        }\n",
    "        \n",
    "        lines = response_content.split('\\n')\n",
    "        current_section = None\n",
    "        current_content = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line_stripped = line.strip()\n",
    "            \n",
    "            # Detect sections\n",
    "            if \"ðŸŽ¯ Evaluation Result:\" in line and \"ðŸ†\" in line:\n",
    "                if \"MODEL_A\" in line.upper():\n",
    "                    result[\"chosen_model\"] = \"model_a\"\n",
    "                elif \"MODEL_B\" in line.upper():\n",
    "                    result[\"chosen_model\"] = \"model_b\"\n",
    "                current_section = \"evaluation_result\"\n",
    "                \n",
    "            elif \"ðŸ“‹ Original Evaluator Request\" in line:\n",
    "                current_section = \"original_request\"\n",
    "                \n",
    "            elif \"ðŸ“Š File Analysis Summary\" in line:\n",
    "                current_section = \"file_summary\"\n",
    "                \n",
    "            elif \"âœ… Why\" in line and \"Superior Implementation\" in line:\n",
    "                current_section = \"winner_justification\"\n",
    "                current_content = []\n",
    "                \n",
    "            elif \"âŒ Why\" in line and \"Inferior\" in line:\n",
    "                current_section = \"loser_critique\"\n",
    "                current_content = []\n",
    "                \n",
    "            elif \"**Issue Type:**\" in line:\n",
    "                issue_type_line = line.replace(\"**Issue Type:**\", \"\").strip()\n",
    "                for issue_type in [\"technical_inconsistency\", \"tool\", \"code_correctness\", \"setup\", \"other\"]:\n",
    "                    if issue_type in issue_type_line:\n",
    "                        result[\"issue_type\"] = issue_type\n",
    "                        break\n",
    "                current_section = \"issue_type\"\n",
    "                \n",
    "            elif \"ðŸ“ˆ Technical Assessment Scores\" in line:\n",
    "                current_section = \"scores\"\n",
    "                \n",
    "            elif \"**Model A Scores:**\" in line:\n",
    "                current_section = \"model_a_scores\"\n",
    "                \n",
    "            elif \"**Model B Scores:**\" in line:\n",
    "                current_section = \"model_b_scores\"\n",
    "                \n",
    "            elif \"ðŸ”„ Next Improvement Prompt\" in line:\n",
    "                current_section = \"next_prompt\"\n",
    "                current_content = []\n",
    "                \n",
    "            # Parse content based on current section\n",
    "            elif current_section == \"original_request\" and line_stripped.startswith(\">\"):\n",
    "                result[\"original_request\"] = line_stripped[1:].strip().strip('\"')\n",
    "                \n",
    "            elif current_section == \"file_summary\" and line_stripped.startswith(\"- **\"):\n",
    "                if \"Current Files:\" in line:\n",
    "                    try:\n",
    "                        result[\"file_summary\"][\"current\"] = int(line_stripped.split(\":\")[-1].strip())\n",
    "                    except:\n",
    "                        pass\n",
    "                elif \"Model A Files:\" in line:\n",
    "                    try:\n",
    "                        result[\"file_summary\"][\"model_a\"] = int(line_stripped.split(\":\")[-1].strip())\n",
    "                    except:\n",
    "                        pass\n",
    "                elif \"Model B Files:\" in line:\n",
    "                    try:\n",
    "                        result[\"file_summary\"][\"model_b\"] = int(line_stripped.split(\":\")[-1].strip())\n",
    "                    except:\n",
    "                        pass\n",
    "                elif \"Total Unique Files:\" in line:\n",
    "                    try:\n",
    "                        result[\"file_summary\"][\"total\"] = int(line_stripped.split(\":\")[-1].strip())\n",
    "                    except:\n",
    "                        pass\n",
    "                        \n",
    "            elif current_section == \"winner_justification\" and line_stripped and not line_stripped.startswith(\"##\"):\n",
    "                current_content.append(line_stripped)\n",
    "                \n",
    "            elif current_section == \"loser_critique\" and line_stripped and not line_stripped.startswith(\"##\") and not line_stripped.startswith(\"**Issue Type:**\"):\n",
    "                current_content.append(line_stripped)\n",
    "                \n",
    "            elif current_section in [\"model_a_scores\", \"model_b_scores\"] and \":\" in line_stripped and line_stripped.startswith(\"- \"):\n",
    "                try:\n",
    "                    score_line = line_stripped[2:].strip()  # Remove \"- \"\n",
    "                    score_name, score_value = score_line.split(\":\", 1)\n",
    "                    score_name = score_name.strip()\n",
    "                    score_value = score_value.strip().strip(\"[]\")\n",
    "                    \n",
    "                    # Extract numeric score\n",
    "                    score_num = None\n",
    "                    for char in score_value:\n",
    "                        if char.isdigit():\n",
    "                            score_num = int(char)\n",
    "                            break\n",
    "                    \n",
    "                    if score_num and current_section == \"model_a_scores\":\n",
    "                        result[\"model_a_scores\"][score_name] = score_num\n",
    "                    elif score_num and current_section == \"model_b_scores\":\n",
    "                        result[\"model_b_scores\"][score_name] = score_num\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "            elif current_section == \"next_prompt\" and line_stripped and not line_stripped.startswith(\"##\"):\n",
    "                current_content.append(line_stripped)\n",
    "        \n",
    "        # Join multi-line content\n",
    "        if current_section == \"winner_justification\":\n",
    "            result[\"winner_justification\"] = \" \".join(current_content).strip()\n",
    "        elif current_section == \"loser_critique\":\n",
    "            result[\"loser_critique\"] = \" \".join(current_content).strip()\n",
    "        elif current_section == \"next_prompt\":\n",
    "            result[\"next_prompt\"] = \" \".join(current_content).strip()\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def _parse_comparison_result(self, response_content: str) -> ModelComparisonResult:\n",
    "        \"\"\"Parse the LLM response into a structured result (legacy method for backward compatibility).\"\"\"\n",
    "        \n",
    "        # Initialize default values\n",
    "        chosen_model = ModelChoice.NEITHER\n",
    "        confidence_score = 5.0\n",
    "        reasoning = \"Unable to parse reasoning from response\"\n",
    "        pros_model_a = []\n",
    "        cons_model_a = []\n",
    "        pros_model_b = []\n",
    "        cons_model_b = []\n",
    "        detailed_analysis = response_content\n",
    "        \n",
    "        lines = response_content.split('\\n')\n",
    "        current_section = None\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Parse chosen model\n",
    "            if line.startswith(\"**CHOSEN MODEL:**\"):\n",
    "                model_text = line.replace(\"**CHOSEN MODEL:**\", \"\").strip().upper()\n",
    "                if \"MODEL_A\" in model_text:\n",
    "                    chosen_model = ModelChoice.MODEL_A\n",
    "                elif \"MODEL_B\" in model_text:\n",
    "                    chosen_model = ModelChoice.MODEL_B\n",
    "                elif \"BOTH_GOOD\" in model_text:\n",
    "                    chosen_model = ModelChoice.BOTH_GOOD\n",
    "                else:\n",
    "                    chosen_model = ModelChoice.NEITHER\n",
    "            \n",
    "            # Parse confidence score\n",
    "            elif line.startswith(\"**CONFIDENCE SCORE:**\") or line.startswith(\"**PRIMARY REASONING:**\"):\n",
    "                if \"CONFIDENCE SCORE\" in line:\n",
    "                    try:\n",
    "                        score_text = line.replace(\"**CONFIDENCE SCORE:**\", \"\").strip()\n",
    "                        confidence_score = float(score_text.split()[0])\n",
    "                    except:\n",
    "                        confidence_score = 5.0\n",
    "                elif \"PRIMARY REASONING\" in line:\n",
    "                    reasoning = line.replace(\"**PRIMARY REASONING:**\", \"\").strip()\n",
    "            \n",
    "            # Track sections\n",
    "            elif \"**MODEL A OVERALL ASSESSMENT:**\" in line:\n",
    "                current_section = \"model_a\"\n",
    "            elif \"**MODEL B OVERALL ASSESSMENT:**\" in line:\n",
    "                current_section = \"model_b\"\n",
    "            elif \"âœ… Strengths:\" in line:\n",
    "                current_section += \"_pros\"\n",
    "            elif \"âŒ Weaknesses:\" in line:\n",
    "                current_section += \"_cons\"\n",
    "            elif \"**DETAILED ANALYSIS:**\" in line:\n",
    "                current_section = \"detailed\"\n",
    "            \n",
    "            # Parse lists\n",
    "            elif line.startswith(\"- \") and current_section:\n",
    "                item = line[2:].strip()\n",
    "                if current_section == \"model_a_pros\":\n",
    "                    pros_model_a.append(item)\n",
    "                elif current_section == \"model_a_cons\":\n",
    "                    cons_model_a.append(item)\n",
    "                elif current_section == \"model_b_pros\":\n",
    "                    pros_model_b.append(item)\n",
    "                elif current_section == \"model_b_cons\":\n",
    "                    cons_model_b.append(item)\n",
    "        \n",
    "        return ModelComparisonResult(\n",
    "            chosen_model=chosen_model,\n",
    "            confidence_score=confidence_score,\n",
    "            reasoning=reasoning,\n",
    "            pros_model_a=pros_model_a,\n",
    "            cons_model_a=cons_model_a,\n",
    "            pros_model_b=pros_model_b,\n",
    "            cons_model_b=cons_model_b,\n",
    "            detailed_analysis=detailed_analysis\n",
    "        )\n",
    "\n",
    "    def evaluate_models_against_requirements(self, \n",
    "                                           evaluator_prompt_text: str,\n",
    "                                           current_files: List[FileContent],\n",
    "                                           model_a_files: List[FileContent],\n",
    "                                           model_b_files: List[FileContent],\n",
    "                                           custom_criteria: List[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate two model implementations with structured output parsing.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Extract requirements from the evaluator prompt\n",
    "        print(\"Extracting requirements from evaluator prompt...\")\n",
    "        evaluator_prompt = self._extract_requirements_from_prompt(evaluator_prompt_text)\n",
    "        \n",
    "        # Create refined analysis request\n",
    "        analysis_request = RefinedMultiModelAnalysisRequest(\n",
    "            evaluator_prompt=evaluator_prompt,\n",
    "            current_files=current_files,\n",
    "            model_a_files=model_a_files,\n",
    "            model_b_files=model_b_files,\n",
    "            analysis_type=\"requirement_focused\",\n",
    "            custom_evaluation_criteria=custom_criteria\n",
    "        )\n",
    "        \n",
    "        # Generate the analysis prompt\n",
    "        analysis_prompt = self._create_requirement_focused_analysis_prompt(analysis_request)\n",
    "        \n",
    "        # Get system message\n",
    "        system_message = self._get_requirement_focused_system_message(analysis_request.analysis_type)\n",
    "        \n",
    "        # Create LLM messages\n",
    "        messages = [\n",
    "            SystemMessage(content=system_message),\n",
    "            HumanMessage(content=analysis_prompt)\n",
    "        ]\n",
    "        \n",
    "        # Get response from LLM\n",
    "        print(\"Evaluating model implementations with structured output...\")\n",
    "        response = self.llm.invoke(messages)\n",
    "        \n",
    "        # Parse the structured response\n",
    "        parsed_evaluation = self._parse_structured_comparison_result(response.content)\n",
    "        \n",
    "        # Process response\n",
    "        analysis_result = {\n",
    "            \"evaluation_type\": \"structured_requirement_focused\",\n",
    "            \"evaluator_prompt\": {\n",
    "                \"original_text\": evaluator_prompt_text,\n",
    "                \"extracted_requirements\": evaluator_prompt.requirements,\n",
    "                \"success_criteria\": evaluator_prompt.success_criteria,\n",
    "                \"priority_aspects\": evaluator_prompt.priority_aspects\n",
    "            },\n",
    "            \"files_analyzed\": {\n",
    "                \"current_files\": len(current_files),\n",
    "                \"model_a_files\": len(model_a_files),\n",
    "                \"model_b_files\": len(model_b_files),\n",
    "                \"total_unique_files\": len(set(f.path for f in current_files) | \n",
    "                                        set(f.path for f in model_a_files) |\n",
    "                                        set(f.path for f in model_b_files))\n",
    "            },\n",
    "            \"parsed_evaluation\": parsed_evaluation,\n",
    "            \"raw_response\": response.content,\n",
    "            \"raw_prompt_sent\": analysis_prompt\n",
    "        }\n",
    "        \n",
    "        return analysis_result\n",
    "\n",
    "    def save_structured_evaluation_report(self, analysis_result: Dict, output_file: str):\n",
    "        \"\"\"\n",
    "        Save structured evaluation results to a file.\n",
    "        \"\"\"\n",
    "        output_path = Path(output_file)\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Generate structured markdown report\n",
    "        report_content = self._generate_structured_evaluation_report(analysis_result)\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(report_content)\n",
    "        \n",
    "        print(f\"Structured evaluation report saved to: {output_file}\")\n",
    "\n",
    "    def _generate_structured_evaluation_report(self, analysis_result: Dict) -> str:\n",
    "        \"\"\"Generate a structured markdown report from evaluation results.\"\"\"\n",
    "        \n",
    "        parsed_result = analysis_result.get(\"parsed_evaluation\", {})\n",
    "        \n",
    "        chosen_model = parsed_result.get(\"chosen_model\", \"neither\")\n",
    "        if chosen_model == \"model_a\":\n",
    "            winner_display = \"ðŸ† **MODEL A** (Winner)\"\n",
    "        elif chosen_model == \"model_b\":\n",
    "            winner_display = \"ðŸ† **MODEL B** (Winner)\"\n",
    "        else:\n",
    "            winner_display = \"âŒ **NO CLEAR WINNER**\"\n",
    "        \n",
    "        report_parts = [\n",
    "            \"# Structured Code Evaluation Report\",\n",
    "            f\"\\n**Evaluation Type:** Requirement-Focused Assessment\",\n",
    "            f\"\\n**Generated:** {self._get_timestamp()}\",\n",
    "            f\"\\n**Evaluator:** CAMB3LL\",\n",
    "            \n",
    "            f\"\\n## ðŸŽ¯ Evaluation Result: {winner_display}\",\n",
    "            \n",
    "            \"\\n## ðŸ“‹ Original Evaluator Request\",\n",
    "            f\"\\n> \\\"{parsed_result.get('original_request', 'Not captured')}\\\"\",\n",
    "            \n",
    "            \"\\n## ðŸ“Š File Analysis Summary\",\n",
    "            f\"- **Current Files:** {parsed_result.get('file_summary', {}).get('current', 'N/A')}\",\n",
    "            f\"- **Model A Files:** {parsed_result.get('file_summary', {}).get('model_a', 'N/A')}\",\n",
    "            f\"- **Model B Files:** {parsed_result.get('file_summary', {}).get('model_b', 'N/A')}\",\n",
    "            f\"- **Total Unique Files:** {parsed_result.get('file_summary', {}).get('total', 'N/A')}\",\n",
    "            \n",
    "            \"\\n## âœ… Winner Justification\",\n",
    "            f\"\\n{parsed_result.get('winner_justification', 'No justification provided')}\",\n",
    "            \n",
    "            \"\\n## âŒ Rejected Model Critique\",\n",
    "            f\"\\n{parsed_result.get('loser_critique', 'No critique provided')}\",\n",
    "            f\"\\n**Issue Type:** {parsed_result.get('issue_type', 'other')}\",\n",
    "            \n",
    "            \"\\n## ðŸ“ˆ Technical Assessment Scores\",\n",
    "            \"\\n### Model A Scores:\",\n",
    "        ]\n",
    "        \n",
    "        # Add Model A scores\n",
    "        model_a_scores = parsed_result.get('model_a_scores', {})\n",
    "        for metric, score in model_a_scores.items():\n",
    "            report_parts.append(f\"- **{metric}:** {score}/7\")\n",
    "        \n",
    "        report_parts.append(\"\\n### Model B Scores:\")\n",
    "        \n",
    "        # Add Model B scores  \n",
    "        model_b_scores = parsed_result.get('model_b_scores', {})\n",
    "        for metric, score in model_b_scores.items():\n",
    "            report_parts.append(f\"- **{metric}:** {score}/7\")\n",
    "        \n",
    "        report_parts.extend([\n",
    "            \"\\n## ðŸ”„ Next Improvement Prompt\",\n",
    "            f\"\\n{parsed_result.get('next_prompt', 'No follow-up prompt provided')}\",\n",
    "            \n",
    "            \"\\n---\",\n",
    "            \"\\n## ðŸ“ Raw LLM Response\",\n",
    "            \"\\n```markdown\",\n",
    "            parsed_result.get('raw_response', 'No raw response captured'),\n",
    "            \"\\n```\"\n",
    "        ])\n",
    "        \n",
    "        return \"\\n\".join(report_parts)\n",
    "\n",
    "    # Legacy methods for backward compatibility\n",
    "    def save_requirement_evaluation_report(self, analysis_result: Dict, output_file: str):\n",
    "        \"\"\"\n",
    "        Legacy method - redirects to structured evaluation report.\n",
    "        \"\"\"\n",
    "        return self.save_structured_evaluation_report(analysis_result, output_file)\n",
    "\n",
    "    def _generate_requirement_evaluation_report(self, analysis_result: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Legacy method - redirects to structured evaluation report.\n",
    "        \"\"\"\n",
    "        return self._generate_structured_evaluation_report(analysis_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe0f83-4c4d-4541-a46f-c30ba149ff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_structured_evaluation_example():\n",
    "    \"\"\"Example usage for structured requirement-focused evaluation.\"\"\"\n",
    "    \n",
    "    # Initialize the agent\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"Error: OPENAI_API_KEY environment variable not set\")\n",
    "        return\n",
    "    \n",
    "    agent = CodeReviewerAgent(api_key=api_key)\n",
    "    \n",
    "    # Example evaluator prompt\n",
    "    evaluator_prompt = \"Genera un cÃ³digo de Python para imprimir en consola 'Hello World' de la manera mÃ¡s eficiente y siguiendo las mejores prÃ¡cticas de Python\"\n",
    "    \n",
    "    # Example paths\n",
    "    current_code_dir = \"current_code\"\n",
    "    model_a_code_dir = \"model_generated_code_a\"\n",
    "    model_b_code_dir = \"model_generated_code_b\"\n",
    "    \n",
    "    try:\n",
    "        # Load files from all directories\n",
    "        print(\"Loading files for structured requirement evaluation...\")\n",
    "        current_files = agent.load_files_from_directory(current_code_dir)\n",
    "        model_a_files = agent.load_files_from_directory(model_a_code_dir)\n",
    "        model_b_files = agent.load_files_from_directory(model_b_code_dir)\n",
    "        \n",
    "        print(f\"Loaded {len(current_files)} current, {len(model_a_files)} Model A, {len(model_b_files)} Model B files\")\n",
    "        \n",
    "        # Perform structured requirement-focused evaluation\n",
    "        print(f\"\\nEvaluating implementations against: '{evaluator_prompt}'\")\n",
    "        results = agent.evaluate_models_against_requirements(\n",
    "            evaluator_prompt_text=evaluator_prompt,\n",
    "            current_files=current_files,\n",
    "            model_a_files=model_a_files,\n",
    "            model_b_files=model_b_files,\n",
    "            custom_criteria=[\"Code efficiency\", \"Python best practices\", \"Simplicity\"]\n",
    "        )\n",
    "        \n",
    "        # Save structured report\n",
    "        agent.save_structured_evaluation_report(results, \"evaluation_reports/structured_requirement_evaluation.md\")\n",
    "        \n",
    "        # Print structured summary\n",
    "        parsed_eval = results.get('parsed_evaluation', {})\n",
    "        chosen = parsed_eval.get('chosen_model', 'unknown')\n",
    "        next_prompt = parsed_eval.get('next_prompt', 'No follow-up prompt generated')\n",
    "        issue_type = parsed_eval.get('issue_type', 'other')\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ STRUCTURED EVALUATION RESULT:\")\n",
    "        print(f\"   Original Request: '{evaluator_prompt}'\")\n",
    "        print(f\"   Winner: {chosen.upper()}\")\n",
    "        print(f\"   Issue Type: {issue_type}\")\n",
    "        print(f\"   Next Improvement Prompt: {next_prompt}\")\n",
    "        print(\"\\nStructured requirement-focused evaluation completed successfully!\")\n",
    "        \n",
    "        # Print scores summary if available\n",
    "        model_a_scores = parsed_eval.get('model_a_scores', {})\n",
    "        model_b_scores = parsed_eval.get('model_b_scores', {})\n",
    "        \n",
    "        if model_a_scores or model_b_scores:\n",
    "            print(f\"\\nðŸ“ˆ SCORING SUMMARY:\")\n",
    "            print(f\"   Model A Scores: {len(model_a_scores)} metrics evaluated\")\n",
    "            print(f\"   Model B Scores: {len(model_b_scores)} metrics evaluated\")\n",
    "            \n",
    "            # Show average scores if available\n",
    "            if model_a_scores:\n",
    "                avg_a = sum(model_a_scores.values()) / len(model_a_scores)\n",
    "                print(f\"   Model A Average: {avg_a:.1f}/7\")\n",
    "            if model_b_scores:\n",
    "                avg_b = sum(model_b_scores.values()) / len(model_b_scores)\n",
    "                print(f\"   Model B Average: {avg_b:.1f}/7\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during structured evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "        main_structured_evaluation_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d3152-02ed-4ae5-9094-daa703431cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
