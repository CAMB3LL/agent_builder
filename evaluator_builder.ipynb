{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415da323-0a70-4ef2-ace6-cb41e7f34a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-community langchain-core openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c85f5292",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from dataclasses import dataclass\n",
    "import difflib\n",
    "from enum import Enum\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fd6ba301-33d6-4263-baa9-5fcb94287e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelChoice(Enum):\n",
    "    \"\"\"Enum for model selection results.\"\"\"\n",
    "\n",
    "    MODEL_A = \"model_a\"\n",
    "    MODEL_B = \"model_b\"\n",
    "    NEITHER = \"neither\"\n",
    "    BOTH_GOOD = \"both_good\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fcff9637-7068-4cf9-b2e8-1738361274e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FileContent:\n",
    "    \"\"\"Represents a file with its content and metadata.\"\"\"\n",
    "\n",
    "    path: str\n",
    "    content: str\n",
    "    file_type: str\n",
    "    hash: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvaluatorPrompt:\n",
    "    \"\"\"Structure for the original evaluator prompt and requirements.\"\"\"\n",
    "\n",
    "    original_prompt: str\n",
    "    task_description: str = None\n",
    "    requirements: List[str] = None\n",
    "    success_criteria: List[str] = None\n",
    "    priority_aspects: List[str] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelComparisonResult:\n",
    "    \"\"\"Result of comparing two model implementations.\"\"\"\n",
    "\n",
    "    chosen_model: ModelChoice\n",
    "    confidence_score: float\n",
    "    reasoning: str\n",
    "    pros_model_a: List[str]\n",
    "    cons_model_a: List[str]\n",
    "    pros_model_b: List[str]\n",
    "    cons_model_b: List[str]\n",
    "    detailed_analysis: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RefinedMultiModelAnalysisRequest:\n",
    "    \"\"\"Enhanced request structure focusing on evaluator requirements.\"\"\"\n",
    "\n",
    "    evaluator_prompt: EvaluatorPrompt\n",
    "    current_files: List[FileContent]\n",
    "    model_a_files: List[FileContent]\n",
    "    model_b_files: List[FileContent]\n",
    "    task_description: str = None\n",
    "    analysis_type: str = \"requirement_focused\"\n",
    "    custom_evaluation_criteria: List[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "809d33b7-30e9-4e8d-b9cf-93bfb2e4909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeReviewerAgent:\n",
    "    \"\"\"\n",
    "    Enhanced agent for performing structured requirement-focused code analysis and comparison.\n",
    "    Evaluates two AI model implementations against original user requirements with structured output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, api_key: str, model: str = \"gpt-4o-mini\", temperature: float = 0\n",
    "    ):\n",
    "        \"\"\"Initialize the code reviewer agent.\"\"\"\n",
    "        self.llm = ChatOpenAI(model=model, temperature=temperature, api_key=api_key)\n",
    "        self.supported_extensions = {\n",
    "            \".py\": \"python\",\n",
    "            \".js\": \"javascript\",\n",
    "            \".ts\": \"typescript\",\n",
    "            \".java\": \"java\",\n",
    "            \".cpp\": \"cpp\",\n",
    "            \".c\": \"c\",\n",
    "            \".cs\": \"csharp\",\n",
    "            \".go\": \"go\",\n",
    "            \".rs\": \"rust\",\n",
    "            \".php\": \"php\",\n",
    "            \".rb\": \"ruby\",\n",
    "            \".swift\": \"swift\",\n",
    "            \".kt\": \"kotlin\",\n",
    "            \".scala\": \"scala\",\n",
    "            \".html\": \"html\",\n",
    "            \".css\": \"css\",\n",
    "            \".sql\": \"sql\",\n",
    "            \".sh\": \"shell\",\n",
    "            \".yaml\": \"yaml\",\n",
    "            \".yml\": \"yaml\",\n",
    "            \".json\": \"json\",\n",
    "            \".xml\": \"xml\",\n",
    "            \".md\": \"markdown\",\n",
    "        }\n",
    "\n",
    "    def _calculate_file_hash(self, content: str) -> str:\n",
    "        \"\"\"Calculate MD5 hash of file content.\"\"\"\n",
    "        return hashlib.md5(content.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    def _get_file_type(self, file_path: str) -> str:\n",
    "        \"\"\"Determine file type from extension.\"\"\"\n",
    "        ext = Path(file_path).suffix.lower()\n",
    "        return self.supported_extensions.get(ext, \"text\")\n",
    "\n",
    "    def _get_timestamp(self) -> str:\n",
    "        \"\"\"Get current timestamp.\"\"\"\n",
    "        return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
    "\n",
    "    def load_files_from_directory(self, directory_path: str) -> List[FileContent]:\n",
    "        \"\"\"\n",
    "        Load all supported files from a directory.\n",
    "\n",
    "        Args:\n",
    "            directory_path: Path to the directory containing files\n",
    "\n",
    "        Returns:\n",
    "            List of FileContent objects\n",
    "        \"\"\"\n",
    "        files = []\n",
    "        directory = Path(directory_path)\n",
    "\n",
    "        if not directory.exists():\n",
    "            raise FileNotFoundError(f\"Directory not found: {directory_path}\")\n",
    "\n",
    "        # Recursively find all files\n",
    "        for file_path in directory.rglob(\"*\"):\n",
    "            if (\n",
    "                file_path.is_file()\n",
    "                and file_path.suffix.lower() in self.supported_extensions\n",
    "            ):\n",
    "                try:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        content = f.read()\n",
    "\n",
    "                    relative_path = str(file_path.relative_to(directory))\n",
    "\n",
    "                    file_content = FileContent(\n",
    "                        path=relative_path,\n",
    "                        content=content,\n",
    "                        file_type=self._get_file_type(str(file_path)),\n",
    "                        hash=self._calculate_file_hash(content),\n",
    "                    )\n",
    "                    files.append(file_content)\n",
    "\n",
    "                except (UnicodeDecodeError, PermissionError) as e:\n",
    "                    print(f\"Warning: Could not read file {file_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        return files\n",
    "\n",
    "    def _generate_file_diff(\n",
    "        self,\n",
    "        file_a: FileContent,\n",
    "        file_b: FileContent,\n",
    "        label_a: str = \"Version A\",\n",
    "        label_b: str = \"Version B\",\n",
    "    ) -> str:\n",
    "        \"\"\"Generate unified diff between two files.\"\"\"\n",
    "        lines_a = file_a.content.splitlines(keepends=True)\n",
    "        lines_b = file_b.content.splitlines(keepends=True)\n",
    "\n",
    "        diff = difflib.unified_diff(\n",
    "            lines_a,\n",
    "            lines_b,\n",
    "            fromfile=f\"{label_a}/{file_a.path}\",\n",
    "            tofile=f\"{label_b}/{file_b.path}\",\n",
    "            lineterm=\"\",\n",
    "        )\n",
    "\n",
    "        return \"\".join(diff)\n",
    "\n",
    "    def _extract_requirements_from_prompt(\n",
    "        self, prompt: str, task_description: str = None\n",
    "    ) -> EvaluatorPrompt:\n",
    "        \"\"\"\n",
    "        Extract requirements and success criteria from the evaluator prompt.\n",
    "\n",
    "        Args:\n",
    "            prompt: Original evaluator prompt\n",
    "            task_description: General task description for additional context\n",
    "\n",
    "        Returns:\n",
    "            EvaluatorPrompt with extracted requirements\n",
    "        \"\"\"\n",
    "\n",
    "        context_section = \"\"\n",
    "        if task_description:\n",
    "            context_section = f\"\"\"\n",
    "            \n",
    "            TASK CONTEXT: \"{task_description}\"\n",
    "            This context should inform the requirements extraction and help orient toward production-ready code quality.\n",
    "            \"\"\"\n",
    "\n",
    "        extraction_prompt = f\"\"\"\n",
    "        Analyze the following prompt and extract the key requirements and success criteria:\n",
    "    \n",
    "        PROMPT: \"{prompt}\"{context_section}\n",
    "    \n",
    "        Please identify:\n",
    "        1. Main functional requirements (what the code should do)\n",
    "        2. Technical requirements (language, frameworks, specific approaches)\n",
    "        3. Quality requirements (performance, security, maintainability, production-readiness)\n",
    "        4. Success criteria (how to measure if the implementation is successful)\n",
    "    \n",
    "        Format your response as:\n",
    "        \n",
    "        **BASE_PROMPT**\n",
    "        {prompt}\n",
    "        \n",
    "        **FUNCTIONAL_REQUIREMENTS:**\n",
    "        - [requirement 1]\n",
    "        - [requirement 2]\n",
    "    \n",
    "        **TECHNICAL_REQUIREMENTS:**\n",
    "        - [requirement 1]\n",
    "        - [requirement 2]\n",
    "    \n",
    "        **QUALITY_REQUIREMENTS:**\n",
    "        - [requirement 1]\n",
    "        - [requirement 2]\n",
    "    \n",
    "        **SUCCESS_CRITERIA:**\n",
    "        - [criteria 1]\n",
    "        - [criteria 2]\n",
    "    \n",
    "        **PRIORITY_ASPECTS:**\n",
    "        - [most important aspect 1]\n",
    "        - [most important aspect 2]\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            messages = [\n",
    "                SystemMessage(\n",
    "                    content=\"You are an expert requirements analyst. Extract clear, actionable requirements from prompts with focus on production-ready code quality.\"\n",
    "                ),\n",
    "                HumanMessage(content=extraction_prompt),\n",
    "            ]\n",
    "\n",
    "            response = self.llm.invoke(messages)\n",
    "\n",
    "            requirements = []\n",
    "            success_criteria = []\n",
    "            priority_aspects = []\n",
    "\n",
    "            lines = response.content.split(\"\\n\")\n",
    "            current_section = None\n",
    "\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if \"**FUNCTIONAL_REQUIREMENTS:**\" in line:\n",
    "                    current_section = \"functional\"\n",
    "                elif \"**TECHNICAL_REQUIREMENTS:**\" in line:\n",
    "                    current_section = \"technical\"\n",
    "                elif \"**QUALITY_REQUIREMENTS:**\" in line:\n",
    "                    current_section = \"quality\"\n",
    "                elif \"**SUCCESS_CRITERIA:**\" in line:\n",
    "                    current_section = \"success\"\n",
    "                elif \"**PRIORITY_ASPECTS:**\" in line:\n",
    "                    current_section = \"priority\"\n",
    "                elif line.startswith(\"- \") and current_section:\n",
    "                    item = line[2:].strip()\n",
    "                    if current_section in [\"functional\", \"technical\", \"quality\"]:\n",
    "                        requirements.append(f\"[{current_section.upper()}] {item}\")\n",
    "                    elif current_section == \"success\":\n",
    "                        success_criteria.append(item)\n",
    "                    elif current_section == \"priority\":\n",
    "                        priority_aspects.append(item)\n",
    "\n",
    "            return EvaluatorPrompt(\n",
    "                original_prompt=prompt,\n",
    "                task_description=task_description,\n",
    "                requirements=(\n",
    "                    requirements if requirements else [f\"Fulfill the request: {prompt}\"]\n",
    "                ),\n",
    "                success_criteria=(\n",
    "                    success_criteria\n",
    "                    if success_criteria\n",
    "                    else [\n",
    "                        \"Code works as requested\",\n",
    "                        \"Follows best practices\",\n",
    "                        \"Production-ready quality\",\n",
    "                    ]\n",
    "                ),\n",
    "                priority_aspects=(\n",
    "                    priority_aspects\n",
    "                    if priority_aspects\n",
    "                    else [\"Correctness\", \"Code quality\", \"Production-readiness\"]\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not extract requirements automatically: {e}\")\n",
    "            return EvaluatorPrompt(\n",
    "                original_prompt=prompt,\n",
    "                task_description=task_description,\n",
    "                requirements=[f\"Fulfill the request: {prompt}\"],\n",
    "                success_criteria=[\n",
    "                    \"Code works as requested\",\n",
    "                    \"Follows best practices\",\n",
    "                    \"Production-ready quality\",\n",
    "                ],\n",
    "                priority_aspects=[\n",
    "                    \"Correctness\",\n",
    "                    \"Code quality\",\n",
    "                    \"Production-readiness\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "    def _get_requirement_focused_system_message(self, analysis_type: str) -> str:\n",
    "        \"\"\"Get refined system message for requirement-focused evaluation with specific output format.\"\"\"\n",
    "\n",
    "        return \"\"\"\n",
    "        You are an expert software engineer/code evaluator specializing in requirement compliance assessment and technical analysis.\n",
    "        \n",
    "        Your PRIMARY goal is to determine which AI model implementation better fulfills the original evaluator request and moves closer to production-ready code quality.\n",
    "        \n",
    "        EVALUATION PRIORITY ORDER:\n",
    "        1. **interaction_rating** - How well does the model engage with the problem? Does it explore edge cases, explain its decisions, and allow for user iteration or clarification?\n",
    "        2. **code_logic** - Is the code logically correct, efficient, and in line with best practices? Are there bugs, performance issues, or flawed reasoning?\n",
    "        3. **naming_clarity** - Are the names of variables, functions, and classes descriptive, intuitive, and consistent with the task and codebase?\n",
    "        4. **organization_modularity** - Is the code well-structured and modular? Does it promote readability, reuse, and maintainability?\n",
    "        5. **interface_design (if applicable)** - Are any user interfaces clear, usable, and appropriate for the task?\n",
    "        6. **error_handling** - Does the code handle invalid inputs and edge cases gracefully? Is exception handling or validation logic appropriate and secure?\n",
    "        7. **documentation** - Are the comments and documentation useful, concise, and focused on non-obvious aspects? Avoid comments that merely restate what the code does or contain the model‚Äôs inner thought process.\n",
    "        8. **review_readiness** - Is the code ready for a pull request review? Does it reflect a clear, consistent style and adhere to the standards of a production-level contribution?\n",
    "        \n",
    "        REQUIRED OUTPUT FORMAT:\n",
    "        You must provide your response in this EXACT structure with NO additional sections:\n",
    "        \n",
    "        ## üéØ Evaluation Result: üèÜ [MODEL_A/MODEL_B] (Winner\n",
    "        \n",
    "        ## üìä File Analysis Summary\n",
    "        - **Current Files:** [number]\n",
    "        - **Model A Files:** [number] \n",
    "        - **Model B Files:** [number]\n",
    "        - **Total Unique Files:** [number]\n",
    "        \n",
    "        ## ‚úÖ Why [CHOSEN_MODEL] is the Superior Implementation\n",
    "        [Write a technical argumentation paragraph of 200-300 words explaining why the chosen model offers the best option for the implementation requested in the user's prompt. Focus on production-readiness, technical quality, and how it aligns with the overall task context.]\n",
    "        \n",
    "        ## ‚ùå Why [REJECTED_MODEL] is Inferior\n",
    "        [Write a technical argumentation paragraph of 100-200 words explaining why the rejected model is inferior using technical concepts. Focus on specific technical shortcomings, requirement gaps, and production-readiness concerns.]\n",
    "        \n",
    "        **Issue Type:** [technical_inconsistency | tool | code_correctness | setup | production_readiness | other]\n",
    "        \n",
    "        ## üìà Technical Assessment Scores\n",
    "        \n",
    "        **Scoring Rules:**\n",
    "\n",
    "        ***Scoring guideline***\n",
    "        - The scores must be assigned according to the BEST RESPONSE AND MODEL CHOOSEN. The analysis of the responses in such a way that these must reflect the choice of the model chosen for providing a better response.\n",
    "        \n",
    "        **Interaction Scores:**\n",
    "        - interaction_rating: [Excellent | Good | Fair  | Poor ]\n",
    "        - code_logic: [Excellent | Good | Fair  | Poor ]\n",
    "        - naming_clarity: [Excellent | Good | Fair  | Poor ]\n",
    "        - organization_modularity: [Excellent | Good | Fair  | Poor ] \n",
    "        - interface_design: [Excellent | Good | Fair  | Poor ]\n",
    "        - error_handling: [Excellent | Good | Fair  | Poor ]\n",
    "        - documentation: [Excellent | Good | Fair  | Poor ]\n",
    "        - review_readiness: [Excellent | Good | Fair  | Poor ]\n",
    "        \n",
    "        ## üîÑ Next Improvement Prompt\n",
    "        [Generate a well-scoped follow-up prompt focused on improving the implementation toward production-ready quality. Consider the task context and current code quality gaps. The prompt should be oriented toward code refinement, optimization or deployment readiness - whatever is most needed to reach production standards. Next prompt should be well scoped and singularly focused. A good heuristic here is the level of details you would provide in a Jira/Asana ticket to a mid-level engineer. You want enough detail that the task is clear, but you don't need to necessarily spell out every edge case and consideration. It will be related to code review and should not increase scope of the task. The next prompt never should be related to implement or update TEST or project documentation.]\n",
    "        \n",
    "        EVALUATION CRITERIA DEFINITIONS:\n",
    "        - **interaction_rating**: How well does the model engage with the problem? Does it explore edge cases, explain decisions, and allow for user iteration?\n",
    "        - **code_logic**: Is the code logically correct, efficient, and follows best practices? Are there bugs or performance issues?\n",
    "        - **naming_clarity**: Are variable, function, and class names descriptive, intuitive, and consistent?\n",
    "        - **organization_modularity**: Is the code well-structured, modular, readable, and maintainable?\n",
    "        - **interface_design**: Are user interfaces clear, usable, and appropriate for the task?\n",
    "        - **error_handling**: Does the code handle invalid inputs and edge cases gracefully with appropriate validation?\n",
    "        - **documentation**: Are comments and documentation useful, concise, and focused on non-obvious aspects?\n",
    "        - **production_readiness**: Is the code ready for deployment with proper configuration, security, and scalability considerations?\n",
    "        - **review_readiness**: Is the code ready for production-level pull request review with consistent style?\n",
    "        \n",
    "        ISSUE TYPE DEFINITIONS:\n",
    "        - **technical_inconsistency**: Code has inconsistent patterns, conflicting approaches, or technical contradictions\n",
    "        - **tool**: Incorrect or inappropriate use of tools, libraries, or frameworks\n",
    "        - **code_correctness**: Logical errors, bugs, or incorrect implementation that prevents proper functionality\n",
    "        - **setup**: Problems with configuration, environment setup, or deployment-related issues\n",
    "        - **production_readiness**: Code lacks necessary features for production deployment (logging, error handling, security, etc.)\n",
    "        - **other**: Issues that don't fit the above categories but represent clear technical problems\n",
    "        \n",
    "        IMPORTANT LIMITATIONS\n",
    "        - The Interaction Scores should reflect the final decision. Is not neccesary generate scores for each model, instead You must generate an overall score that demonstrates that choice according to the scoring rules. \n",
    "        - The next prompt never should be related to implement or update TEST or project documentation. \n",
    "        \n",
    "        Focus on how well each implementation serves the original purpose while progressing toward production-ready code that can be deployed in real-world scenarios.\n",
    "        \"\"\"\n",
    "\n",
    "    def _create_requirement_focused_analysis_prompt(\n",
    "        self, request: RefinedMultiModelAnalysisRequest\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Create a refined prompt focused on structured evaluation output.\n",
    "        \"\"\"\n",
    "        evaluator_prompt = request.evaluator_prompt\n",
    "\n",
    "        current_files_map = {f.path: f for f in request.current_files}\n",
    "        model_a_files_map = {f.path: f for f in request.model_a_files}\n",
    "        model_b_files_map = {f.path: f for f in request.model_b_files}\n",
    "\n",
    "        all_file_paths = (\n",
    "            set(current_files_map.keys())\n",
    "            | set(model_a_files_map.keys())\n",
    "            | set(model_b_files_map.keys())\n",
    "        )\n",
    "\n",
    "        prompt_parts = [\n",
    "            \"# Structured Code Implementation Evaluation\",\n",
    "            f\"\\n## Original Evaluator Request:\",\n",
    "            f'\"{evaluator_prompt.original_prompt}\"',\n",
    "        ]\n",
    "\n",
    "        if evaluator_prompt.task_description:\n",
    "            prompt_parts.extend(\n",
    "                [\n",
    "                    f\"\\n## Task Context:\",\n",
    "                    f'\"{evaluator_prompt.task_description}\"',\n",
    "                    \"\\nThis context should inform your evaluation and guide the next improvement prompt toward production-ready code quality.\",\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        prompt_parts.extend(\n",
    "            [\n",
    "                \"\\n## Your Mission:\",\n",
    "                \"Evaluate two AI model implementations and determine which better fulfills the original request while considering production-ready code quality.\",\n",
    "                \"Provide a structured response following the EXACT format specified in your system message.\",\n",
    "                \"Focus on how each implementation advances toward production deployment readiness.\",\n",
    "                \"\\n## File Analysis Data:\",\n",
    "                f\"- Current Files: {len(request.current_files)}\",\n",
    "                f\"- Model A Files: {len(request.model_a_files)}\",\n",
    "                f\"- Model B Files: {len(request.model_b_files)}\",\n",
    "                f\"- Total Unique Files: {len(all_file_paths)}\",\n",
    "                \"\\n## Code Implementations to Evaluate:\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for file_path in sorted(all_file_paths):\n",
    "            current_file = current_files_map.get(file_path)\n",
    "            model_a_file = model_a_files_map.get(file_path)\n",
    "            model_b_file = model_b_files_map.get(file_path)\n",
    "\n",
    "            prompt_parts.append(f\"\\n### File: {file_path}\")\n",
    "\n",
    "            # Current version\n",
    "            if current_file:\n",
    "                prompt_parts.extend(\n",
    "                    [\n",
    "                        f\"\\n#### CURRENT VERSION ({current_file.file_type}):\",\n",
    "                        f\"```{current_file.file_type}\",\n",
    "                        current_file.content,\n",
    "                        \"```\",\n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                prompt_parts.append(\"\\n#### CURRENT VERSION: *File does not exist*\")\n",
    "\n",
    "            # Model A version\n",
    "            if model_a_file:\n",
    "                prompt_parts.extend(\n",
    "                    [\n",
    "                        f\"\\n#### MODEL A IMPLEMENTATION ({model_a_file.file_type}):\",\n",
    "                        f\"```{model_a_file.file_type}\",\n",
    "                        model_a_file.content,\n",
    "                        \"```\",\n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                prompt_parts.append(\n",
    "                    \"\\n#### MODEL A IMPLEMENTATION: *File does not exist*\"\n",
    "                )\n",
    "\n",
    "            # Model B version\n",
    "            if model_b_file:\n",
    "                prompt_parts.extend(\n",
    "                    [\n",
    "                        f\"\\n#### MODEL B IMPLEMENTATION ({model_b_file.file_type}):\",\n",
    "                        f\"```{model_b_file.file_type}\",\n",
    "                        model_b_file.content,\n",
    "                        \"```\",\n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                prompt_parts.append(\n",
    "                    \"\\n#### MODEL B IMPLEMENTATION: *File does not exist*\"\n",
    "                )\n",
    "\n",
    "        prompt_parts.extend(\n",
    "            [\n",
    "                \"\\n## IMPORTANT:\",\n",
    "                \"Follow the EXACT output format specified in your system message.\",\n",
    "                \"Include all required sections in the specified order.\",\n",
    "                \"Provide technical justifications for your evaluations.\",\n",
    "                \"Generate scores according to the specified scoring rules.\",\n",
    "                \"Create a meaningful next improvement prompt oriented toward production-ready code quality.\",\n",
    "                \"Consider the task context when generating the next improvement prompt.\",\n",
    "                \"The next prompt never should be related to implement or update TEST or project documentation.\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return \"\\n\".join(prompt_parts)\n",
    "\n",
    "    def _parse_structured_comparison_result(self, response_content: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Parse the structured LLM response into organized data.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize result structure\n",
    "        result = {\n",
    "            \"chosen_model\": \"neither\",\n",
    "            \"confidence_score\": 5.0,\n",
    "            \"original_request\": \"\",\n",
    "            \"task_context\": \"\",\n",
    "            \"file_summary\": {},\n",
    "            \"winner_justification\": \"\",\n",
    "            \"loser_critique\": \"\",\n",
    "            \"issue_type\": \"other\",\n",
    "            \"model_a_scores\": {},\n",
    "            \"model_b_scores\": {},\n",
    "            \"next_prompt\": \"\",\n",
    "            \"raw_response\": response_content,\n",
    "        }\n",
    "\n",
    "        lines = response_content.split(\"\\n\")\n",
    "        current_section = None\n",
    "        current_content = []\n",
    "\n",
    "        for line in lines:\n",
    "            line_stripped = line.strip()\n",
    "\n",
    "            # Detect sections\n",
    "            if \"üéØ Evaluation Result:\" in line and \"üèÜ\" in line:\n",
    "                if \"MODEL_A\" in line.upper():\n",
    "                    result[\"chosen_model\"] = \"model_a\"\n",
    "                elif \"MODEL_B\" in line.upper():\n",
    "                    result[\"chosen_model\"] = \"model_b\"\n",
    "                current_section = \"evaluation_result\"\n",
    "\n",
    "            elif \"üìã Original Evaluator Request\" in line:\n",
    "                current_section = \"original_request\"\n",
    "\n",
    "            elif \"üéØ Task Context\" in line:\n",
    "                current_section = \"task_context\"\n",
    "\n",
    "            elif \"üìä File Analysis Summary\" in line:\n",
    "                current_section = \"file_summary\"\n",
    "\n",
    "            elif \"‚úÖ Why\" in line and \"Superior Implementation\" in line:\n",
    "                current_section = \"winner_justification\"\n",
    "                current_content = []\n",
    "\n",
    "            elif \"‚ùå Why\" in line and \"Inferior\" in line:\n",
    "                current_section = \"loser_critique\"\n",
    "                current_content = []\n",
    "\n",
    "            elif \"**Issue Type:**\" in line:\n",
    "                issue_type_line = line.replace(\"**Issue Type:**\", \"\").strip()\n",
    "                for issue_type in [\n",
    "                    \"technical_inconsistency\",\n",
    "                    \"tool\",\n",
    "                    \"code_correctness\",\n",
    "                    \"setup\",\n",
    "                    \"production_readiness\",\n",
    "                    \"other\",\n",
    "                ]:\n",
    "                    if issue_type in issue_type_line:\n",
    "                        result[\"issue_type\"] = issue_type\n",
    "                        break\n",
    "                current_section = \"issue_type\"\n",
    "\n",
    "            elif \"üìà Technical Assessment Scores\" in line:\n",
    "                current_section = \"scores\"\n",
    "\n",
    "            elif \"**Model A Scores:**\" in line:\n",
    "                current_section = \"model_a_scores\"\n",
    "\n",
    "            elif \"**Model B Scores:**\" in line:\n",
    "                current_section = \"model_b_scores\"\n",
    "\n",
    "            elif \"üîÑ Next Improvement Prompt\" in line:\n",
    "                current_section = \"next_prompt\"\n",
    "                current_content = []\n",
    "\n",
    "            elif current_section == \"original_request\" and line_stripped.startswith(\n",
    "                \">\"\n",
    "            ):\n",
    "                result[\"original_request\"] = line_stripped[1:].strip().strip('\"')\n",
    "\n",
    "            elif current_section == \"task_context\" and line_stripped.startswith(\">\"):\n",
    "                result[\"task_context\"] = line_stripped[1:].strip().strip('\"')\n",
    "\n",
    "            elif current_section == \"file_summary\" and line_stripped.startswith(\"- **\"):\n",
    "                if \"Current Files:\" in line:\n",
    "                    try:\n",
    "                        result[\"file_summary\"][\"current\"] = int(\n",
    "                            line_stripped.split(\":\")[-1].strip()\n",
    "                        )\n",
    "                    except:\n",
    "                        pass\n",
    "                elif \"Model A Files:\" in line:\n",
    "                    try:\n",
    "                        result[\"file_summary\"][\"model_a\"] = int(\n",
    "                            line_stripped.split(\":\")[-1].strip()\n",
    "                        )\n",
    "                    except:\n",
    "                        pass\n",
    "                elif \"Model B Files:\" in line:\n",
    "                    try:\n",
    "                        result[\"file_summary\"][\"model_b\"] = int(\n",
    "                            line_stripped.split(\":\")[-1].strip()\n",
    "                        )\n",
    "                    except:\n",
    "                        pass\n",
    "                elif \"Total Unique Files:\" in line:\n",
    "                    try:\n",
    "                        result[\"file_summary\"][\"total\"] = int(\n",
    "                            line_stripped.split(\":\")[-1].strip()\n",
    "                        )\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            elif (\n",
    "                current_section == \"winner_justification\"\n",
    "                and line_stripped\n",
    "                and not line_stripped.startswith(\"##\")\n",
    "            ):\n",
    "                current_content.append(line_stripped)\n",
    "\n",
    "            elif (\n",
    "                current_section == \"loser_critique\"\n",
    "                and line_stripped\n",
    "                and not line_stripped.startswith(\"##\")\n",
    "                and not line_stripped.startswith(\"**Issue Type:**\")\n",
    "            ):\n",
    "                current_content.append(line_stripped)\n",
    "\n",
    "            elif (\n",
    "                current_section in [\"model_a_scores\", \"model_b_scores\"]\n",
    "                and \":\" in line_stripped\n",
    "                and line_stripped.startswith(\"- \")\n",
    "            ):\n",
    "                try:\n",
    "                    score_line = line_stripped[2:].strip()  # Remove \"- \"\n",
    "                    score_name, score_value = score_line.split(\":\", 1)\n",
    "                    score_name = score_name.strip()\n",
    "                    score_value = score_value.strip().strip(\"[]\")\n",
    "\n",
    "                    score_num = None\n",
    "                    for char in score_value:\n",
    "                        if char.isdigit():\n",
    "                            score_num = int(char)\n",
    "                            break\n",
    "\n",
    "                    if score_num and current_section == \"model_a_scores\":\n",
    "                        result[\"model_a_scores\"][score_name] = score_num\n",
    "                    elif score_num and current_section == \"model_b_scores\":\n",
    "                        result[\"model_b_scores\"][score_name] = score_num\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            elif (\n",
    "                current_section == \"next_prompt\"\n",
    "                and line_stripped\n",
    "                and not line_stripped.startswith(\"##\")\n",
    "            ):\n",
    "                current_content.append(line_stripped)\n",
    "\n",
    "        if current_section == \"winner_justification\":\n",
    "            result[\"winner_justification\"] = \" \".join(current_content).strip()\n",
    "        elif current_section == \"loser_critique\":\n",
    "            result[\"loser_critique\"] = \" \".join(current_content).strip()\n",
    "        elif current_section == \"next_prompt\":\n",
    "            result[\"next_prompt\"] = \" \".join(current_content).strip()\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _parse_comparison_result(self, response_content: str) -> ModelComparisonResult:\n",
    "        \"\"\"Parse the LLM response into a structured result (legacy method for backward compatibility).\"\"\"\n",
    "\n",
    "        # Initialize default values\n",
    "        chosen_model = ModelChoice.NEITHER\n",
    "        confidence_score = 5.0\n",
    "        reasoning = \"Unable to parse reasoning from response\"\n",
    "        pros_model_a = []\n",
    "        cons_model_a = []\n",
    "        pros_model_b = []\n",
    "        cons_model_b = []\n",
    "        detailed_analysis = response_content\n",
    "\n",
    "        lines = response_content.split(\"\\n\")\n",
    "        current_section = None\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "\n",
    "            # Parse chosen model\n",
    "            if line.startswith(\"**CHOSEN MODEL:**\"):\n",
    "                model_text = line.replace(\"**CHOSEN MODEL:**\", \"\").strip().upper()\n",
    "                if \"MODEL_A\" in model_text:\n",
    "                    chosen_model = ModelChoice.MODEL_A\n",
    "                elif \"MODEL_B\" in model_text:\n",
    "                    chosen_model = ModelChoice.MODEL_B\n",
    "                elif \"BOTH_GOOD\" in model_text:\n",
    "                    chosen_model = ModelChoice.BOTH_GOOD\n",
    "                else:\n",
    "                    chosen_model = ModelChoice.NEITHER\n",
    "\n",
    "            # Parse confidence score\n",
    "            elif line.startswith(\"**CONFIDENCE SCORE:**\") or line.startswith(\n",
    "                \"**PRIMARY REASONING:**\"\n",
    "            ):\n",
    "                if \"CONFIDENCE SCORE\" in line:\n",
    "                    try:\n",
    "                        score_text = line.replace(\"**CONFIDENCE SCORE:**\", \"\").strip()\n",
    "                        confidence_score = float(score_text.split()[0])\n",
    "                    except:\n",
    "                        confidence_score = 5.0\n",
    "                elif \"PRIMARY REASONING\" in line:\n",
    "                    reasoning = line.replace(\"**PRIMARY REASONING:**\", \"\").strip()\n",
    "\n",
    "            # Track sections\n",
    "            elif \"**MODEL A OVERALL ASSESSMENT:**\" in line:\n",
    "                current_section = \"model_a\"\n",
    "            elif \"**MODEL B OVERALL ASSESSMENT:**\" in line:\n",
    "                current_section = \"model_b\"\n",
    "            elif \"‚úÖ Strengths:\" in line:\n",
    "                current_section += \"_pros\"\n",
    "            elif \"‚ùå Weaknesses:\" in line:\n",
    "                current_section += \"_cons\"\n",
    "            elif \"**DETAILED ANALYSIS:**\" in line:\n",
    "                current_section = \"detailed\"\n",
    "\n",
    "            # Parse lists\n",
    "            elif line.startswith(\"- \") and current_section:\n",
    "                item = line[2:].strip()\n",
    "                if current_section == \"model_a_pros\":\n",
    "                    pros_model_a.append(item)\n",
    "                elif current_section == \"model_a_cons\":\n",
    "                    cons_model_a.append(item)\n",
    "                elif current_section == \"model_b_pros\":\n",
    "                    pros_model_b.append(item)\n",
    "                elif current_section == \"model_b_cons\":\n",
    "                    cons_model_b.append(item)\n",
    "\n",
    "        return ModelComparisonResult(\n",
    "            chosen_model=chosen_model,\n",
    "            confidence_score=confidence_score,\n",
    "            reasoning=reasoning,\n",
    "            pros_model_a=pros_model_a,\n",
    "            cons_model_a=cons_model_a,\n",
    "            pros_model_b=pros_model_b,\n",
    "            cons_model_b=cons_model_b,\n",
    "            detailed_analysis=detailed_analysis,\n",
    "        )\n",
    "\n",
    "    def evaluate_models_against_requirements(\n",
    "        self,\n",
    "        evaluator_prompt_text: str,\n",
    "        current_files: List[FileContent],\n",
    "        model_a_files: List[FileContent],\n",
    "        model_b_files: List[FileContent],\n",
    "        task_description: str = None,  # Nueva entrada\n",
    "        custom_criteria: List[str] = None,\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate two model implementations with structured output parsing.\n",
    "\n",
    "        Args:\n",
    "            evaluator_prompt_text: The specific evaluation prompt\n",
    "            current_files: Current implementation files\n",
    "            model_a_files: Model A implementation files\n",
    "            model_b_files: Model B implementation files\n",
    "            task_description: General task context for production-oriented improvements\n",
    "            custom_criteria: Custom evaluation criteria\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract requirements from the evaluator prompt\n",
    "        print(\"Extracting requirements from evaluator prompt...\")\n",
    "        evaluator_prompt = self._extract_requirements_from_prompt(\n",
    "            evaluator_prompt_text, task_description\n",
    "        )\n",
    "\n",
    "        # Create refined analysis request\n",
    "        analysis_request = RefinedMultiModelAnalysisRequest(\n",
    "            evaluator_prompt=evaluator_prompt,\n",
    "            current_files=current_files,\n",
    "            model_a_files=model_a_files,\n",
    "            model_b_files=model_b_files,\n",
    "            task_description=task_description,\n",
    "            analysis_type=\"requirement_focused\",\n",
    "            custom_evaluation_criteria=custom_criteria,\n",
    "        )\n",
    "\n",
    "        # Generate the analysis prompt\n",
    "        analysis_prompt = self._create_requirement_focused_analysis_prompt(\n",
    "            analysis_request\n",
    "        )\n",
    "\n",
    "        # Get system message\n",
    "        system_message = self._get_requirement_focused_system_message(\n",
    "            analysis_request.analysis_type\n",
    "        )\n",
    "\n",
    "        # Create LLM messages\n",
    "        messages = [\n",
    "            SystemMessage(content=system_message),\n",
    "            HumanMessage(content=analysis_prompt),\n",
    "        ]\n",
    "\n",
    "        # Get response from LLM\n",
    "        print(\"Evaluating model implementations with structured output...\")\n",
    "        response = self.llm.invoke(messages)\n",
    "\n",
    "        # Parse the structured response\n",
    "        parsed_evaluation = self._parse_structured_comparison_result(response.content)\n",
    "\n",
    "        # Process response\n",
    "        analysis_result = {\n",
    "            \"evaluation_type\": \"structured_requirement_focused\",\n",
    "            \"evaluator_prompt\": {\n",
    "                \"original_text\": evaluator_prompt_text,\n",
    "                \"task_description\": task_description,  # Incluir en el resultado\n",
    "                \"extracted_requirements\": evaluator_prompt.requirements,\n",
    "                \"success_criteria\": evaluator_prompt.success_criteria,\n",
    "                \"priority_aspects\": evaluator_prompt.priority_aspects,\n",
    "            },\n",
    "            \"files_analyzed\": {\n",
    "                \"current_files\": len(current_files),\n",
    "                \"model_a_files\": len(model_a_files),\n",
    "                \"model_b_files\": len(model_b_files),\n",
    "                \"total_unique_files\": len(\n",
    "                    set(f.path for f in current_files)\n",
    "                    | set(f.path for f in model_a_files)\n",
    "                    | set(f.path for f in model_b_files)\n",
    "                ),\n",
    "            },\n",
    "            \"parsed_evaluation\": parsed_evaluation,\n",
    "            \"raw_response\": response.content,\n",
    "            \"raw_prompt_sent\": analysis_prompt,\n",
    "        }\n",
    "\n",
    "        return analysis_result\n",
    "\n",
    "    def save_structured_evaluation_report(\n",
    "        self, analysis_result: Dict, output_file: str\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Save structured evaluation results to a file.\n",
    "        \"\"\"\n",
    "        output_path = Path(output_file)\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        report_content = self._generate_structured_evaluation_report(analysis_result)\n",
    "\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(report_content)\n",
    "\n",
    "        print(f\"Structured evaluation report saved to: {output_file}\")\n",
    "\n",
    "    def _generate_structured_evaluation_report(self, analysis_result: Dict) -> str:\n",
    "        \"\"\"Generate a structured markdown report from evaluation results.\"\"\"\n",
    "\n",
    "        parsed_result = analysis_result.get(\"parsed_evaluation\", {})\n",
    "\n",
    "        chosen_model = parsed_result.get(\"chosen_model\", \"neither\")\n",
    "        if chosen_model == \"model_a\":\n",
    "            winner_display = \"üèÜ **MODEL A** (Winner)\"\n",
    "        elif chosen_model == \"model_b\":\n",
    "            winner_display = \"üèÜ **MODEL B** (Winner)\"\n",
    "        else:\n",
    "            winner_display = \"‚ùå **NO CLEAR WINNER**\"\n",
    "\n",
    "        report_parts = [\n",
    "            \"# Structured Code Evaluation Report\",\n",
    "            f\"\\n**Generated:** {self._get_timestamp()}\",\n",
    "            f\"\\n## üéØ Evaluation Result: {winner_display}\",\n",
    "        ]\n",
    "\n",
    "        report_parts.extend(\n",
    "            [\n",
    "                \"\\n---\",\n",
    "                \"\\n## üìù Raw LLM Response\",\n",
    "                \"\\n```markdown\",\n",
    "                parsed_result.get(\"raw_response\", \"No raw response captured\"),\n",
    "                \"\\n```\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return \"\\n\".join(report_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe0f83-4c4d-4541-a46f-c30ba149ff4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files for structured requirement evaluation...\n",
      "Loaded 0 current, 2 Model A, 2 Model B files\n",
      "\n",
      "Evaluating implementations against: 'Refactor the MCP configuration parsing logic to enhance modularity and maintainability. Focus on separating concerns further by creating distinct classes or modules for each server type, ensuring that each class encapsulates its own validation and serialization logic. Additionally, implement a more robust error handling mechanism that provides clearer feedback to users when invalid configurations are encountered. Finally, consider adding type hints and improving the overall code documentation to facilitate easier understanding and future contributions.'\n",
      "Task context: 'Extend the MCP configuration parsing logic to support new server types while maintaining backward compatibility. The goal is to allow seamless integration of additional server configurations, ensuring that the parsing logic is modular and extensible.'\n",
      "Extracting requirements from evaluator prompt...\n",
      "Evaluating model implementations with structured output...\n",
      "Structured evaluation report saved to: evaluation_reports/structured_requirement_evaluation.md\n",
      "\n",
      "üéØ STRUCTURED EVALUATION RESULT:\n",
      "   Original Request: 'Refactor the MCP configuration parsing logic to enhance modularity and maintainability. Focus on separating concerns further by creating distinct classes or modules for each server type, ensuring that each class encapsulates its own validation and serialization logic. Additionally, implement a more robust error handling mechanism that provides clearer feedback to users when invalid configurations are encountered. Finally, consider adding type hints and improving the overall code documentation to facilitate easier understanding and future contributions.'\n",
      "   Task Context: ''\n",
      "   Winner: NEITHER\n",
      "   Issue Type: production_readiness\n",
      "   Next Improvement Prompt: \"Review the current implementation of the MCP configuration parsing logic to identify any potential performance bottlenecks, especially in the validation and error handling processes. Consider optimizing the validation logic to reduce execution time while maintaining clarity and usability of error messages. Additionally, explore the possibility of implementing asynchronous validation methods to enhance responsiveness in environments where configuration parsing is a frequent operation.\"\n",
      "\n",
      "Structured requirement-focused evaluation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "def main_structured_evaluation_example():\n",
    "    \"\"\"Example usage for structured requirement-focused evaluation.\"\"\"\n",
    "\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"Error: OPENAI_API_KEY environment variable not set\")\n",
    "        return\n",
    "\n",
    "    agent = CodeReviewerAgent(api_key=api_key)\n",
    "\n",
    "    task_description = \"Extend the MCP configuration parsing logic to support new server types while maintaining backward compatibility. The goal is to allow seamless integration of additional server configurations, ensuring that the parsing logic is modular and extensible.\"\n",
    "    evaluator_prompt = \"Refactor the MCP configuration parsing logic to enhance modularity and maintainability. Focus on separating concerns further by creating distinct classes or modules for each server type, ensuring that each class encapsulates its own validation and serialization logic. Additionally, implement a more robust error handling mechanism that provides clearer feedback to users when invalid configurations are encountered. Finally, consider adding type hints and improving the overall code documentation to facilitate easier understanding and future contributions.\"\n",
    "\n",
    "    current_code_dir = \"current_code\"\n",
    "    model_a_code_dir = \"model_generated_code_a\"\n",
    "    model_b_code_dir = \"model_generated_code_b\"\n",
    "\n",
    "    try:\n",
    "        print(\"Loading files for structured requirement evaluation...\")\n",
    "        current_files = agent.load_files_from_directory(current_code_dir)\n",
    "        model_a_files = agent.load_files_from_directory(model_a_code_dir)\n",
    "        model_b_files = agent.load_files_from_directory(model_b_code_dir)\n",
    "\n",
    "        print(\n",
    "            f\"Loaded {len(current_files)} current, {len(model_a_files)} Model A, {len(model_b_files)} Model B files\"\n",
    "        )\n",
    "\n",
    "        print(f\"\\nEvaluating implementations against: '{evaluator_prompt}'\")\n",
    "        print(f\"Task context: '{task_description}'\")\n",
    "        results = agent.evaluate_models_against_requirements(\n",
    "            evaluator_prompt_text=evaluator_prompt,\n",
    "            current_files=current_files,\n",
    "            model_a_files=model_a_files,\n",
    "            model_b_files=model_b_files,\n",
    "            task_description=task_description,\n",
    "            custom_criteria=[\n",
    "                \"Code efficiency\",\n",
    "                \"Python best practices\",\n",
    "                \"Production readiness\",\n",
    "                \"Scalability\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        agent.save_structured_evaluation_report(\n",
    "            results, \"evaluation_reports/structured_requirement_evaluation.md\"\n",
    "        )\n",
    "\n",
    "        parsed_eval = results.get(\"parsed_evaluation\", {})\n",
    "        chosen = parsed_eval.get(\"chosen_model\", \"unknown\")\n",
    "        next_prompt = parsed_eval.get(\"next_prompt\", \"No follow-up prompt generated\")\n",
    "        issue_type = parsed_eval.get(\"issue_type\", \"other\")\n",
    "        task_context = parsed_eval.get(\"task_context\", \"No task context provided\")\n",
    "\n",
    "        print(f\"\\nüéØ STRUCTURED EVALUATION RESULT:\")\n",
    "        print(f\"   Original Request: '{evaluator_prompt}'\")\n",
    "        print(f\"   Task Context: '{task_context}'\")\n",
    "        print(f\"   Winner: {chosen.upper()}\")\n",
    "        print(f\"   Issue Type: {issue_type}\")\n",
    "        print(f\"   Next Improvement Prompt: {next_prompt}\")\n",
    "        print(\"\\nStructured requirement-focused evaluation completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during structured evaluation: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_structured_evaluation_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e2446c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d3152-02ed-4ae5-9094-daa703431cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
