{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415da323-0a70-4ef2-ace6-cb41e7f34a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-community langchain-core openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f5292",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from dataclasses import dataclass\n",
    "import difflib\n",
    "from enum import Enum\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6ba301-33d6-4263-baa9-5fcb94287e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelChoice(Enum):\n",
    "    \"\"\"Enum for model selection results.\"\"\"\n",
    "\n",
    "    MODEL_A = \"model_a\"\n",
    "    MODEL_B = \"model_b\"\n",
    "    NEITHER = \"neither\"\n",
    "    BOTH_GOOD = \"both_good\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcff9637-7068-4cf9-b2e8-1738361274e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FileContent:\n",
    "    \"\"\"Represents a file with its content and metadata.\"\"\"\n",
    "\n",
    "    path: str\n",
    "    content: str\n",
    "    file_type: str\n",
    "    hash: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvaluatorPrompt:\n",
    "    \"\"\"Structure for the original evaluator prompt and requirements.\"\"\"\n",
    "\n",
    "    original_prompt: str\n",
    "    task_description: str = None\n",
    "    requirements: List[str] = None\n",
    "    success_criteria: List[str] = None\n",
    "    priority_aspects: List[str] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelComparisonResult:\n",
    "    \"\"\"Result of comparing two model implementations.\"\"\"\n",
    "\n",
    "    chosen_model: ModelChoice\n",
    "    confidence_score: float\n",
    "    reasoning: str\n",
    "    pros_model_a: List[str]\n",
    "    cons_model_a: List[str]\n",
    "    pros_model_b: List[str]\n",
    "    cons_model_b: List[str]\n",
    "    detailed_analysis: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RefinedMultiModelAnalysisRequest:\n",
    "    \"\"\"Enhanced request structure focusing on evaluator requirements.\"\"\"\n",
    "\n",
    "    evaluator_prompt: EvaluatorPrompt\n",
    "    current_files: List[FileContent]\n",
    "    model_a_files: List[FileContent]\n",
    "    model_b_files: List[FileContent]\n",
    "    task_description: str = None\n",
    "    analysis_type: str = \"requirement_focused\"\n",
    "    custom_evaluation_criteria: List[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809d33b7-30e9-4e8d-b9cf-93bfb2e4909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeReviewerAgent:\n",
    "    \"\"\"\n",
    "    Enhanced agent for performing structured requirement-focused code analysis and comparison.\n",
    "    Evaluates two AI model implementations against original user requirements with structured output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, api_key: str, model: str = \"gpt-4o-mini\", temperature: float = 0\n",
    "    ):\n",
    "        \"\"\"Initialize the code reviewer agent.\"\"\"\n",
    "        self.llm = ChatOpenAI(model=model, temperature=temperature, api_key=api_key)\n",
    "        self.supported_extensions = {\n",
    "            \".py\": \"python\",\n",
    "            \".js\": \"javascript\",\n",
    "            \".ts\": \"typescript\",\n",
    "            \".java\": \"java\",\n",
    "            \".cpp\": \"cpp\",\n",
    "            \".c\": \"c\",\n",
    "            \".cs\": \"csharp\",\n",
    "            \".go\": \"go\",\n",
    "            \".rs\": \"rust\",\n",
    "            \".php\": \"php\",\n",
    "            \".rb\": \"ruby\",\n",
    "            \".swift\": \"swift\",\n",
    "            \".kt\": \"kotlin\",\n",
    "            \".scala\": \"scala\",\n",
    "            \".html\": \"html\",\n",
    "            \".css\": \"css\",\n",
    "            \".sql\": \"sql\",\n",
    "            \".sh\": \"shell\",\n",
    "            \".yaml\": \"yaml\",\n",
    "            \".yml\": \"yaml\",\n",
    "            \".json\": \"json\",\n",
    "            \".xml\": \"xml\",\n",
    "            \".md\": \"markdown\",\n",
    "        }\n",
    "\n",
    "    def _calculate_file_hash(self, content: str) -> str:\n",
    "        \"\"\"Calculate MD5 hash of file content.\"\"\"\n",
    "        return hashlib.md5(content.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    def _get_file_type(self, file_path: str) -> str:\n",
    "        \"\"\"Determine file type from extension.\"\"\"\n",
    "        ext = Path(file_path).suffix.lower()\n",
    "        return self.supported_extensions.get(ext, \"text\")\n",
    "\n",
    "    def _get_timestamp(self) -> str:\n",
    "        \"\"\"Get current timestamp.\"\"\"\n",
    "        return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
    "\n",
    "    def load_files_from_directory(self, directory_path: str) -> List[FileContent]:\n",
    "        \"\"\"\n",
    "        Load all supported files from a directory.\n",
    "\n",
    "        Args:\n",
    "            directory_path: Path to the directory containing files\n",
    "\n",
    "        Returns:\n",
    "            List of FileContent objects\n",
    "        \"\"\"\n",
    "        files = []\n",
    "        directory = Path(directory_path)\n",
    "\n",
    "        if not directory.exists():\n",
    "            raise FileNotFoundError(f\"Directory not found: {directory_path}\")\n",
    "\n",
    "        # Recursively find all files\n",
    "        for file_path in directory.rglob(\"*\"):\n",
    "            if (\n",
    "                file_path.is_file()\n",
    "                and file_path.suffix.lower() in self.supported_extensions\n",
    "            ):\n",
    "                try:\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        content = f.read()\n",
    "\n",
    "                    relative_path = str(file_path.relative_to(directory))\n",
    "\n",
    "                    file_content = FileContent(\n",
    "                        path=relative_path,\n",
    "                        content=content,\n",
    "                        file_type=self._get_file_type(str(file_path)),\n",
    "                        hash=self._calculate_file_hash(content),\n",
    "                    )\n",
    "                    files.append(file_content)\n",
    "\n",
    "                except (UnicodeDecodeError, PermissionError) as e:\n",
    "                    print(f\"Warning: Could not read file {file_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        return files\n",
    "\n",
    "    def _generate_file_diff(\n",
    "        self,\n",
    "        file_a: FileContent,\n",
    "        file_b: FileContent,\n",
    "        label_a: str = \"Version A\",\n",
    "        label_b: str = \"Version B\",\n",
    "    ) -> str:\n",
    "        \"\"\"Generate unified diff between two files.\"\"\"\n",
    "        lines_a = file_a.content.splitlines(keepends=True)\n",
    "        lines_b = file_b.content.splitlines(keepends=True)\n",
    "\n",
    "        diff = difflib.unified_diff(\n",
    "            lines_a,\n",
    "            lines_b,\n",
    "            fromfile=f\"{label_a}/{file_a.path}\",\n",
    "            tofile=f\"{label_b}/{file_b.path}\",\n",
    "            lineterm=\"\",\n",
    "        )\n",
    "\n",
    "        return \"\".join(diff)\n",
    "\n",
    "    def _extract_requirements_from_prompt(\n",
    "        self, prompt: str, task_description: str = None\n",
    "    ) -> EvaluatorPrompt:\n",
    "        \"\"\"\n",
    "        Extract requirements and success criteria from the evaluator prompt.\n",
    "\n",
    "        Args:\n",
    "            prompt: Original evaluator prompt\n",
    "            task_description: General task description for additional context\n",
    "\n",
    "        Returns:\n",
    "            EvaluatorPrompt with extracted requirements\n",
    "        \"\"\"\n",
    "\n",
    "        context_section = \"\"\n",
    "        if task_description:\n",
    "            context_section = f\"\"\"\n",
    "            \n",
    "            TASK CONTEXT: \"{task_description}\"\n",
    "            This context should inform the requirements extraction and help orient toward production-ready code quality.\n",
    "            \"\"\"\n",
    "\n",
    "        extraction_prompt = f\"\"\"\n",
    "        Analyze the following prompt and extract the key requirements and success criteria:\n",
    "    \n",
    "        PROMPT: \"{prompt}\"{context_section}\n",
    "    \n",
    "        Please identify:\n",
    "        1. Main functional requirements (what the code should do)\n",
    "        2. Technical requirements (language, frameworks, specific approaches)\n",
    "        3. Quality requirements (performance, security, maintainability, production-readiness)\n",
    "        4. Success criteria (how to measure if the implementation is successful)\n",
    "    \n",
    "        Format your response as:\n",
    "        **FUNCTIONAL_REQUIREMENTS:**\n",
    "        - [requirement 1]\n",
    "        - [requirement 2]\n",
    "    \n",
    "        **TECHNICAL_REQUIREMENTS:**\n",
    "        - [requirement 1]\n",
    "        - [requirement 2]\n",
    "    \n",
    "        **QUALITY_REQUIREMENTS:**\n",
    "        - [requirement 1]\n",
    "        - [requirement 2]\n",
    "    \n",
    "        **SUCCESS_CRITERIA:**\n",
    "        - [criteria 1]\n",
    "        - [criteria 2]\n",
    "    \n",
    "        **PRIORITY_ASPECTS:**\n",
    "        - [most important aspect 1]\n",
    "        - [most important aspect 2]\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            messages = [\n",
    "                SystemMessage(\n",
    "                    content=\"You are an expert requirements analyst. Extract clear, actionable requirements from prompts with focus on production-ready code quality.\"\n",
    "                ),\n",
    "                HumanMessage(content=extraction_prompt),\n",
    "            ]\n",
    "\n",
    "            response = self.llm.invoke(messages)\n",
    "\n",
    "            requirements = []\n",
    "            success_criteria = []\n",
    "            priority_aspects = []\n",
    "\n",
    "            lines = response.content.split(\"\\n\")\n",
    "            current_section = None\n",
    "\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if \"**FUNCTIONAL_REQUIREMENTS:**\" in line:\n",
    "                    current_section = \"functional\"\n",
    "                elif \"**TECHNICAL_REQUIREMENTS:**\" in line:\n",
    "                    current_section = \"technical\"\n",
    "                elif \"**QUALITY_REQUIREMENTS:**\" in line:\n",
    "                    current_section = \"quality\"\n",
    "                elif \"**SUCCESS_CRITERIA:**\" in line:\n",
    "                    current_section = \"success\"\n",
    "                elif \"**PRIORITY_ASPECTS:**\" in line:\n",
    "                    current_section = \"priority\"\n",
    "                elif line.startswith(\"- \") and current_section:\n",
    "                    item = line[2:].strip()\n",
    "                    if current_section in [\"functional\", \"technical\", \"quality\"]:\n",
    "                        requirements.append(f\"[{current_section.upper()}] {item}\")\n",
    "                    elif current_section == \"success\":\n",
    "                        success_criteria.append(item)\n",
    "                    elif current_section == \"priority\":\n",
    "                        priority_aspects.append(item)\n",
    "\n",
    "            return EvaluatorPrompt(\n",
    "                original_prompt=prompt,\n",
    "                task_description=task_description,\n",
    "                requirements=(\n",
    "                    requirements if requirements else [f\"Fulfill the request: {prompt}\"]\n",
    "                ),\n",
    "                success_criteria=(\n",
    "                    success_criteria\n",
    "                    if success_criteria\n",
    "                    else [\n",
    "                        \"Code works as requested\",\n",
    "                        \"Follows best practices\",\n",
    "                        \"Production-ready quality\",\n",
    "                    ]\n",
    "                ),\n",
    "                priority_aspects=(\n",
    "                    priority_aspects\n",
    "                    if priority_aspects\n",
    "                    else [\"Correctness\", \"Code quality\", \"Production-readiness\"]\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not extract requirements automatically: {e}\")\n",
    "            return EvaluatorPrompt(\n",
    "                original_prompt=prompt,\n",
    "                task_description=task_description,\n",
    "                requirements=[f\"Fulfill the request: {prompt}\"],\n",
    "                success_criteria=[\n",
    "                    \"Code works as requested\",\n",
    "                    \"Follows best practices\",\n",
    "                    \"Production-ready quality\",\n",
    "                ],\n",
    "                priority_aspects=[\n",
    "                    \"Correctness\",\n",
    "                    \"Code quality\",\n",
    "                    \"Production-readiness\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "    def _get_requirement_focused_system_message(self, analysis_type: str) -> str:\n",
    "        \"\"\"Get refined system message for requirement-focused evaluation with specific output format.\"\"\"\n",
    "\n",
    "        return \"\"\"\n",
    "        You are an expert code evaluator specializing in requirement compliance assessment and technical analysis.\n",
    "        \n",
    "        Your PRIMARY goal is to determine which AI model implementation better fulfills the original evaluator request and moves closer to production-ready code quality.\n",
    "        \n",
    "        EVALUATION PRIORITY ORDER:\n",
    "        1. **Requirement Fulfillment** - Does it do what was asked?\n",
    "        2. **Correctness** - Does the code work as intended?\n",
    "        3. **Completeness** - Does it address all aspects of the request?\n",
    "        4. **Production Readiness** - Is it ready for real-world deployment?\n",
    "        5. **Code Quality** - Is it well-written and maintainable?\n",
    "        6. **Best Practices** - Does it follow good coding standards?\n",
    "        \n",
    "        REQUIRED OUTPUT FORMAT:\n",
    "        You must provide your response in this EXACT structure with NO additional sections:\n",
    "        \n",
    "        ## üéØ Evaluation Result: üèÜ [MODEL_A/MODEL_B] (Winner)\n",
    "        \n",
    "        ## üìã Original Evaluator Request\n",
    "        > \"[original request text]\"\n",
    "        \n",
    "        ## üéØ Task Context\n",
    "        > \"[task description context if provided]\"\n",
    "        \n",
    "        ## üìä File Analysis Summary\n",
    "        - **Current Files:** [number]\n",
    "        - **Model A Files:** [number] \n",
    "        - **Model B Files:** [number]\n",
    "        - **Total Unique Files:** [number]\n",
    "        \n",
    "        ## ‚úÖ Why [CHOSEN_MODEL] is the Superior Implementation\n",
    "        [Write a technical argumentation paragraph of 200-300 words explaining why the chosen model offers the best option for the implementation requested in the user's prompt. Focus on production-readiness, technical quality, and how it aligns with the overall task context.]\n",
    "        \n",
    "        ## ‚ùå Why [REJECTED_MODEL] is Inferior\n",
    "        [Write a technical argumentation paragraph of 100-200 words explaining why the rejected model is inferior using technical concepts. Focus on specific technical shortcomings, requirement gaps, and production-readiness concerns.]\n",
    "        \n",
    "        **Issue Type:** [technical_inconsistency | tool | code_correctness | setup | production_readiness | other]\n",
    "        \n",
    "        ## üìà Technical Assessment Scores\n",
    "        \n",
    "        **Scoring Rules:**\n",
    "        - Numbers 0-3: Positive score for Model A (0 = best, 3 = acceptable | wrong answer)\n",
    "        - Numbers 4-7: Positive score for Model B (7 = best, 4 = acceptable | wrong answer)\n",
    "        \n",
    "        **Model A Scores:**\n",
    "        - interaction_rating: [1-7]\n",
    "        - code_logic: [1-7]\n",
    "        - naming_clarity: [1-7]\n",
    "        - organization_modularity: [1-7]\n",
    "        - interface_design: [1-7]\n",
    "        - error_handling: [1-7]\n",
    "        - documentation: [1-7]\n",
    "        - production_readiness: [1-7]\n",
    "        - review_readiness: [1-7]\n",
    "        \n",
    "        **Model B Scores:**\n",
    "        - interaction_rating: [1-7]\n",
    "        - code_logic: [1-7]\n",
    "        - naming_clarity: [1-7]\n",
    "        - organization_modularity: [1-7]\n",
    "        - interface_design: [1-7]\n",
    "        - error_handling: [1-7]\n",
    "        - documentation: [1-7]\n",
    "        - production_readiness: [1-7]\n",
    "        - review_readiness: [1-7]\n",
    "        \n",
    "        ## üîÑ Next Improvement Prompt\n",
    "        [Generate a well-scoped follow-up prompt focused on improving the implementation toward production-ready quality. Consider the task context and current code quality gaps. The prompt should be oriented toward code refinement, optimization, testing, documentation, or deployment readiness - whatever is most needed to reach production standards.]\n",
    "        \n",
    "        EVALUATION CRITERIA DEFINITIONS:\n",
    "        - **interaction_rating**: How well does the model engage with the problem? Does it explore edge cases, explain decisions, and allow for user iteration?\n",
    "        - **code_logic**: Is the code logically correct, efficient, and follows best practices? Are there bugs or performance issues?\n",
    "        - **naming_clarity**: Are variable, function, and class names descriptive, intuitive, and consistent?\n",
    "        - **organization_modularity**: Is the code well-structured, modular, readable, and maintainable?\n",
    "        - **interface_design**: Are user interfaces clear, usable, and appropriate for the task?\n",
    "        - **error_handling**: Does the code handle invalid inputs and edge cases gracefully with appropriate validation?\n",
    "        - **documentation**: Are comments and documentation useful, concise, and focused on non-obvious aspects?\n",
    "        - **production_readiness**: Is the code ready for deployment with proper configuration, security, and scalability considerations?\n",
    "        - **review_readiness**: Is the code ready for production-level pull request review with consistent style?\n",
    "        \n",
    "        ISSUE TYPE DEFINITIONS:\n",
    "        - **technical_inconsistency**: Code has inconsistent patterns, conflicting approaches, or technical contradictions\n",
    "        - **tool**: Incorrect or inappropriate use of tools, libraries, or frameworks\n",
    "        - **code_correctness**: Logical errors, bugs, or incorrect implementation that prevents proper functionality\n",
    "        - **setup**: Problems with configuration, environment setup, or deployment-related issues\n",
    "        - **production_readiness**: Code lacks necessary features for production deployment (logging, error handling, security, etc.)\n",
    "        - **other**: Issues that don't fit the above categories but represent clear technical problems\n",
    "        \n",
    "        Focus on how well each implementation serves the original purpose while progressing toward production-ready code that can be deployed in real-world scenarios.\n",
    "        \"\"\"\n",
    "\n",
    "    def _create_requirement_focused_analysis_prompt(\n",
    "        self, request: RefinedMultiModelAnalysisRequest\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Create a refined prompt focused on structured evaluation output.\n",
    "        \"\"\"\n",
    "        evaluator_prompt = request.evaluator_prompt\n",
    "\n",
    "        current_files_map = {f.path: f for f in request.current_files}\n",
    "        model_a_files_map = {f.path: f for f in request.model_a_files}\n",
    "        model_b_files_map = {f.path: f for f in request.model_b_files}\n",
    "\n",
    "        all_file_paths = (\n",
    "            set(current_files_map.keys())\n",
    "            | set(model_a_files_map.keys())\n",
    "            | set(model_b_files_map.keys())\n",
    "        )\n",
    "\n",
    "        prompt_parts = [\n",
    "            \"# Structured Code Implementation Evaluation\",\n",
    "            f\"\\n## Original Evaluator Request:\",\n",
    "            f'\"{evaluator_prompt.original_prompt}\"',\n",
    "        ]\n",
    "\n",
    "        if evaluator_prompt.task_description:\n",
    "            prompt_parts.extend(\n",
    "                [\n",
    "                    f\"\\n## Task Context:\",\n",
    "                    f'\"{evaluator_prompt.task_description}\"',\n",
    "                    \"\\nThis context should inform your evaluation and guide the next improvement prompt toward production-ready code quality.\",\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        prompt_parts.extend(\n",
    "            [\n",
    "                \"\\n## Your Mission:\",\n",
    "                \"Evaluate two AI model implementations and determine which better fulfills the original request while considering production-ready code quality.\",\n",
    "                \"Provide a structured response following the EXACT format specified in your system message.\",\n",
    "                \"Focus on how each implementation advances toward production deployment readiness.\",\n",
    "                \"\\n## File Analysis Data:\",\n",
    "                f\"- Current Files: {len(request.current_files)}\",\n",
    "                f\"- Model A Files: {len(request.model_a_files)}\",\n",
    "                f\"- Model B Files: {len(request.model_b_files)}\",\n",
    "                f\"- Total Unique Files: {len(all_file_paths)}\",\n",
    "                \"\\n## Code Implementations to Evaluate:\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for file_path in sorted(all_file_paths):\n",
    "            current_file = current_files_map.get(file_path)\n",
    "            model_a_file = model_a_files_map.get(file_path)\n",
    "            model_b_file = model_b_files_map.get(file_path)\n",
    "\n",
    "            prompt_parts.append(f\"\\n### File: {file_path}\")\n",
    "\n",
    "            # Current version\n",
    "            if current_file:\n",
    "                prompt_parts.extend(\n",
    "                    [\n",
    "                        f\"\\n#### CURRENT VERSION ({current_file.file_type}):\",\n",
    "                        f\"```{current_file.file_type}\",\n",
    "                        current_file.content,\n",
    "                        \"```\",\n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                prompt_parts.append(\"\\n#### CURRENT VERSION: *File does not exist*\")\n",
    "\n",
    "            # Model A version\n",
    "            if model_a_file:\n",
    "                prompt_parts.extend(\n",
    "                    [\n",
    "                        f\"\\n#### MODEL A IMPLEMENTATION ({model_a_file.file_type}):\",\n",
    "                        f\"```{model_a_file.file_type}\",\n",
    "                        model_a_file.content,\n",
    "                        \"```\",\n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                prompt_parts.append(\n",
    "                    \"\\n#### MODEL A IMPLEMENTATION: *File does not exist*\"\n",
    "                )\n",
    "\n",
    "            # Model B version\n",
    "            if model_b_file:\n",
    "                prompt_parts.extend(\n",
    "                    [\n",
    "                        f\"\\n#### MODEL B IMPLEMENTATION ({model_b_file.file_type}):\",\n",
    "                        f\"```{model_b_file.file_type}\",\n",
    "                        model_b_file.content,\n",
    "                        \"```\",\n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                prompt_parts.append(\n",
    "                    \"\\n#### MODEL B IMPLEMENTATION: *File does not exist*\"\n",
    "                )\n",
    "\n",
    "        prompt_parts.extend(\n",
    "            [\n",
    "                \"\\n## IMPORTANT:\",\n",
    "                \"Follow the EXACT output format specified in your system message.\",\n",
    "                \"Include all required sections in the specified order.\",\n",
    "                \"Provide technical justifications for your evaluations.\",\n",
    "                \"Generate scores according to the specified scoring rules.\",\n",
    "                \"Create a meaningful next improvement prompt oriented toward production-ready code quality.\",\n",
    "                \"Consider the task context when generating the next improvement prompt.\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return \"\\n\".join(prompt_parts)\n",
    "\n",
    "    def _parse_structured_comparison_result(self, response_content: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Parse the structured LLM response into organized data.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize result structure\n",
    "        result = {\n",
    "            \"chosen_model\": \"neither\",\n",
    "            \"confidence_score\": 5.0,\n",
    "            \"original_request\": \"\",\n",
    "            \"task_context\": \"\",\n",
    "            \"file_summary\": {},\n",
    "            \"winner_justification\": \"\",\n",
    "            \"loser_critique\": \"\",\n",
    "            \"issue_type\": \"other\",\n",
    "            \"model_a_scores\": {},\n",
    "            \"model_b_scores\": {},\n",
    "            \"next_prompt\": \"\",\n",
    "            \"raw_response\": response_content,\n",
    "        }\n",
    "\n",
    "        lines = response_content.split(\"\\n\")\n",
    "        current_section = None\n",
    "        current_content = []\n",
    "\n",
    "        for line in lines:\n",
    "            line_stripped = line.strip()\n",
    "\n",
    "            # Detect sections\n",
    "            if \"üéØ Evaluation Result:\" in line and \"üèÜ\" in line:\n",
    "                if \"MODEL_A\" in line.upper():\n",
    "                    result[\"chosen_model\"] = \"model_a\"\n",
    "                elif \"MODEL_B\" in line.upper():\n",
    "                    result[\"chosen_model\"] = \"model_b\"\n",
    "                current_section = \"evaluation_result\"\n",
    "\n",
    "            elif \"üìã Original Evaluator Request\" in line:\n",
    "                current_section = \"original_request\"\n",
    "\n",
    "            elif \"üéØ Task Context\" in line:\n",
    "                current_section = \"task_context\"\n",
    "\n",
    "            elif \"üìä File Analysis Summary\" in line:\n",
    "                current_section = \"file_summary\"\n",
    "\n",
    "            elif \"‚úÖ Why\" in line and \"Superior Implementation\" in line:\n",
    "                current_section = \"winner_justification\"\n",
    "                current_content = []\n",
    "\n",
    "            elif \"‚ùå Why\" in line and \"Inferior\" in line:\n",
    "                current_section = \"loser_critique\"\n",
    "                current_content = []\n",
    "\n",
    "            elif \"**Issue Type:**\" in line:\n",
    "                issue_type_line = line.replace(\"**Issue Type:**\", \"\").strip()\n",
    "                for issue_type in [\n",
    "                    \"technical_inconsistency\",\n",
    "                    \"tool\",\n",
    "                    \"code_correctness\",\n",
    "                    \"setup\",\n",
    "                    \"production_readiness\",\n",
    "                    \"other\",\n",
    "                ]:\n",
    "                    if issue_type in issue_type_line:\n",
    "                        result[\"issue_type\"] = issue_type\n",
    "                        break\n",
    "                current_section = \"issue_type\"\n",
    "\n",
    "            elif \"üìà Technical Assessment Scores\" in line:\n",
    "                current_section = \"scores\"\n",
    "\n",
    "            elif \"**Model A Scores:**\" in line:\n",
    "                current_section = \"model_a_scores\"\n",
    "\n",
    "            elif \"**Model B Scores:**\" in line:\n",
    "                current_section = \"model_b_scores\"\n",
    "\n",
    "            elif \"üîÑ Next Improvement Prompt\" in line:\n",
    "                current_section = \"next_prompt\"\n",
    "                current_content = []\n",
    "\n",
    "            elif current_section == \"original_request\" and line_stripped.startswith(\n",
    "                \">\"\n",
    "            ):\n",
    "                result[\"original_request\"] = line_stripped[1:].strip().strip('\"')\n",
    "\n",
    "            elif current_section == \"task_context\" and line_stripped.startswith(\">\"):\n",
    "                result[\"task_context\"] = line_stripped[1:].strip().strip('\"')\n",
    "\n",
    "            elif current_section == \"file_summary\" and line_stripped.startswith(\"- **\"):\n",
    "                if \"Current Files:\" in line:\n",
    "                    try:\n",
    "                        result[\"file_summary\"][\"current\"] = int(\n",
    "                            line_stripped.split(\":\")[-1].strip()\n",
    "                        )\n",
    "                    except:\n",
    "                        pass\n",
    "                elif \"Model A Files:\" in line:\n",
    "                    try:\n",
    "                        result[\"file_summary\"][\"model_a\"] = int(\n",
    "                            line_stripped.split(\":\")[-1].strip()\n",
    "                        )\n",
    "                    except:\n",
    "                        pass\n",
    "                elif \"Model B Files:\" in line:\n",
    "                    try:\n",
    "                        result[\"file_summary\"][\"model_b\"] = int(\n",
    "                            line_stripped.split(\":\")[-1].strip()\n",
    "                        )\n",
    "                    except:\n",
    "                        pass\n",
    "                elif \"Total Unique Files:\" in line:\n",
    "                    try:\n",
    "                        result[\"file_summary\"][\"total\"] = int(\n",
    "                            line_stripped.split(\":\")[-1].strip()\n",
    "                        )\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            elif (\n",
    "                current_section == \"winner_justification\"\n",
    "                and line_stripped\n",
    "                and not line_stripped.startswith(\"##\")\n",
    "            ):\n",
    "                current_content.append(line_stripped)\n",
    "\n",
    "            elif (\n",
    "                current_section == \"loser_critique\"\n",
    "                and line_stripped\n",
    "                and not line_stripped.startswith(\"##\")\n",
    "                and not line_stripped.startswith(\"**Issue Type:**\")\n",
    "            ):\n",
    "                current_content.append(line_stripped)\n",
    "\n",
    "            elif (\n",
    "                current_section in [\"model_a_scores\", \"model_b_scores\"]\n",
    "                and \":\" in line_stripped\n",
    "                and line_stripped.startswith(\"- \")\n",
    "            ):\n",
    "                try:\n",
    "                    score_line = line_stripped[2:].strip()  # Remove \"- \"\n",
    "                    score_name, score_value = score_line.split(\":\", 1)\n",
    "                    score_name = score_name.strip()\n",
    "                    score_value = score_value.strip().strip(\"[]\")\n",
    "\n",
    "                    score_num = None\n",
    "                    for char in score_value:\n",
    "                        if char.isdigit():\n",
    "                            score_num = int(char)\n",
    "                            break\n",
    "\n",
    "                    if score_num and current_section == \"model_a_scores\":\n",
    "                        result[\"model_a_scores\"][score_name] = score_num\n",
    "                    elif score_num and current_section == \"model_b_scores\":\n",
    "                        result[\"model_b_scores\"][score_name] = score_num\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            elif (\n",
    "                current_section == \"next_prompt\"\n",
    "                and line_stripped\n",
    "                and not line_stripped.startswith(\"##\")\n",
    "            ):\n",
    "                current_content.append(line_stripped)\n",
    "\n",
    "        if current_section == \"winner_justification\":\n",
    "            result[\"winner_justification\"] = \" \".join(current_content).strip()\n",
    "        elif current_section == \"loser_critique\":\n",
    "            result[\"loser_critique\"] = \" \".join(current_content).strip()\n",
    "        elif current_section == \"next_prompt\":\n",
    "            result[\"next_prompt\"] = \" \".join(current_content).strip()\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _parse_comparison_result(self, response_content: str) -> ModelComparisonResult:\n",
    "        \"\"\"Parse the LLM response into a structured result (legacy method for backward compatibility).\"\"\"\n",
    "\n",
    "        # Initialize default values\n",
    "        chosen_model = ModelChoice.NEITHER\n",
    "        confidence_score = 5.0\n",
    "        reasoning = \"Unable to parse reasoning from response\"\n",
    "        pros_model_a = []\n",
    "        cons_model_a = []\n",
    "        pros_model_b = []\n",
    "        cons_model_b = []\n",
    "        detailed_analysis = response_content\n",
    "\n",
    "        lines = response_content.split(\"\\n\")\n",
    "        current_section = None\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "\n",
    "            # Parse chosen model\n",
    "            if line.startswith(\"**CHOSEN MODEL:**\"):\n",
    "                model_text = line.replace(\"**CHOSEN MODEL:**\", \"\").strip().upper()\n",
    "                if \"MODEL_A\" in model_text:\n",
    "                    chosen_model = ModelChoice.MODEL_A\n",
    "                elif \"MODEL_B\" in model_text:\n",
    "                    chosen_model = ModelChoice.MODEL_B\n",
    "                elif \"BOTH_GOOD\" in model_text:\n",
    "                    chosen_model = ModelChoice.BOTH_GOOD\n",
    "                else:\n",
    "                    chosen_model = ModelChoice.NEITHER\n",
    "\n",
    "            # Parse confidence score\n",
    "            elif line.startswith(\"**CONFIDENCE SCORE:**\") or line.startswith(\n",
    "                \"**PRIMARY REASONING:**\"\n",
    "            ):\n",
    "                if \"CONFIDENCE SCORE\" in line:\n",
    "                    try:\n",
    "                        score_text = line.replace(\"**CONFIDENCE SCORE:**\", \"\").strip()\n",
    "                        confidence_score = float(score_text.split()[0])\n",
    "                    except:\n",
    "                        confidence_score = 5.0\n",
    "                elif \"PRIMARY REASONING\" in line:\n",
    "                    reasoning = line.replace(\"**PRIMARY REASONING:**\", \"\").strip()\n",
    "\n",
    "            # Track sections\n",
    "            elif \"**MODEL A OVERALL ASSESSMENT:**\" in line:\n",
    "                current_section = \"model_a\"\n",
    "            elif \"**MODEL B OVERALL ASSESSMENT:**\" in line:\n",
    "                current_section = \"model_b\"\n",
    "            elif \"‚úÖ Strengths:\" in line:\n",
    "                current_section += \"_pros\"\n",
    "            elif \"‚ùå Weaknesses:\" in line:\n",
    "                current_section += \"_cons\"\n",
    "            elif \"**DETAILED ANALYSIS:**\" in line:\n",
    "                current_section = \"detailed\"\n",
    "\n",
    "            # Parse lists\n",
    "            elif line.startswith(\"- \") and current_section:\n",
    "                item = line[2:].strip()\n",
    "                if current_section == \"model_a_pros\":\n",
    "                    pros_model_a.append(item)\n",
    "                elif current_section == \"model_a_cons\":\n",
    "                    cons_model_a.append(item)\n",
    "                elif current_section == \"model_b_pros\":\n",
    "                    pros_model_b.append(item)\n",
    "                elif current_section == \"model_b_cons\":\n",
    "                    cons_model_b.append(item)\n",
    "\n",
    "        return ModelComparisonResult(\n",
    "            chosen_model=chosen_model,\n",
    "            confidence_score=confidence_score,\n",
    "            reasoning=reasoning,\n",
    "            pros_model_a=pros_model_a,\n",
    "            cons_model_a=cons_model_a,\n",
    "            pros_model_b=pros_model_b,\n",
    "            cons_model_b=cons_model_b,\n",
    "            detailed_analysis=detailed_analysis,\n",
    "        )\n",
    "\n",
    "    def evaluate_models_against_requirements(\n",
    "        self,\n",
    "        evaluator_prompt_text: str,\n",
    "        current_files: List[FileContent],\n",
    "        model_a_files: List[FileContent],\n",
    "        model_b_files: List[FileContent],\n",
    "        task_description: str = None,  # Nueva entrada\n",
    "        custom_criteria: List[str] = None,\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate two model implementations with structured output parsing.\n",
    "\n",
    "        Args:\n",
    "            evaluator_prompt_text: The specific evaluation prompt\n",
    "            current_files: Current implementation files\n",
    "            model_a_files: Model A implementation files\n",
    "            model_b_files: Model B implementation files\n",
    "            task_description: General task context for production-oriented improvements\n",
    "            custom_criteria: Custom evaluation criteria\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract requirements from the evaluator prompt\n",
    "        print(\"Extracting requirements from evaluator prompt...\")\n",
    "        evaluator_prompt = self._extract_requirements_from_prompt(\n",
    "            evaluator_prompt_text, task_description\n",
    "        )\n",
    "\n",
    "        # Create refined analysis request\n",
    "        analysis_request = RefinedMultiModelAnalysisRequest(\n",
    "            evaluator_prompt=evaluator_prompt,\n",
    "            current_files=current_files,\n",
    "            model_a_files=model_a_files,\n",
    "            model_b_files=model_b_files,\n",
    "            task_description=task_description,\n",
    "            analysis_type=\"requirement_focused\",\n",
    "            custom_evaluation_criteria=custom_criteria,\n",
    "        )\n",
    "\n",
    "        # Generate the analysis prompt\n",
    "        analysis_prompt = self._create_requirement_focused_analysis_prompt(\n",
    "            analysis_request\n",
    "        )\n",
    "\n",
    "        # Get system message\n",
    "        system_message = self._get_requirement_focused_system_message(\n",
    "            analysis_request.analysis_type\n",
    "        )\n",
    "\n",
    "        # Create LLM messages\n",
    "        messages = [\n",
    "            SystemMessage(content=system_message),\n",
    "            HumanMessage(content=analysis_prompt),\n",
    "        ]\n",
    "\n",
    "        # Get response from LLM\n",
    "        print(\"Evaluating model implementations with structured output...\")\n",
    "        response = self.llm.invoke(messages)\n",
    "\n",
    "        # Parse the structured response\n",
    "        parsed_evaluation = self._parse_structured_comparison_result(response.content)\n",
    "\n",
    "        # Process response\n",
    "        analysis_result = {\n",
    "            \"evaluation_type\": \"structured_requirement_focused\",\n",
    "            \"evaluator_prompt\": {\n",
    "                \"original_text\": evaluator_prompt_text,\n",
    "                \"task_description\": task_description,  # Incluir en el resultado\n",
    "                \"extracted_requirements\": evaluator_prompt.requirements,\n",
    "                \"success_criteria\": evaluator_prompt.success_criteria,\n",
    "                \"priority_aspects\": evaluator_prompt.priority_aspects,\n",
    "            },\n",
    "            \"files_analyzed\": {\n",
    "                \"current_files\": len(current_files),\n",
    "                \"model_a_files\": len(model_a_files),\n",
    "                \"model_b_files\": len(model_b_files),\n",
    "                \"total_unique_files\": len(\n",
    "                    set(f.path for f in current_files)\n",
    "                    | set(f.path for f in model_a_files)\n",
    "                    | set(f.path for f in model_b_files)\n",
    "                ),\n",
    "            },\n",
    "            \"parsed_evaluation\": parsed_evaluation,\n",
    "            \"raw_response\": response.content,\n",
    "            \"raw_prompt_sent\": analysis_prompt,\n",
    "        }\n",
    "\n",
    "        return analysis_result\n",
    "\n",
    "    def save_structured_evaluation_report(\n",
    "        self, analysis_result: Dict, output_file: str\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Save structured evaluation results to a file.\n",
    "        \"\"\"\n",
    "        output_path = Path(output_file)\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        report_content = self._generate_structured_evaluation_report(analysis_result)\n",
    "\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(report_content)\n",
    "\n",
    "        print(f\"Structured evaluation report saved to: {output_file}\")\n",
    "\n",
    "    def _generate_structured_evaluation_report(self, analysis_result: Dict) -> str:\n",
    "        \"\"\"Generate a structured markdown report from evaluation results.\"\"\"\n",
    "\n",
    "        parsed_result = analysis_result.get(\"parsed_evaluation\", {})\n",
    "\n",
    "        chosen_model = parsed_result.get(\"chosen_model\", \"neither\")\n",
    "        if chosen_model == \"model_a\":\n",
    "            winner_display = \"üèÜ **MODEL A** (Winner)\"\n",
    "        elif chosen_model == \"model_b\":\n",
    "            winner_display = \"üèÜ **MODEL B** (Winner)\"\n",
    "        else:\n",
    "            winner_display = \"‚ùå **NO CLEAR WINNER**\"\n",
    "\n",
    "        report_parts = [\n",
    "            \"# Structured Code Evaluation Report\",\n",
    "            f\"\\n**Evaluation Type:** Requirement-Focused Assessment\",\n",
    "            f\"\\n**Generated:** {self._get_timestamp()}\",\n",
    "            f\"\\n**Evaluator:** CAMB3LL\",\n",
    "            f\"\\n## üéØ Evaluation Result: {winner_display}\",\n",
    "            \"\\n## üìã Original Evaluator Request\",\n",
    "            f\"\\n> \\\"{parsed_result.get('original_request', 'Not captured')}\\\"\",\n",
    "            \"\\n## üìä File Analysis Summary\",\n",
    "            f\"- **Current Files:** {parsed_result.get('file_summary', {}).get('current', 'N/A')}\",\n",
    "            f\"- **Model A Files:** {parsed_result.get('file_summary', {}).get('model_a', 'N/A')}\",\n",
    "            f\"- **Model B Files:** {parsed_result.get('file_summary', {}).get('model_b', 'N/A')}\",\n",
    "            f\"- **Total Unique Files:** {parsed_result.get('file_summary', {}).get('total', 'N/A')}\",\n",
    "            \"\\n## ‚úÖ Winner Justification\",\n",
    "            f\"\\n{parsed_result.get('winner_justification', 'No justification provided')}\",\n",
    "            \"\\n## ‚ùå Rejected Model Critique\",\n",
    "            f\"\\n{parsed_result.get('loser_critique', 'No critique provided')}\",\n",
    "            f\"\\n**Issue Type:** {parsed_result.get('issue_type', 'other')}\",\n",
    "            \"\\n## üìà Technical Assessment Scores\",\n",
    "            \"\\n### Model A Scores:\",\n",
    "        ]\n",
    "\n",
    "        # Add Model A scores\n",
    "        model_a_scores = parsed_result.get(\"model_a_scores\", {})\n",
    "        for metric, score in model_a_scores.items():\n",
    "            report_parts.append(f\"- **{metric}:** {score}/7\")\n",
    "\n",
    "        report_parts.append(\"\\n### Model B Scores:\")\n",
    "\n",
    "        # Add Model B scores\n",
    "        model_b_scores = parsed_result.get(\"model_b_scores\", {})\n",
    "        for metric, score in model_b_scores.items():\n",
    "            report_parts.append(f\"- **{metric}:** {score}/7\")\n",
    "\n",
    "        report_parts.extend(\n",
    "            [\n",
    "                \"\\n## üîÑ Next Improvement Prompt\",\n",
    "                f\"\\n{parsed_result.get('next_prompt', 'No follow-up prompt provided')}\",\n",
    "                \"\\n---\",\n",
    "                \"\\n## üìù Raw LLM Response\",\n",
    "                \"\\n```markdown\",\n",
    "                parsed_result.get(\"raw_response\", \"No raw response captured\"),\n",
    "                \"\\n```\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return \"\\n\".join(report_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe0f83-4c4d-4541-a46f-c30ba149ff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_structured_evaluation_example():\n",
    "    \"\"\"Example usage for structured requirement-focused evaluation.\"\"\"\n",
    "\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"Error: OPENAI_API_KEY environment variable not set\")\n",
    "        return\n",
    "\n",
    "    agent = CodeReviewerAgent(api_key=api_key)\n",
    "\n",
    "    task_description = \"Desarrollar un sistema que permita ver en consola si una palabra es un palindromo o no\"\n",
    "    evaluator_prompt = \"Genera un c√≥digo de Python para imprimir en consola 'Hello World' de la manera m√°s eficiente y siguiendo las mejores pr√°cticas de Python\"\n",
    "\n",
    "    current_code_dir = \"current_code\"\n",
    "    model_a_code_dir = \"model_generated_code_a\"\n",
    "    model_b_code_dir = \"model_generated_code_b\"\n",
    "\n",
    "    try:\n",
    "        print(\"Loading files for structured requirement evaluation...\")\n",
    "        current_files = agent.load_files_from_directory(current_code_dir)\n",
    "        model_a_files = agent.load_files_from_directory(model_a_code_dir)\n",
    "        model_b_files = agent.load_files_from_directory(model_b_code_dir)\n",
    "\n",
    "        print(\n",
    "            f\"Loaded {len(current_files)} current, {len(model_a_files)} Model A, {len(model_b_files)} Model B files\"\n",
    "        )\n",
    "\n",
    "        print(f\"\\nEvaluating implementations against: '{evaluator_prompt}'\")\n",
    "        print(f\"Task context: '{task_description}'\")\n",
    "        results = agent.evaluate_models_against_requirements(\n",
    "            evaluator_prompt_text=evaluator_prompt,\n",
    "            current_files=current_files,\n",
    "            model_a_files=model_a_files,\n",
    "            model_b_files=model_b_files,\n",
    "            task_description=task_description,\n",
    "            custom_criteria=[\n",
    "                \"Code efficiency\",\n",
    "                \"Python best practices\",\n",
    "                \"Production readiness\",\n",
    "                \"Scalability\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        agent.save_structured_evaluation_report(\n",
    "            results, \"evaluation_reports/structured_requirement_evaluation.md\"\n",
    "        )\n",
    "\n",
    "        parsed_eval = results.get(\"parsed_evaluation\", {})\n",
    "        chosen = parsed_eval.get(\"chosen_model\", \"unknown\")\n",
    "        next_prompt = parsed_eval.get(\"next_prompt\", \"No follow-up prompt generated\")\n",
    "        issue_type = parsed_eval.get(\"issue_type\", \"other\")\n",
    "        task_context = parsed_eval.get(\"task_context\", \"No task context provided\")\n",
    "\n",
    "        print(f\"\\nüéØ STRUCTURED EVALUATION RESULT:\")\n",
    "        print(f\"   Original Request: '{evaluator_prompt}'\")\n",
    "        print(f\"   Task Context: '{task_context}'\")\n",
    "        print(f\"   Winner: {chosen.upper()}\")\n",
    "        print(f\"   Issue Type: {issue_type}\")\n",
    "        print(f\"   Next Improvement Prompt: {next_prompt}\")\n",
    "        print(\"\\nStructured requirement-focused evaluation completed successfully!\")\n",
    "\n",
    "        model_a_scores = parsed_eval.get(\"model_a_scores\", {})\n",
    "        model_b_scores = parsed_eval.get(\"model_b_scores\", {})\n",
    "\n",
    "        if model_a_scores or model_b_scores:\n",
    "            print(f\"\\nüìà SCORING SUMMARY:\")\n",
    "            print(f\"   Model A Scores: {len(model_a_scores)} metrics evaluated\")\n",
    "            print(f\"   Model B Scores: {len(model_b_scores)} metrics evaluated\")\n",
    "\n",
    "            if model_a_scores:\n",
    "                avg_a = sum(model_a_scores.values()) / len(model_a_scores)\n",
    "                print(f\"   Model A Average: {avg_a:.1f}/7\")\n",
    "            if model_b_scores:\n",
    "                avg_b = sum(model_b_scores.values()) / len(model_b_scores)\n",
    "                print(f\"   Model B Average: {avg_b:.1f}/7\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during structured evaluation: {e}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_structured_evaluation_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d3152-02ed-4ae5-9094-daa703431cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
