{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0ae0b8ab-f3f3-4ffc-b6a9-b3aa588a7da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from pathlib import Path\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from dataclasses import dataclass\n",
    "import difflib\n",
    "from enum import Enum\n",
    "from datetime import datetime\n",
    "import PyPDF2\n",
    "import fitz  # PyMuPDF\n",
    "import pdfplumber\n",
    "import logging\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a880841f-39ec-4aaf-938c-4c9baf241758",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "class ModelChoice(Enum):\n",
    "    \"\"\"Enum for model selection results.\"\"\"\n",
    "\n",
    "    MODEL_A = \"model_a\"\n",
    "    MODEL_B = \"model_b\"\n",
    "    NEITHER = \"neither\"\n",
    "    BOTH_GOOD = \"both_good\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PDFContent:\n",
    "    \"\"\"Represents extracted PDF content with metadata.\"\"\"\n",
    "\n",
    "    original_pdf_path: str\n",
    "    extracted_content: str\n",
    "    extraction_method: str\n",
    "    extraction_timestamp: str\n",
    "    content_hash: str\n",
    "    log_file_path: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvaluatorPrompt:\n",
    "    \"\"\"Structure for the original evaluator prompt and requirements.\"\"\"\n",
    "\n",
    "    original_prompt: str\n",
    "    task_description: str = None\n",
    "    requirements: List[str] = None\n",
    "    success_criteria: List[str] = None\n",
    "    priority_aspects: List[str] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PDFBasedAnalysisRequest:\n",
    "    \"\"\"Enhanced request structure for PDF-based evaluation.\"\"\"\n",
    "\n",
    "    evaluator_prompt: EvaluatorPrompt\n",
    "    pdf_content: PDFContent\n",
    "    task_description: str = None\n",
    "    analysis_type: str = \"pdf_content_focused\"\n",
    "    custom_evaluation_criteria: List[str] = None\n",
    "    prompt_history: List[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c847f2cc-a0be-49a8-b557-634071d8f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFExtractionEngine:\n",
    "    \"\"\"Robust PDF extraction engine with fallback methods.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.extraction_methods = [\n",
    "            (\"pymupdf_robust\", self._extract_with_pymupdf_robust),\n",
    "            (\"pdfplumber_safe\", self._extract_with_pdfplumber_safe),\n",
    "            (\"pypdf2_safe\", self._extract_with_pypdf2_safe),\n",
    "            (\"raw_text_fallback\", self._extract_raw_text_fallback),\n",
    "        ]\n",
    "\n",
    "    def extract_pdf_to_log(\n",
    "        self, pdf_path: str, output_format: str = \"log\"\n",
    "    ) -> PDFContent:\n",
    "        \"\"\"\n",
    "        Extract PDF content and save to log file with robust error handling.\n",
    "\n",
    "        Args:\n",
    "            pdf_path: Path to PDF file\n",
    "            output_format: Format ('log' or 'txt')\n",
    "\n",
    "        Returns:\n",
    "            PDFContent object with extraction details\n",
    "        \"\"\"\n",
    "        if not os.path.exists(pdf_path):\n",
    "            raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "\n",
    "        # Create output file path\n",
    "        pdf_name = Path(pdf_path).stem\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_extension = \"log\" if output_format == \"log\" else \"txt\"\n",
    "        log_file_path = f\"{pdf_name}_extracted_{timestamp}.{output_extension}\"\n",
    "\n",
    "        extracted_text = \"\"\n",
    "        successful_method = None\n",
    "\n",
    "        # Try extraction methods in order of robustness\n",
    "        for method_name, method_func in self.extraction_methods:\n",
    "            try:\n",
    "                logger.info(f\"Attempting extraction with {method_name}...\")\n",
    "                extracted_text = method_func(pdf_path)\n",
    "\n",
    "                if extracted_text and len(extracted_text.strip()) > 0:\n",
    "                    successful_method = method_name\n",
    "                    logger.info(f\"âœ… Extraction successful with {method_name}\")\n",
    "                    break\n",
    "                else:\n",
    "                    logger.warning(f\"âš ï¸ {method_name} extracted no content\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Error with {method_name}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        if not extracted_text:\n",
    "            raise Exception(\"Failed to extract content with any method\")\n",
    "\n",
    "        # Save to log file\n",
    "        try:\n",
    "            with open(\n",
    "                log_file_path, \"w\", encoding=\"utf-8\", errors=\"ignore\"\n",
    "            ) as output_file:\n",
    "                if output_format == \"log\":\n",
    "                    # Add log header\n",
    "                    extraction_timestamp = datetime.now().strftime(\n",
    "                        \"%Y-%m-%d %H:%M:%S UTC\"\n",
    "                    )\n",
    "                    output_file.write(f\"[{extraction_timestamp}] PDF Extraction Log\\n\")\n",
    "                    output_file.write(\"=\" * 60 + \"\\n\")\n",
    "                    output_file.write(f\"Source PDF: {pdf_path}\\n\")\n",
    "                    output_file.write(f\"Extraction Method: {successful_method}\\n\")\n",
    "                    output_file.write(f\"User: CAMB3LL\\n\")\n",
    "                    output_file.write(f\"Timestamp: 2025-08-10 22:12:32\\n\")\n",
    "                    output_file.write(\"=\" * 60 + \"\\n\\n\")\n",
    "\n",
    "                output_file.write(extracted_text)\n",
    "\n",
    "            # Calculate content hash\n",
    "            content_hash = hashlib.md5(extracted_text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "            return PDFContent(\n",
    "                original_pdf_path=pdf_path,\n",
    "                extracted_content=extracted_text,\n",
    "                extraction_method=successful_method,\n",
    "                extraction_timestamp=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S UTC\"),\n",
    "                content_hash=content_hash,\n",
    "                log_file_path=log_file_path,\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving to log file: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _extract_with_pymupdf_robust(self, pdf_path: str) -> str:\n",
    "        \"\"\"Robust extraction with PyMuPDF - handles font errors.\"\"\"\n",
    "        text = \"\"\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "\n",
    "            for page_num in range(len(doc)):\n",
    "                try:\n",
    "                    page = doc.load_page(page_num)\n",
    "                    text += f\"\\n{'='*20} PÃ¡gina {page_num + 1} {'='*20}\\n\"\n",
    "\n",
    "                    try:\n",
    "                        page_text = page.get_text()\n",
    "                        if page_text:\n",
    "                            text += page_text + \"\\n\"\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Error in page {page_num + 1}: {str(e)}\")\n",
    "                        try:\n",
    "                            page_text = page.get_text(\n",
    "                                \"text\", flags=fitz.TEXT_PRESERVE_WHITESPACE\n",
    "                            )\n",
    "                            if page_text:\n",
    "                                text += page_text + \"\\n\"\n",
    "                        except:\n",
    "                            text += (\n",
    "                                f\"[Error extracting text from page {page_num + 1}]\\n\"\n",
    "                            )\n",
    "\n",
    "                    # Additional content info\n",
    "                    images = page.get_images()\n",
    "                    if images:\n",
    "                        text += f\"[Images found: {len(images)}]\\n\"\n",
    "\n",
    "                except Exception as e:\n",
    "                    text += f\"\\n[Error processing page {page_num + 1}: {str(e)}]\\n\"\n",
    "                    continue\n",
    "\n",
    "            doc.close()\n",
    "            return text\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"General PyMuPDF error: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _extract_with_pdfplumber_safe(self, pdf_path: str) -> str:\n",
    "        \"\"\"Safe extraction with pdfplumber.\"\"\"\n",
    "        text = \"\"\n",
    "        try:\n",
    "            with pdfplumber.open(pdf_path) as pdf:\n",
    "                for page_num, page in enumerate(pdf.pages, 1):\n",
    "                    try:\n",
    "                        text += f\"\\n{'='*20} PÃ¡gina {page_num} {'='*20}\\n\"\n",
    "\n",
    "                        page_text = page.extract_text(\n",
    "                            x_tolerance=2,\n",
    "                            y_tolerance=2,\n",
    "                            layout=True,\n",
    "                            x_density=7.25,\n",
    "                            y_density=13,\n",
    "                        )\n",
    "\n",
    "                        if page_text:\n",
    "                            text += page_text + \"\\n\"\n",
    "\n",
    "                        # Try to extract tables\n",
    "                        try:\n",
    "                            tables = page.extract_tables()\n",
    "                            if tables:\n",
    "                                text += f\"\\n[TABLES - Page {page_num}]\\n\"\n",
    "                                for table_num, table in enumerate(tables, 1):\n",
    "                                    text += f\"\\nTable {table_num}:\\n\"\n",
    "                                    for row in table:\n",
    "                                        if row:\n",
    "                                            text += (\n",
    "                                                \" | \".join(\n",
    "                                                    [\n",
    "                                                        str(cell) if cell else \"\"\n",
    "                                                        for cell in row\n",
    "                                                    ]\n",
    "                                                )\n",
    "                                                + \"\\n\"\n",
    "                                            )\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    except Exception as e:\n",
    "                        text += f\"\\n[Error on page {page_num}: {str(e)}]\\n\"\n",
    "                        continue\n",
    "\n",
    "            return text\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"pdfplumber error: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _extract_with_pypdf2_safe(self, pdf_path: str) -> str:\n",
    "        \"\"\"Safe extraction with PyPDF2 - ignores font errors.\"\"\"\n",
    "        text = \"\"\n",
    "        try:\n",
    "            with open(pdf_path, \"rb\") as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file, strict=False)\n",
    "\n",
    "                for page_num, page in enumerate(pdf_reader.pages, 1):\n",
    "                    try:\n",
    "                        text += f\"\\n{'='*20} PÃ¡gina {page_num} {'='*20}\\n\"\n",
    "\n",
    "                        try:\n",
    "                            page_text = page.extract_text()\n",
    "                            if page_text:\n",
    "                                text += page_text + \"\\n\"\n",
    "                        except:\n",
    "                            try:\n",
    "                                if \"/Contents\" in page:\n",
    "                                    text += \"[Content detected but not extractable]\\n\"\n",
    "                            except:\n",
    "                                text += \"[Page with no extractable text content]\\n\"\n",
    "\n",
    "                    except Exception as e:\n",
    "                        text += f\"[Error on page {page_num}: {str(e)}]\\n\"\n",
    "                        continue\n",
    "\n",
    "            return text\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"PyPDF2 error: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _extract_raw_text_fallback(self, pdf_path: str) -> str:\n",
    "        \"\"\"Last resort method - basic PDF file analysis.\"\"\"\n",
    "        text = \"\"\n",
    "        try:\n",
    "            with open(pdf_path, \"rb\") as file:\n",
    "                content = file.read()\n",
    "\n",
    "            text += \"=\" * 50 + \"\\n\"\n",
    "            text += \"RAW TEXT EXTRACTION (FALLBACK METHOD)\\n\"\n",
    "            text += \"=\" * 50 + \"\\n\\n\"\n",
    "\n",
    "            try:\n",
    "                content_str = content.decode(\"latin-1\", errors=\"ignore\")\n",
    "\n",
    "                import re\n",
    "\n",
    "                # Patterns to find readable text\n",
    "                text_patterns = [\n",
    "                    r\"\\((.*?)\\)\",  # Text in parentheses\n",
    "                    r\"/Title\\s*\\((.*?)\\)\",  # Titles\n",
    "                    r\"/Subject\\s*\\((.*?)\\)\",  # Subjects\n",
    "                    r\"/Author\\s*\\((.*?)\\)\",  # Authors\n",
    "                    r\"BT\\s+(.*?)\\s+ET\",  # PDF text blocks\n",
    "                ]\n",
    "\n",
    "                found_text = []\n",
    "                for pattern in text_patterns:\n",
    "                    matches = re.findall(pattern, content_str, re.MULTILINE | re.DOTALL)\n",
    "                    found_text.extend(matches)\n",
    "\n",
    "                if found_text:\n",
    "                    text += \"Text fragments found:\\n\\n\"\n",
    "                    for i, fragment in enumerate(found_text[:50], 1):\n",
    "                        clean_fragment = fragment.strip()\n",
    "                        if len(clean_fragment) > 2:\n",
    "                            text += f\"{i}. {clean_fragment}\\n\"\n",
    "                else:\n",
    "                    text += \"No readable text fragments found.\\n\"\n",
    "                    text += f\"File size: {len(content)} bytes\\n\"\n",
    "                    text += \"File may contain only images or be heavily encoded.\\n\"\n",
    "\n",
    "            except Exception as e:\n",
    "                text += f\"Error in raw analysis: {str(e)}\\n\"\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fallback method error: {str(e)}\")\n",
    "            text = \"Could not extract content with any available method.\"\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15fcd34-d2a4-4d80-bddb-a09c58281484",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedCodeReviewerAgent:\n",
    "    \"\"\"\n",
    "    Enhanced agent for PDF-based code evaluation.\n",
    "    Sends complete log content for LLM to identify model responses.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, api_key: str, model: str = \"gpt-4o-mini\", temperature: float = 0\n",
    "    ):\n",
    "        \"\"\"Initialize the enhanced code reviewer agent.\"\"\"\n",
    "        self.llm = ChatOpenAI(model=model, temperature=temperature, api_key=api_key)\n",
    "        self.pdf_extractor = PDFExtractionEngine()\n",
    "\n",
    "    def _calculate_content_hash(self, content: str) -> str:\n",
    "        \"\"\"Calculate MD5 hash of content.\"\"\"\n",
    "        return hashlib.md5(content.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    def _get_timestamp(self) -> str:\n",
    "        \"\"\"Get current timestamp.\"\"\"\n",
    "        return \"2025-08-10 22:12:32\"\n",
    "\n",
    "    def process_pdf_conversation(self, pdf_path: str) -> PDFContent:\n",
    "        \"\"\"\n",
    "        Process PDF file and extract content to log file.\n",
    "\n",
    "        Args:\n",
    "            pdf_path: Path to the PDF file to process\n",
    "\n",
    "        Returns:\n",
    "            PDFContent object with extraction details\n",
    "        \"\"\"\n",
    "        logger.info(f\"Processing PDF: {pdf_path}\")\n",
    "        return self.pdf_extractor.extract_pdf_to_log(pdf_path, output_format=\"log\")\n",
    "\n",
    "    def _extract_requirements_from_prompt(\n",
    "        self, prompt: str, task_description: str = None\n",
    "    ) -> EvaluatorPrompt:\n",
    "        \"\"\"\n",
    "        Extract requirements and success criteria from the evaluator prompt.\n",
    "        \"\"\"\n",
    "        context_section = \"\"\n",
    "        if task_description:\n",
    "            context_section = f\"\"\"\n",
    "            \n",
    "            TASK CONTEXT: \"{task_description}\"\n",
    "            This context should inform the requirements extraction and help orient toward production-ready code quality.\n",
    "            \"\"\"\n",
    "\n",
    "        extraction_prompt = f\"\"\"\n",
    "        Analyze the following prompt and extract the key requirements and success criteria:\n",
    "    \n",
    "        PROMPT: \"{prompt}\"{context_section}\n",
    "    \n",
    "        Please identify:\n",
    "        1. Main functional requirements (what the code should do)\n",
    "        2. Technical requirements (language, frameworks, specific approaches)\n",
    "        3. Quality requirements (performance, security, maintainability, production-readiness)\n",
    "        4. Success criteria (how to measure if the implementation is successful)\n",
    "    \n",
    "        Format your response as:\n",
    "        \n",
    "        **BASE_PROMPT**\n",
    "        {prompt}\n",
    "        \n",
    "        **FUNCTIONAL_REQUIREMENTS:**\n",
    "        - [requirement 1]\n",
    "        - [requirement 2]\n",
    "    \n",
    "        **TECHNICAL_REQUIREMENTS:**\n",
    "        - [requirement 1]\n",
    "        - [requirement 2]\n",
    "    \n",
    "        **QUALITY_REQUIREMENTS:**\n",
    "        - [requirement 1]\n",
    "        - [requirement 2]\n",
    "    \n",
    "        **SUCCESS_CRITERIA:**\n",
    "        - [criteria 1]\n",
    "        - [criteria 2]\n",
    "    \n",
    "        **PRIORITY_ASPECTS:**\n",
    "        - [most important aspect 1]\n",
    "        - [most important aspect 2]\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            messages = [\n",
    "                SystemMessage(\n",
    "                    content=\"You are an expert requirements analyst. Extract clear, actionable requirements from prompts with focus on production-ready code quality.\"\n",
    "                ),\n",
    "                HumanMessage(content=extraction_prompt),\n",
    "            ]\n",
    "\n",
    "            response = self.llm.invoke(messages)\n",
    "\n",
    "            requirements = []\n",
    "            success_criteria = []\n",
    "            priority_aspects = []\n",
    "\n",
    "            lines = response.content.split(\"\\n\")\n",
    "            current_section = None\n",
    "\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if \"**FUNCTIONAL_REQUIREMENTS:**\" in line:\n",
    "                    current_section = \"functional\"\n",
    "                elif \"**TECHNICAL_REQUIREMENTS:**\" in line:\n",
    "                    current_section = \"technical\"\n",
    "                elif \"**QUALITY_REQUIREMENTS:**\" in line:\n",
    "                    current_section = \"quality\"\n",
    "                elif \"**SUCCESS_CRITERIA:**\" in line:\n",
    "                    current_section = \"success\"\n",
    "                elif \"**PRIORITY_ASPECTS:**\" in line:\n",
    "                    current_section = \"priority\"\n",
    "                elif line.startswith(\"- \") and current_section:\n",
    "                    item = line[2:].strip()\n",
    "                    if current_section in [\"functional\", \"technical\", \"quality\"]:\n",
    "                        requirements.append(f\"[{current_section.upper()}] {item}\")\n",
    "                    elif current_section == \"success\":\n",
    "                        success_criteria.append(item)\n",
    "                    elif current_section == \"priority\":\n",
    "                        priority_aspects.append(item)\n",
    "\n",
    "            return EvaluatorPrompt(\n",
    "                original_prompt=prompt,\n",
    "                task_description=task_description,\n",
    "                requirements=(\n",
    "                    requirements if requirements else [f\"Fulfill the request: {prompt}\"]\n",
    "                ),\n",
    "                success_criteria=(\n",
    "                    success_criteria\n",
    "                    if success_criteria\n",
    "                    else [\n",
    "                        \"Code works as requested\",\n",
    "                        \"Follows best practices\",\n",
    "                        \"Production-ready quality\",\n",
    "                    ]\n",
    "                ),\n",
    "                priority_aspects=(\n",
    "                    priority_aspects\n",
    "                    if priority_aspects\n",
    "                    else [\"Correctness\", \"Code quality\", \"Production-readiness\"]\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not extract requirements automatically: {e}\")\n",
    "            return EvaluatorPrompt(\n",
    "                original_prompt=prompt,\n",
    "                task_description=task_description,\n",
    "                requirements=[f\"Fulfill the request: {prompt}\"],\n",
    "                success_criteria=[\n",
    "                    \"Code works as requested\",\n",
    "                    \"Follows best practices\",\n",
    "                    \"Production-ready quality\",\n",
    "                ],\n",
    "                priority_aspects=[\n",
    "                    \"Correctness\",\n",
    "                    \"Code quality\",\n",
    "                    \"Production-readiness\",\n",
    "                ],\n",
    "            )\n",
    "\n",
    "    def _get_pdf_focused_system_message(self, analysis_type: str) -> str:\n",
    "        \"\"\"Get system message for PDF content-based evaluation.\"\"\"\n",
    "        return \"\"\"\n",
    "        You are an expert software engineer/code evaluator specializing in requirement compliance assessment and technical analysis.\n",
    "        \n",
    "        Your PRIMARY goal is to evaluate conversation content that contains interactions between a user and TWO DIFFERENT AI MODELS (Model A and Model B). You must identify which model provided better technical guidance, code quality, and production-readiness.\n",
    "        \n",
    "        CRITICAL INSTRUCTIONS:\n",
    "        1. **IDENTIFY THE TWO MODELS**: From the conversation content, you must identify which responses belong to Model A and which belong to Model B\n",
    "        2. **COMPLETE CONTENT ANALYSIS**: The entire log content represents a conversation - analyze all of it to understand the full context\n",
    "        3. **NO ARTIFICIAL DIVISION**: The content has NOT been pre-divided - you must determine what constitutes each model's contribution\n",
    "        \n",
    "        EVALUATION PRIORITY ORDER:\n",
    "        1. **interaction_rating** - How well does the model engage with the problem? Does it explore edge cases, explain its decisions, and allow for user iteration or clarification?\n",
    "        2. **code_logic** - Is the code logically correct, efficient, and in line with best practices? Are there bugs, performance issues, or flawed reasoning?\n",
    "        3. **naming_clarity** - Are the names of variables, functions, and classes descriptive, intuitive, and consistent with the task and codebase?\n",
    "        4. **organization_modularity** - Is the code well-structured and modular? Does it promote readability, reuse, and maintainability?\n",
    "        5. **interface_design (if applicable)** - Are any user interfaces clear, usable, and appropriate for the task?\n",
    "        6. **error_handling** - Does the code handle invalid inputs and edge cases gracefully? Is exception handling or validation logic appropriate and secure?\n",
    "        7. **documentation** - Are the comments and documentation useful, concise, and focused on non-obvious aspects? Avoid comments that merely restate what the code does or contain the model's reasoning process.\n",
    "        8. **review_readiness** - Is the code ready for a pull request review? Does it reflect a clear, consistent style and adhere to the standards of a production-level contribution?\n",
    "        \n",
    "        REQUIRED OUTPUT FORMAT:\n",
    "        You must provide your response in this EXACT structure with NO additional sections:\n",
    "        \n",
    "        ## ðŸŽ¯ Evaluation Result: ðŸ† [MODEL_A/MODEL_B] (Winner)\n",
    "        \n",
    "        ## âœ… Why [CHOSEN_MODEL] is the Superior Response\n",
    "        [Write a technical argumentation paragraph of 200-300 words explaining why the chosen model offers the best technical guidance and code quality. Focus on production-readiness, problem-solving approach, and technical depth. Reference specific parts of the conversation.]\n",
    "        \n",
    "        ## âŒ Why [REJECTED_MODEL] is Inferior\n",
    "        [Write a technical argumentation paragraph of 100-200 words explaining why the rejected model is inferior using technical concepts. Focus on specific technical shortcomings, gaps in reasoning, or production-readiness issues. Reference specific parts of the conversation.]\n",
    "        \n",
    "        **Issue Type:** [technical_inconsistency | tool | code_correctness | setup | production_readiness | other]\n",
    "        \n",
    "        ## ðŸ“ˆ Technical Assessment Scores\n",
    "        \n",
    "        **Scoring Rules:**\n",
    "        \n",
    "        ***Scoring guideline***\n",
    "        - The scores must be assigned according to the BEST RESPONSE AND MODEL CHOSEN. The analysis of the responses in such a way that these must reflect the choice of the model chosen for providing the best solution.\n",
    "        \n",
    "        **Interaction Scores:**\n",
    "        - interaction_rating: [Excellent | Good | Fair  | Poor ]\n",
    "        - code_logic: [Excellent | Good | Fair  | Poor ]\n",
    "        - naming_clarity: [Excellent | Good | Fair  | Poor ]\n",
    "        - organization_modularity: [Excellent | Good | Fair  | Poor ] \n",
    "        - interface_design: [Excellent | Good | Fair  | Poor ]\n",
    "        - error_handling: [Excellent | Good | Fair  | Poor ]\n",
    "        - documentation: [Excellent | Good | Fair  | Poor ]\n",
    "        - review_readiness: [Excellent | Good | Fair  | Poor ]\n",
    "        \n",
    "        ## ðŸ”„ Next Improvement Prompt\n",
    "        [Generate a well-scoped follow-up prompt focused on improving the implementation toward production-ready quality. Next prompt should be SIMPLER than the task description. Consider the task context and current code quality gaps. The prompt should build upon previous work without repeating already implemented features. CRITICAL: Review the prompt history provided to avoid suggesting already completed tasks or implementations. Focus on logical next steps that advance toward the task description goal without increasing complexity or requesting tests/documentation updates. The next prompt should be simple. ALWAYS focused on solve problems in the choosen response and oriented to improve: Security Assessment, Performance Analysis, Build & Deploy, Code Quality or Architecture Review.]\n",
    "        \n",
    "        EVALUATION CRITERIA DEFINITIONS:\n",
    "        - **interaction_rating**: How well does the model engage with the problem? Does it explore edge cases, explain decisions, and allow for user iteration?\n",
    "        - **code_logic**: Is the code logically correct, efficient, and follows best practices? Are there bugs or performance issues?\n",
    "        - **naming_clarity**: Are variable, function, and class names descriptive, intuitive, and consistent?\n",
    "        - **organization_modularity**: Is the code well-structured, modular, readable, and maintainable?\n",
    "        - **interface_design**: Are user interfaces clear, usable, and appropriate for the task?\n",
    "        - **error_handling**: Does the code handle invalid inputs and edge cases gracefully with appropriate validation?\n",
    "        - **documentation**: Are comments and documentation useful, concise, and focused on non-obvious aspects?\n",
    "        - **production_readiness**: Is the code ready for deployment with proper configuration, security, and scalability considerations?\n",
    "        - **review_readiness**: Is the code ready for production-level pull request review with consistent style?\n",
    "        \n",
    "        ISSUE TYPE DEFINITIONS:\n",
    "        - **technical_inconsistency**: Code has inconsistent patterns, conflicting approaches, or technical contradictions\n",
    "        - **tool**: Incorrect or inappropriate use of tools, libraries, or frameworks\n",
    "        - **code_correctness**: Logical errors, bugs, or incorrect implementation that prevents proper functionality\n",
    "        - **setup**: Problems with configuration, environment setup, or deployment-related issues\n",
    "        - **production_readiness**: Code lacks necessary features for production deployment (logging, error handling, security, etc.)\n",
    "        - **other**: Issues that don't fit the above categories but represent clear technical problems\n",
    "        \n",
    "        IMPORTANT LIMITATIONS FOR NEXT PROMPT GENERATION:\n",
    "        - The Interaction Scores should reflect the final decision. It's not necessary to generate scores for each model, instead you must generate an overall score that demonstrates that choice according to the best model selected.\n",
    "        - CRITICAL: The next prompt NEVER should be related to implement or update TEST or project documentation. It should be ALWAYS related to fix problems in the current response and NEVER should increase the complexity of the general task. \n",
    "        - CRITICAL: Always review the provided prompt history to avoid repeating previously completed implementations or tasks. The next prompt should be ALWAYS focused in to improve the current response or fix issues or weaknesses foun in the analyzed response. \n",
    "        - CRITICAL: The next prompt should be ALWAYS focused on solve problems in the choosen response and oriented to improve: Security Assessment, Performance Analysis, Build & Deploy, Code Quality or Architecture Review.\n",
    "        - CRITICAL: Next prompt ALWAYS shoud mention the file or module where the improvement should be implemented.\n",
    "        - Focus on incremental improvements that logically build upon existing work.\n",
    "        - Ensure the next prompt advances toward the task description goal without unnecessary complexity.\n",
    "        \n",
    "        Focus on how well each implementation serves the original purpose while progressing toward production-ready code that can be deployed in real-world scenarios.\n",
    "        \"\"\"\n",
    "\n",
    "    def _create_pdf_focused_analysis_prompt(\n",
    "        self, request: PDFBasedAnalysisRequest\n",
    "    ) -> str:\n",
    "        \"\"\"Create analysis prompt focused on complete PDF content evaluation.\"\"\"\n",
    "        evaluator_prompt = request.evaluator_prompt\n",
    "        pdf_content = request.pdf_content\n",
    "\n",
    "        prompt_parts = [\n",
    "            \"# Complete PDF Conversation Analysis\",\n",
    "            f\"\\n## Original Evaluator Request:\",\n",
    "            f'\"{evaluator_prompt.original_prompt}\"',\n",
    "        ]\n",
    "\n",
    "        if evaluator_prompt.task_description:\n",
    "            prompt_parts.extend(\n",
    "                [\n",
    "                    f\"\\n## Task Context:\",\n",
    "                    f'\"{evaluator_prompt.task_description}\"',\n",
    "                    \"\\nThis context should inform your evaluation and guide the next improvement prompt toward production-ready code quality.\",\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # Add prompt history section\n",
    "        if request.prompt_history and len(request.prompt_history) > 0:\n",
    "            prompt_parts.extend(\n",
    "                [\n",
    "                    f\"\\n## ðŸ“‹ Previous Prompts History:\",\n",
    "                    \"The following prompts have been previously executed in this development session:\",\n",
    "                ]\n",
    "            )\n",
    "            for i, prev_prompt in enumerate(request.prompt_history, 1):\n",
    "                prompt_parts.append(f'{i}. \"{prev_prompt}\"')\n",
    "\n",
    "            prompt_parts.extend(\n",
    "                [\n",
    "                    \"\\n**CRITICAL**: When generating the next improvement prompt, you MUST:\",\n",
    "                    \"- Review this history to avoid repeating already completed tasks\",\n",
    "                    \"- Build upon previous work without duplicating implementations\",\n",
    "                    \"- Focus on logical next steps that advance toward the task description goal\",\n",
    "                    \"- Ensure progression without unnecessary complexity increases\",\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        prompt_parts.extend(\n",
    "            [\n",
    "                \"\\n## Your Mission:\",\n",
    "                \"Analyze the COMPLETE conversation content extracted from the PDF to identify TWO DIFFERENT AI MODELS (Model A and Model B) and determine which provided superior technical guidance.\",\n",
    "                \"The content has NOT been pre-divided - you must identify what constitutes each model's responses within the conversation.\",\n",
    "                \"Provide a structured response following the EXACT format specified in your system message.\",\n",
    "                \"\\n## COMPLETE CONVERSATION CONTENT TO ANALYZE:\",\n",
    "                \"\\n### Full Extracted Content:\",\n",
    "                \"```\",\n",
    "                pdf_content.extracted_content,\n",
    "                \"```\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        prompt_parts.extend(\n",
    "            [\n",
    "                \"\\n## ANALYSIS INSTRUCTIONS:\",\n",
    "                \"1. **IDENTIFY THE MODELS**: Look for patterns, signatures, or formatting that distinguish Model A from Model B responses\",\n",
    "                \"2. **ANALYZE COMPLETE INTERACTION**: Consider the full conversation flow and how each model handled the user's requests\",\n",
    "                \"3. **EVALUATE TECHNICAL QUALITY**: Assess code quality, problem-solving approach, and production-readiness for each model\",\n",
    "                \"4. **DETERMINE WINNER**: Choose which model provided overall superior technical guidance\",\n",
    "                \"\\n## IMPORTANT:\",\n",
    "                \"Follow the EXACT output format specified in your system message.\",\n",
    "                \"Include all required sections in the specified order.\",\n",
    "                \"Provide technical justifications based on the complete conversation analysis.\",\n",
    "                \"Generate scores according to the specified scoring rules.\",\n",
    "                \"Create a meaningful next improvement prompt oriented toward production-ready code quality.\",\n",
    "                \"Consider the full conversation context when generating the next improvement prompt.\",\n",
    "                \"The next prompt never should be related to implement or update TEST or project documentation.\",\n",
    "                \"CRITICAL: Review the prompt history to avoid repeating previously completed implementations.\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return \"\\n\".join(prompt_parts)\n",
    "\n",
    "    def _parse_structured_comparison_result(self, response_content: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Parse the structured LLM response into organized data.\n",
    "        \"\"\"\n",
    "        # Initialize result structure\n",
    "        result = {\n",
    "            \"chosen_model\": \"neither\",\n",
    "            \"confidence_score\": 5.0,\n",
    "            \"original_request\": \"\",\n",
    "            \"task_context\": \"\",\n",
    "            \"content_summary\": {},\n",
    "            \"winner_justification\": \"\",\n",
    "            \"loser_critique\": \"\",\n",
    "            \"issue_type\": \"other\",\n",
    "            \"interaction_scores\": {},\n",
    "            \"next_prompt\": \"\",\n",
    "            \"raw_response\": response_content,\n",
    "        }\n",
    "\n",
    "        lines = response_content.split(\"\\n\")\n",
    "        current_section = None\n",
    "        current_content = []\n",
    "\n",
    "        for line in lines:\n",
    "            line_stripped = line.strip()\n",
    "\n",
    "            # Detect sections\n",
    "            if \"ðŸŽ¯ Evaluation Result:\" in line and \"ðŸ†\" in line:\n",
    "                if \"MODEL_A\" in line.upper():\n",
    "                    result[\"chosen_model\"] = \"model_a\"\n",
    "                elif \"MODEL_B\" in line.upper():\n",
    "                    result[\"chosen_model\"] = \"model_b\"\n",
    "                current_section = \"evaluation_result\"\n",
    "\n",
    "            elif \"ðŸ“Š Content Analysis Summary\" in line:\n",
    "                current_section = \"content_summary\"\n",
    "\n",
    "            elif \"âœ… Why\" in line and \"Superior\" in line:\n",
    "                current_section = \"winner_justification\"\n",
    "                current_content = []\n",
    "\n",
    "            elif \"âŒ Why\" in line and \"Inferior\" in line:\n",
    "                current_section = \"loser_critique\"\n",
    "                current_content = []\n",
    "\n",
    "            elif \"**Issue Type:**\" in line:\n",
    "                issue_type_line = line.replace(\"**Issue Type:**\", \"\").strip()\n",
    "                for issue_type in [\n",
    "                    \"technical_inconsistency\",\n",
    "                    \"tool\",\n",
    "                    \"code_correctness\",\n",
    "                    \"setup\",\n",
    "                    \"production_readiness\",\n",
    "                    \"other\",\n",
    "                ]:\n",
    "                    if issue_type in issue_type_line:\n",
    "                        result[\"issue_type\"] = issue_type\n",
    "                        break\n",
    "\n",
    "            elif \"ðŸ“ˆ Technical Assessment Scores\" in line:\n",
    "                current_section = \"scores\"\n",
    "\n",
    "            elif \"ðŸ”„ Next Improvement Prompt\" in line:\n",
    "                current_section = \"next_prompt\"\n",
    "                current_content = []\n",
    "\n",
    "            elif current_section == \"content_summary\" and line_stripped.startswith(\n",
    "                \"- **\"\n",
    "            ):\n",
    "                if \"PDF Source:\" in line:\n",
    "                    result[\"content_summary\"][\"pdf_source\"] = line_stripped.split(\":\")[\n",
    "                        -1\n",
    "                    ].strip()\n",
    "                elif \"Extraction Method:\" in line:\n",
    "                    result[\"content_summary\"][\"extraction_method\"] = (\n",
    "                        line_stripped.split(\":\")[-1].strip()\n",
    "                    )\n",
    "                elif \"Total Content Length:\" in line:\n",
    "                    try:\n",
    "                        result[\"content_summary\"][\"total_length\"] = int(\n",
    "                            line_stripped.split(\":\")[-1].strip().split()[0]\n",
    "                        )\n",
    "                    except:\n",
    "                        pass\n",
    "                elif \"Log File Generated:\" in line:\n",
    "                    result[\"content_summary\"][\"log_file\"] = line_stripped.split(\":\")[\n",
    "                        -1\n",
    "                    ].strip()\n",
    "                elif \"Model Identification:\" in line:\n",
    "                    result[\"content_summary\"][\"model_identification\"] = (\n",
    "                        line_stripped.split(\":\")[-1].strip()\n",
    "                    )\n",
    "\n",
    "            elif (\n",
    "                current_section == \"winner_justification\"\n",
    "                and line_stripped\n",
    "                and not line_stripped.startswith(\"##\")\n",
    "            ):\n",
    "                current_content.append(line_stripped)\n",
    "\n",
    "            elif (\n",
    "                current_section == \"loser_critique\"\n",
    "                and line_stripped\n",
    "                and not line_stripped.startswith(\"##\")\n",
    "                and not line_stripped.startswith(\"**Issue Type:**\")\n",
    "            ):\n",
    "                current_content.append(line_stripped)\n",
    "\n",
    "            elif (\n",
    "                current_section == \"scores\"\n",
    "                and \":\" in line_stripped\n",
    "                and line_stripped.startswith(\"- \")\n",
    "            ):\n",
    "                try:\n",
    "                    score_line = line_stripped[2:].strip()  # Remove \"- \"\n",
    "                    score_name, score_value = score_line.split(\":\", 1)\n",
    "                    score_name = score_name.strip()\n",
    "                    score_value = score_value.strip().strip(\"[]\")\n",
    "\n",
    "                    result[\"interaction_scores\"][score_name] = score_value\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            elif (\n",
    "                current_section == \"next_prompt\"\n",
    "                and line_stripped\n",
    "                and not line_stripped.startswith(\"##\")\n",
    "            ):\n",
    "                current_content.append(line_stripped)\n",
    "\n",
    "        # Finalize content sections\n",
    "        if current_section == \"winner_justification\":\n",
    "            result[\"winner_justification\"] = \" \".join(current_content).strip()\n",
    "        elif current_section == \"loser_critique\":\n",
    "            result[\"loser_critique\"] = \" \".join(current_content).strip()\n",
    "        elif current_section == \"next_prompt\":\n",
    "            result[\"next_prompt\"] = \" \".join(current_content).strip()\n",
    "\n",
    "        return result\n",
    "\n",
    "    def evaluate_pdf_based_models(\n",
    "        self,\n",
    "        evaluator_prompt_text: str,\n",
    "        pdf_path: str,\n",
    "        task_description: str = None,\n",
    "        custom_criteria: List[str] = None,\n",
    "        prompt_history: List[str] = None,\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate model responses from complete PDF conversation content.\n",
    "\n",
    "        Args:\n",
    "            evaluator_prompt_text: The specific evaluation prompt\n",
    "            pdf_path: Path to the PDF file containing conversation\n",
    "            task_description: General task context for production-oriented improvements\n",
    "            custom_criteria: Custom evaluation criteria\n",
    "            prompt_history: List of previously executed prompts to avoid repetition\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with structured evaluation results\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract PDF content\n",
    "        print(\"Extracting content from PDF...\")\n",
    "        pdf_content = self.process_pdf_conversation(pdf_path)\n",
    "        print(f\"âœ… PDF content extracted to: {pdf_content.log_file_path}\")\n",
    "        print(\n",
    "            f\"ðŸ“Š Total content length: {len(pdf_content.extracted_content)} characters\"\n",
    "        )\n",
    "\n",
    "        # Extract requirements from the evaluator prompt\n",
    "        print(\"Extracting requirements from evaluator prompt...\")\n",
    "        evaluator_prompt = self._extract_requirements_from_prompt(\n",
    "            evaluator_prompt_text, task_description\n",
    "        )\n",
    "\n",
    "        # Create PDF-based analysis request\n",
    "        analysis_request = PDFBasedAnalysisRequest(\n",
    "            evaluator_prompt=evaluator_prompt,\n",
    "            pdf_content=pdf_content,\n",
    "            task_description=task_description,\n",
    "            analysis_type=\"pdf_content_focused\",\n",
    "            custom_evaluation_criteria=custom_criteria,\n",
    "            prompt_history=prompt_history,\n",
    "        )\n",
    "\n",
    "        # Generate the analysis prompt\n",
    "        analysis_prompt = self._create_pdf_focused_analysis_prompt(analysis_request)\n",
    "\n",
    "        # Get system message\n",
    "        system_message = self._get_pdf_focused_system_message(\n",
    "            analysis_request.analysis_type\n",
    "        )\n",
    "\n",
    "        # Create LLM messages\n",
    "        messages = [\n",
    "            SystemMessage(content=system_message),\n",
    "            HumanMessage(content=analysis_prompt),\n",
    "        ]\n",
    "\n",
    "        # Get response from LLM\n",
    "        print(\"Evaluating complete conversation content with LLM...\")\n",
    "        response = self.llm.invoke(messages)\n",
    "\n",
    "        # Parse the structured response\n",
    "        parsed_evaluation = self._parse_structured_comparison_result(response.content)\n",
    "\n",
    "        # Process response\n",
    "        analysis_result = {\n",
    "            \"evaluation_type\": \"pdf_complete_content_focused\",\n",
    "            \"user_login\": \"CAMB3LL\",\n",
    "            \"timestamp\": \"2025-08-10 22:12:32\",\n",
    "            \"evaluator_prompt\": {\n",
    "                \"original_text\": evaluator_prompt_text,\n",
    "                \"task_description\": task_description,\n",
    "                \"extracted_requirements\": evaluator_prompt.requirements,\n",
    "                \"success_criteria\": evaluator_prompt.success_criteria,\n",
    "                \"priority_aspects\": evaluator_prompt.priority_aspects,\n",
    "            },\n",
    "            \"pdf_analysis\": {\n",
    "                \"original_pdf_path\": pdf_content.original_pdf_path,\n",
    "                \"log_file_path\": pdf_content.log_file_path,\n",
    "                \"extraction_method\": pdf_content.extraction_method,\n",
    "                \"extraction_timestamp\": pdf_content.extraction_timestamp,\n",
    "                \"content_hash\": pdf_content.content_hash,\n",
    "                \"total_content_length\": len(pdf_content.extracted_content),\n",
    "            },\n",
    "            \"prompt_history\": prompt_history,\n",
    "            \"parsed_evaluation\": parsed_evaluation,\n",
    "            \"raw_response\": response.content,\n",
    "            \"raw_prompt_sent\": analysis_prompt,\n",
    "        }\n",
    "\n",
    "        return analysis_result\n",
    "\n",
    "    def save_pdf_evaluation_report(self, analysis_result: Dict, output_file: str):\n",
    "        \"\"\"\n",
    "        Save PDF-based evaluation results to a file.\n",
    "        \"\"\"\n",
    "        output_path = Path(output_file)\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        report_content = self._generate_pdf_evaluation_report(analysis_result)\n",
    "\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(report_content)\n",
    "\n",
    "        print(f\"PDF evaluation report saved to: {output_file}\")\n",
    "\n",
    "    def _generate_pdf_evaluation_report(self, analysis_result: Dict) -> str:\n",
    "        \"\"\"Generate a structured markdown report from PDF evaluation results.\"\"\"\n",
    "\n",
    "        parsed_result = analysis_result.get(\"parsed_evaluation\", {})\n",
    "        pdf_analysis = analysis_result.get(\"pdf_analysis\", {})\n",
    "\n",
    "        chosen_model = parsed_result.get(\"chosen_model\", \"neither\")\n",
    "        if chosen_model == \"model_a\":\n",
    "            winner_display = \"ðŸ† **MODEL A** (Winner)\"\n",
    "        elif chosen_model == \"model_b\":\n",
    "            winner_display = \"ðŸ† **MODEL B** (Winner)\"\n",
    "        else:\n",
    "            winner_display = \"âŒ **NO CLEAR WINNER**\"\n",
    "\n",
    "        report_parts = [\n",
    "            \"# PDF-Based Complete Conversation Evaluation Report\",\n",
    "            f\"\\n## ðŸŽ¯ Evaluation Result: {winner_display}\",\n",
    "        ]\n",
    "\n",
    "        report_parts.extend(\n",
    "            [\n",
    "                \"\\n---\",\n",
    "                \"\\n## ðŸ“ Raw LLM Response\",\n",
    "                \"\\n```markdown\",\n",
    "                parsed_result.get(\"raw_response\", \"No raw response captured\"),\n",
    "                \"\\n```\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return \"\\n\".join(report_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbf2969-4d2a-4a63-81e4-a89b82ffeab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing PDF: conversation.pdf\n",
      "INFO:__main__:Attempting extraction with pymupdf_robust...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Complete PDF Conversation Evaluation\n",
      "Extracting content from PDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:âœ… Extraction successful with pymupdf_robust\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PDF content extracted to: conversation_extracted_20250811_201708.log\n",
      "ðŸ“Š Total content length: 195461 characters\n",
      "Extracting requirements from evaluator prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating complete conversation content with LLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF evaluation report saved to: pdf_complete_evaluation_20250811_201726.md\n",
      "\n",
      "ðŸŽ¯ EVALUATION RESULT: MODEL_A WINS!\n",
      "\n",
      "ðŸ”„ NEXT IMPROVEMENT PROMPT:\n",
      "'\"Review the `_response_handlers_optimized.py` module to further enhance its performance and security. Focus on implementing additional caching strategies for the `BodyDecoder` and `StatusCodeValidator` classes to optimize repeated operations. Additionally, assess the current regex patterns for potential improvements in efficiency and robustness against edge cases. Ensure that all changes maintain backward compatibility and do not introduce breaking changes.\"'\n",
      "\n",
      "ðŸ“‹ PDF EXTRACTION DETAILS:\n",
      "   Log file: conversation_extracted_20250811_201708.log\n",
      "   Method: pymupdf_robust\n",
      "   Length: 195461 characters\n",
      "   Time: 2025-08-11 20:17:09 UTC\n",
      "\n",
      "ðŸ“„ Detailed report saved to: pdf_complete_evaluation_20250811_201726.md\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Example usage of the enhanced PDF-based evaluator.\"\"\"\n",
    "\n",
    "    # Initialize the agent\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"âŒ Please set OPENAI_API_KEY environment variable\")\n",
    "        return\n",
    "\n",
    "    agent = EnhancedCodeReviewerAgent(api_key=api_key)\n",
    "    pdf_path = \"conversation.pdf\"\n",
    "\n",
    "    # PROMPTING SECTION\n",
    "\n",
    "    task_description = \"The current HTTP response handling logic within the httpx repository is functional but lacks clarity and maintainability, especially as new features and edge cases are added. This task aims to refactor the existing response handling code to improve code readability, reduce complexity, and ensure consistency across the codebase. This will involve restructuring the logic for parsing and validating HTTP responses, separating concerns into smaller, more modular functions, and maintaining all public function signatures for backward compatibility.\"\n",
    "\n",
    "    evaluator_prompt = \"Review the `_response_handlers.py` module to ensure that all functions are optimized for performance and maintainability. Specifically, focus on the `parse_header_links` function to enhance its robustness against malformed input while maintaining clarity in its implementation. Consider edge cases that may not have been addressed in the current implementation.\"\n",
    "\n",
    "    prompt_history = [\n",
    "        \"Refactor the HTTP response handling logic in the httpx library to improve maintainability and clarity. Focus on modularizing header parsing, status code validation, and body decoding into separate functions while ensuring backward compatibility. Introduce structured error classes for common HTTP response issues with meaningful error messages. Update internal API calls and test cases to align with the refactored structure, writing new tests for uncovered edge cases like malformed headers or partial response bodies. Ensure the refactored code adheres to repository style and quality standards and does not degrade performance.\"\n",
    "        \"\"\n",
    "        \"\"\n",
    "        \"\"\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        print(\"ðŸš€ Starting Complete PDF Conversation Evaluation\")\n",
    "\n",
    "        result = agent.evaluate_pdf_based_models(\n",
    "            evaluator_prompt_text=evaluator_prompt,\n",
    "            pdf_path=pdf_path,\n",
    "            task_description=task_description,\n",
    "            prompt_history=prompt_history,\n",
    "        )\n",
    "\n",
    "        # Save report\n",
    "        report_file = (\n",
    "            f\"pdf_complete_evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "        )\n",
    "        agent.save_pdf_evaluation_report(result, report_file)\n",
    "\n",
    "        # Display results\n",
    "        chosen_model = result[\"parsed_evaluation\"][\"chosen_model\"]\n",
    "        print(f\"\\nðŸŽ¯ EVALUATION RESULT: {chosen_model.upper()} WINS!\")\n",
    "\n",
    "        if result[\"parsed_evaluation\"][\"next_prompt\"]:\n",
    "            print(f\"\\nðŸ”„ NEXT IMPROVEMENT PROMPT:\")\n",
    "            print(f\"'{result['parsed_evaluation']['next_prompt']}'\")\n",
    "\n",
    "        # Display PDF extraction info\n",
    "        pdf_info = result[\"pdf_analysis\"]\n",
    "        print(f\"\\nðŸ“‹ PDF EXTRACTION DETAILS:\")\n",
    "        print(f\"   Log file: {pdf_info['log_file_path']}\")\n",
    "        print(f\"   Method: {pdf_info['extraction_method']}\")\n",
    "        print(f\"   Length: {pdf_info['total_content_length']} characters\")\n",
    "        print(f\"   Time: {pdf_info['extraction_timestamp']}\")\n",
    "\n",
    "        print(f\"\\nðŸ“„ Detailed report saved to: {report_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during evaluation: {str(e)}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66e67f6-6ead-44e7-9a09-721fa050d4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
